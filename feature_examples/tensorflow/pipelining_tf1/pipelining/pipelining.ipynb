{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70a4a853",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590434da",
   "metadata": {},
   "source": [
    "# TensorFlow 1 Pipelining Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06892bf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "If a model is too big to fit on one IPU, you will need to distribute it over \n",
    "multiple IPUs. This is called model parallelism.  \n",
    "Documentation for model parallelism on IPU with TensorFlow can be found here: \n",
    "[TensorFlow Model Parallelism](<https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/model.html#model-parallelism>)\n",
    "\n",
    "This tutorial first provides an overview of the key concepts from this document. \n",
    "It then provides a walkthrough of how pipelining can be applied to an existing \n",
    "TensorFlow application that currently runs on a single IPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7181bf",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "For software installation and setup details, please see the Getting Started \n",
    "guide for your hardware setup, available here: \n",
    "[Getting Started Guides](<https://docs.graphcore.ai/en/latest/getting-started.html>).\n",
    "\n",
    "You must have installed the Graphcore TensorFlow 1 wheel into your current \n",
    "active Python environment before starting the tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90c0339",
   "metadata": {},
   "source": [
    "## Directory Structure\n",
    "\n",
    "| Filename                               | Description                                                                                    |\n",
    "| -------------------------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| `README.md`                            | This tutorial in markdown format                                                               |\n",
    "| `pipelining.ipynb`                     | This tutorial in Jupyter notebook format                                                       |\n",
    "| `pipelining.py`                        | An executable python file that is used as a single source to generate this file                |\n",
    "| `step1_single_ipu.py`                  | Step 1 - the existing TensorFlow application that runs **without** pipelining on a single IPU. |\n",
    "| `answers/step2_sharding.py`            | Step 2 - shows how to run on multiple IPUs, still **without** pipelining.                      |\n",
    "| `answers/step3_pipelining.py`          | Step 3 - shows how to add pipelining.                                                          |\n",
    "| `answers/step4_configurable_stages.py` | Step 4 - shows how configurable stages might be implemented.                                   |\n",
    "| `scripts/profile.sh`                   | Helper script to capture profiling reports.                                                    |\n",
    "| `images/`                              | Images used in this README.                                                                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf51aa84",
   "metadata": {},
   "source": [
    "## Key Principles of Model Pipelining\n",
    "\n",
    "This section describes the key principles of model pipelining with TensorFlow \n",
    "1 on IPU. A later section will apply this to the existing TensorFlow application \n",
    "that initially runs on a single IPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85084a94",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "TensorFlow on IPU supports two methods for model parallelism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381116e5",
   "metadata": {},
   "source": [
    "### 1. Model Parallelism With Sharding\n",
    "\n",
    "With model sharding, the model is split into stages where each stage can fit \n",
    "and be run on a single IPU. The output of each stage is fed to the input of \n",
    "the stage that follows it. Execution of the model is serialised. That is, each \n",
    "stage is executed in turn while the IPUs allocated to other stages remain idle.\n",
    "\n",
    "![Sharding outline](images/sharding_outline.png)\n",
    "\n",
    "Refer to the technical note on TensorFlow Model Parallelism for full details: \n",
    "[TensorFlow Model Parallelism - Sharding](<https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/sharding.html#sharding>)\n",
    "\n",
    "Model sharding provides a method to run larger models that is conceptually \n",
    "straightforward and might be useful for initial development or debugging. \n",
    "However, it does not offer good utilisation of the allocated IPU resource and, \n",
    "for this reason, sharding is not recommended for production models where \n",
    "performance is critical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec899f1",
   "metadata": {},
   "source": [
    "### 2. Model Parallelism With Pipelining\n",
    "\n",
    "With pipelining, as with sharding, the model is split into stages where each\n",
    "stage can fit and be run on a single IPU. However, unlike sharding, the compute\n",
    "for separate batches is overlapped so that execution of the model is parallelised.\n",
    "That is, each stage (part of the original model) is executed on its IPU while\n",
    "the IPUs allocated to previous stages are already working on subsequent batches.\n",
    "This provides improved utilisation compared to sharding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2075c2cd",
   "metadata": {},
   "source": [
    "![Pipelining outline](images/pipelining_outline.png)\n",
    "\n",
    "Refer to the technical note on TensorFlow Model Parallelism for full details:\n",
    "[TensorFlow Model Parallelism - Pipelining](<https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html#pipelining>)\n",
    "\n",
    "Pipelining provides a method to run larger models that is conceptually less\n",
    "straightforward compared to sharding. However, it offers better utilisation of\n",
    "the allocated IPU resource and, for this reason, pipelining is recommended\n",
    "where performance is critical.\n",
    "\n",
    "This tutorial focuses on how to apply pipelining in TensorFlow 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b81deb",
   "metadata": {},
   "source": [
    "### Pipeline Execution Phases\n",
    "\n",
    "It is important to understand the key phases of pipeline execution:\n",
    "\n",
    "1. Ramp up -  the pipeline is being filled; work is flowing into each stage \n",
    "until all stages are filled (all IPUs are busy).\n",
    "2. Main execution - all stages are filled and IPU utilisation is maximised.\n",
    "3. Ramp down - the pipeline is being drained; work is flowing out of each stage \n",
    "until all stages are empty (no IPUs are busy).\n",
    "4. Weight updates - all pipeline batches have been processed, so accumulated \n",
    "gradients can be processed (gradient descent) and weights updated.\n",
    "\n",
    "Note:  \n",
    "\n",
    "* Each individual batch passed through the pipeline is called a **mini-batch**.  \n",
    "* Weights are updated only once a set of mini-batches has been fully processed.  \n",
    "* Gradients are accumulated across a set of mini-batches.  \n",
    "* Weight updates are applied once all the complete set of mini-batches are \n",
    "processed.  \n",
    "\n",
    "In short, pipelining enforces **gradient accumulation** where:  \n",
    "\n",
    "`effective batch size = mini-batch size * gradient accumulation count`  \n",
    "\n",
    "Performing gradient accumulation is still valid because summing the gradients \n",
    "across all the examples in a batch immediately and accumulating them over \n",
    "several steps are equivalent.  \n",
    "\n",
    "Increasing the gradient accumulation count has these benefits:\n",
    "\n",
    "1. A smaller proportion of time is spent in the ramp up and ramp down - that is, \n",
    "more time is spent in the main execution phase where maximum utilisation of the \n",
    "IPUs is made.\n",
    "2. Fewer overall weight updates are made, which saves compute time.\n",
    "\n",
    "Here is the pipeline outline extended to show the progress of 16 mini-batches \n",
    "followed by a weight update. Notice that the best utilization of the IPUs is \n",
    "during the main phase and that this is sustained until the last mini-batch enters \n",
    "the pipeline, following which the ramp down begins. Also notice that weight \n",
    "updates are only applied once, following the ramp down (after the pipeline has \n",
    "been drained of all mini-batches).\n",
    "\n",
    "![Execution Phases](images/execution_phases.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e348b982",
   "metadata": {},
   "source": [
    "## Tutorial Walkthrough\n",
    "\n",
    "This tutorial starts with a simple multi-stage model that trains on the MNIST dataset.  \n",
    "\n",
    "Note:  \n",
    "\n",
    "* Your 'real-world' models are probably more complicated, but the techniques \n",
    "learned in this tutorial can still be applied.  \n",
    "* The results you see locally will depend on which IPU hardware you run on and \n",
    "which SDK you are using.  \n",
    "* Similarly, when looking at profiling reports you have generated yourself, \n",
    "they may not look exactly the same as the reports presented in this tutorial.  \n",
    "* The `scripts/profile.sh` helper script is used to capture profiling reports; \n",
    "this will  \n",
    "  a) override the application parameters to run just a single step without \n",
    "  repeat and with constrained batch accumulation,  \n",
    "  b) enable autoReport to capture all reports,  \n",
    "  c) enable synthetic data to remove host IO from the execution trace.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7688994",
   "metadata": {},
   "source": [
    "### Tutorial Step 1: The Existing Single IPU Application\n",
    "\n",
    "The code we will start with can also be found [`step1_single_ipu.py`](step1_single_ipu.py).\n",
    "\n",
    "Here is a slightly simplified version of it. Take a look at the code and \n",
    "familiarise yourself with it. \n",
    "\n",
    "We start with importing all necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6caa65be",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.python import ipu\n",
    "\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fcfcba",
   "metadata": {},
   "source": [
    "If you see output something like this below then it means that you forgot to \n",
    "install the Graphcore TensorFlow 1 wheel. See [the Requirements section](#requirements).\n",
    "\n",
    "```\n",
    "Traceback (most recent call last):\n",
    "  File \"step1_single_ipu.py\", line 6, in <module>\n",
    "    import tensorflow.compat.v1 as tf\n",
    "ModuleNotFoundError: No module named 'tensorflow'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba147f",
   "metadata": {},
   "source": [
    "Set hyperparameters, if you want to run the script with different ones remember\n",
    "to rerun all cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091d2e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "REPEAT_COUNT = 160  # The number of times the pipeline will be executed for each step.\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 0.01\n",
    "BATCHES_TO_ACCUMULATE = 16  # How many batches to process before processing gradients and updating weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dcbc53",
   "metadata": {},
   "source": [
    "Create a function responsible for generating a dataset. Remember to set \n",
    "`dataset.batch(drop_remainder=True)` because XLA (and the compiled static IPU \n",
    "graph) expect a complete, fixed sized, set of data as input. Moreover, caching\n",
    "and prefetching are important to prevent the host data feed from being the \n",
    "bottleneck for throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325f45f5",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "def create_dataset(batch_size):\n",
    "    train_data, _ = mnist.load_data()\n",
    "\n",
    "    def normalise(x, y):\n",
    "        return x.astype(\"float32\") / 255.0, y.astype(\"int32\")\n",
    "\n",
    "    x_train, y_train = normalise(*train_data)\n",
    "\n",
    "    def generator():\n",
    "        return zip(x_train, y_train)\n",
    "\n",
    "    types = (x_train.dtype, y_train.dtype)\n",
    "    shapes = (x_train.shape[1:], y_train.shape[1:])\n",
    "\n",
    "    num_examples = len(x_train)\n",
    "    dataset = tf.data.Dataset.from_generator(generator, types, shapes)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.shuffle(num_examples)\n",
    "    dataset = dataset.cache()\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return num_examples, dataset\n",
    "\n",
    "\n",
    "num_examples, dataset = create_dataset(batch_size=BATCH_SIZE)\n",
    "num_train_examples = int(EPOCHS * num_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaac98f",
   "metadata": {},
   "source": [
    "Create the data queues from/to IPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47285ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "infeed_queue = ipu.ipu_infeed_queue.IPUInfeedQueue(dataset)\n",
    "outfeed_queue = ipu.ipu_outfeed_queue.IPUOutfeedQueue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf58af8",
   "metadata": {},
   "source": [
    "With batch size equal to `BATCH_SIZE` and repeat count set to `REPEAT_COUNT`,\n",
    "at every step `n = BATCH_SIZE * REPEAT_COUNT` examples are used. Ensure we \n",
    "process a whole multiple of the batch accumulation count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51dc390",
   "metadata": {},
   "outputs": [],
   "source": [
    "remainder = REPEAT_COUNT % BATCHES_TO_ACCUMULATE\n",
    "if remainder > 0:\n",
    "    REPEAT_COUNT += BATCHES_TO_ACCUMULATE - remainder\n",
    "    print(f'Rounding up repeat count to whole multiple of '\n",
    "          f'batches-to-accumulate (== {REPEAT_COUNT})')\n",
    "examples_per_step = BATCH_SIZE * REPEAT_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803bcaf",
   "metadata": {},
   "source": [
    "In order to evaluate at least N total examples, do ceil(N / n) steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095b157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = (num_train_examples + examples_per_step - 1) // examples_per_step\n",
    "training_samples = steps * examples_per_step\n",
    "print(f'Steps {steps} x examples per step {examples_per_step} '\n",
    "      f'(== {training_samples} training examples, {training_samples / num_examples:.2f} '\n",
    "      f'epochs of {num_examples} examples)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1529a8ae",
   "metadata": {},
   "source": [
    "Now we will compile the learning rate and create the model ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a051a63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('cpu'):\n",
    "    learning_rate = tf.placeholder(np.float32, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4e91a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(learning_rate, images, labels):\n",
    "    # Receiving images,labels (x BATCH_SIZE) via infeed.\n",
    "    # The scoping here helps clarify the execution trace when using --profile.\n",
    "    with tf.variable_scope(\"flatten\"):\n",
    "        activations = layers.Flatten()(images)\n",
    "\n",
    "    with tf.variable_scope(\"dense256\"):\n",
    "        activations = layers.Dense(256, activation=tf.nn.relu)(activations)\n",
    "\n",
    "    with tf.variable_scope(\"dense128\"):\n",
    "        activations = layers.Dense(128, activation=tf.nn.relu)(activations)\n",
    "\n",
    "    with tf.variable_scope(\"dense64\"):\n",
    "        activations = layers.Dense(64, activation=tf.nn.relu)(activations)\n",
    "\n",
    "    with tf.variable_scope(\"dense32\"):\n",
    "        activations = layers.Dense(32, activation=tf.nn.relu)(activations)\n",
    "\n",
    "    with tf.variable_scope(\"logits\"):\n",
    "        logits = layers.Dense(10)(activations)\n",
    "\n",
    "    with tf.variable_scope(\"softmax_ce\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=labels, logits=logits)\n",
    "\n",
    "    with tf.variable_scope(\"mean\"):\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "    with tf.variable_scope(\"optimizer\"):\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=learning_rate)\n",
    "        if BATCHES_TO_ACCUMULATE > 1:\n",
    "            optimizer = ipu.optimizers.GradientAccumulationOptimizerV2(\n",
    "                optimizer, num_mini_batches=BATCHES_TO_ACCUMULATE)\n",
    "        train_op = optimizer.minimize(loss=loss)\n",
    "\n",
    "    return learning_rate, train_op, outfeed_queue.enqueue(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512aa4cd",
   "metadata": {},
   "source": [
    "Create functions that runs the training step `REPEAT_COUNT` times by iterating \n",
    "the model in an IPU repeat loop, then compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5a4e86",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "def loop_repeat_model(learning_rate):\n",
    "    r = ipu.loops.repeat(REPEAT_COUNT, model, [learning_rate], infeed_queue)\n",
    "    return r\n",
    "\n",
    "\n",
    "with ipu.scopes.ipu_scope(\"/device:IPU:0\"):\n",
    "    compiled_model = ipu.ipu_compiler.compile(loop_repeat_model,\n",
    "                                              inputs=[learning_rate])\n",
    "outfeed_op = outfeed_queue.dequeue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5605f0a1",
   "metadata": {},
   "source": [
    "Configure the IPU.  The auto_select_ipus option is used to set the limit of \n",
    "used IPUs. In this example we need only one IPU, however this parameter could \n",
    "be changed in order to obtain data parallelism. If you are interested you can \n",
    "find more information in [documentation](https://docs.graphcore.ai/projects/tensorflow1-user-guide/en/latest/perf_training.html?#selecting-the-number-of-replicas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec514f7a",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "ipu.utils.move_variable_initialization_to_cpu()\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "ipu_configuration = ipu.config.IPUConfig()\n",
    "ipu_configuration.auto_select_ipus = 1\n",
    "ipu_configuration.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235553e7",
   "metadata": {},
   "source": [
    "We are ready to start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ad3779",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize\n",
    "        sess.run(init_op)\n",
    "        sess.run(infeed_queue.initializer)\n",
    "        # Run\n",
    "        begin = time.time()\n",
    "        for step in range(steps):\n",
    "            sess.run(compiled_model, {learning_rate: LEARNING_RATE})\n",
    "            # Read the outfeed for the training losses\n",
    "            losses = sess.run(outfeed_op)\n",
    "            epoch = float(examples_per_step * step / num_examples)\n",
    "            if step == (steps - 1) or (step % 10) == 0:\n",
    "                print(\"Step {}, Epoch {:.1f}, Mean loss: {:.3f}\".format(\n",
    "                    step, epoch, np.mean(losses)))\n",
    "        end = time.time()\n",
    "        elapsed = end - begin\n",
    "        samples_per_second = training_samples / elapsed\n",
    "        print(\"Elapsed {:.2f}, {:.2f} samples/sec\".format(elapsed,\n",
    "                                                          samples_per_second))\n",
    "\n",
    "\n",
    "train()\n",
    "print(\"Stage 1 ran successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9043d5",
   "metadata": {},
   "source": [
    "You can also run it with `$ python3 step1_single_ipu.py`.\n",
    "\n",
    "The model is running on a single IPU without pipelining. You should see it train \n",
    "with output similar to this below:\n",
    "\n",
    "```\n",
    "$ python3 step1_single_ipu.py\n",
    "<CUT>\n",
    "Steps 586 x examples per step 5120 (== 3000320 training examples, 50.00 epochs of 60000 examples)\n",
    "<CUT>\n",
    "Step 0, Epoch 0.0, Mean loss: 2.157\n",
    "Step 10, Epoch 0.9, Mean loss: 0.351\n",
    "<CUT>\n",
    "Step 585, Epoch 49.9, Mean loss: 0.001\n",
    "Elapsed <CUT>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea5f742",
   "metadata": {},
   "source": [
    "This is the model outline:\n",
    "\n",
    "![Model Schematic](images/model_schematic.png)\n",
    "\n",
    "The loss is optimized using `GradientDescentOptimizer` and `GradientAccumulationOptimizerV2`:\n",
    "\n",
    "```python\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "if BATCHES_TO_ACCUMULATE > 1:\n",
    "    optimizer = ipu.optimizers.GradientAccumulationOptimizerV2(\n",
    "            optimizer, num_mini_batches=BATCHES_TO_ACCUMULATE)\n",
    "train_op = optimizer.minimize(loss=loss)\n",
    "```\n",
    "\n",
    "The `GradientAccumulationOptimizerV2` is a wrapper for an optimizer where \n",
    "instead of performing the weight update for every batch, gradients across \n",
    "multiple batches are accumulated. After multiple batches have been processed, \n",
    "their accumulated gradients are used to compute the weight update. The \n",
    "effective batch size is the product of the model batch size and the gradient \n",
    "accumulation count. The `GradientAccumulationOptimizerV2` optimizer can be \n",
    "used to wrap any other TensorFlow optimizer. In this case it is wrapping \n",
    "a `GradientDescentOptimizer`. \n",
    "\n",
    "See the TensorFlow 1 API documentation for details: [GradientAccumulationOptimizerV2](<https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#tensorflow.python.ipu.optimizers.GradientAccumulationOptimizerV2>)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f386fa2",
   "metadata": {},
   "source": [
    "Generate a profile report into directory `./profile_step1_single_ipu` with:\n",
    "\n",
    "`$ scripts/profile.sh step1_single_ipu.py`\n",
    "\n",
    "Use PopVision Graph Analyser to view the execution trace.\n",
    "Remember that the `profile.sh` script limits the execution to a single batch.\n",
    "\n",
    "If this is your first time using the PopVision Graph Analyser, then see the user \n",
    "guide: [PopVision User Guide](<https://docs.graphcore.ai/projects/graphcore-popvision-user-guide/en/latest/index.html#popvision-user-guide>)\n",
    "\n",
    "You should see something like this (zoomed in to show a single mini-batch):\n",
    "\n",
    "![Execution trace](images/step1_single_ipu_execution_trace.png)\n",
    "\n",
    "The key points to note are:\n",
    "\n",
    "* IPU0 runs all layers from `flatten` to `softmax_ce` and the optimizer.  \n",
    "* Because `GradientAccumulationOptimizerV2` is being used, the gradient descent \n",
    "is deferred until `BATCHES_TO_ACCUMULATE` mini-batches have been processed.\n",
    "* The application uses `with tf.variable_scope(...)` to declare a context manager\n",
    "for each layer; variables and ops inherit the scope name, which provides useful \n",
    "context in the Graph Analyser.\n",
    "\n",
    "Scroll to the far right in Graph Analyser to see the gradient descent step:\n",
    "\n",
    "![Execution trace - Gradient Descent](images/step1_single_ipu_execution_trace_gradient_descent.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb8e6a",
   "metadata": {},
   "source": [
    "### Tutorial Step 2: Running The Model On Multiple IPUs Using Sharding\n",
    "\n",
    "Let's look at how we can shard a model to run it on multiple IPUs **without**\n",
    "pipelining.\n",
    "\n",
    "This model outline shows how the operations will be allocated to shards:\n",
    "\n",
    "![Model Schematic (sharded)](images/model_schematic_sharded.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3dbb995",
   "metadata": {},
   "source": [
    "#### Tutorial Step 2: Code Changes\n",
    "Now we will modify the model code so it will be divided between two shards.\n",
    "In the beginning we add two sharding scopes:\n",
    "- `ipu.scopes.ipu_shard(0)` - this will be the context for the set of layers \n",
    "that will end up running on IPU0\n",
    "- `ipu.scopes.ipu_shard(1)` - this will be the context for the set of layers \n",
    "that will end up running on IPU1\n",
    "\n",
    "Then, we split the model across the two scopes, so that IPU0 runs layers \n",
    "`flatten` to `dense64` and IPU1 runs layers  `dense32` to `softmax_ce` plus \n",
    "the optimizer.\n",
    "\n",
    "After this change is applied, the model definition function should look like\n",
    "this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b6994a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(learning_rate, images, labels):\n",
    "    # Receiving images,labels (x BATCH_SIZE) via infeed.\n",
    "    # The scoping here helps clarify the execution trace when using --profile.\n",
    "\n",
    "    with ipu.scopes.ipu_shard(0):\n",
    "        with tf.variable_scope(\"flatten\"):\n",
    "            activations = layers.Flatten()(images)\n",
    "        with tf.variable_scope(\"dense256\"):\n",
    "            activations = layers.Dense(256, activation=tf.nn.relu)(activations)\n",
    "        with tf.variable_scope(\"dense128\"):\n",
    "            activations = layers.Dense(128, activation=tf.nn.relu)(activations)\n",
    "        with tf.variable_scope(\"dense64\"):\n",
    "            activations = layers.Dense(64, activation=tf.nn.relu)(activations)\n",
    "\n",
    "    with ipu.scopes.ipu_shard(1):\n",
    "        with tf.variable_scope(\"dense32\"):\n",
    "            activations = layers.Dense(32, activation=tf.nn.relu)(activations)\n",
    "        with tf.variable_scope(\"logits\"):\n",
    "            logits = layers.Dense(10)(activations)\n",
    "        with tf.variable_scope(\"softmax_ce\"):\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                labels=labels, logits=logits)\n",
    "        with tf.variable_scope(\"mean\"):\n",
    "            loss = tf.reduce_mean(cross_entropy)\n",
    "        with tf.variable_scope(\"optimizer\"):\n",
    "            optimizer = tf.train.GradientDescentOptimizer(\n",
    "                learning_rate=learning_rate)\n",
    "            if BATCHES_TO_ACCUMULATE > 1:\n",
    "                optimizer = ipu.optimizers.GradientAccumulationOptimizerV2(\n",
    "                    optimizer,\n",
    "                    num_mini_batches=BATCHES_TO_ACCUMULATE)\n",
    "            train_op = optimizer.minimize(loss=loss)\n",
    "        # A control dependency is used here to ensure that\n",
    "        # the train_op is not removed.\n",
    "        with tf.control_dependencies([train_op]):\n",
    "            return learning_rate, outfeed_queue.enqueue(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dbd173",
   "metadata": {},
   "source": [
    "We also have to increase the IPU count from 1 to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386b948f",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "ipu_configuration = ipu.config.IPUConfig()\n",
    "ipu_configuration.auto_select_ipus = 2\n",
    "ipu_configuration.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac15375",
   "metadata": {},
   "source": [
    "We are ready to compile model again and start the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba12595",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "infeed_queue = ipu.ipu_infeed_queue.IPUInfeedQueue(dataset)\n",
    "outfeed_queue = ipu.ipu_outfeed_queue.IPUOutfeedQueue()\n",
    "\n",
    "with ipu.scopes.ipu_scope(\"/device:IPU:0\"):\n",
    "    compiled_model = ipu.ipu_compiler.compile(loop_repeat_model,\n",
    "                                              inputs=[learning_rate])\n",
    "\n",
    "outfeed_op = outfeed_queue.dequeue()\n",
    "\n",
    "ipu.utils.move_variable_initialization_to_cpu()\n",
    "init_op = tf.global_variables_initializer()\n",
    "\n",
    "train()\n",
    "print(\"Stage 2 ran successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eccab2",
   "metadata": {},
   "source": [
    "You should see it train with output something like this below.\n",
    "\n",
    "```\n",
    "$ python3 step2_sharding.py\n",
    "<CUT>\n",
    "Steps 586 x examples per step 5120 (== 3000320 training examples, 50.00 epochs of 60000 examples)\n",
    "<CUT>\n",
    "Step 0, Epoch 0.0, Mean loss: 2.184\n",
    "Step 10, Epoch 0.9, Mean loss: 0.362\n",
    "<CUT>\n",
    "Step 585, Epoch 49.9, Mean loss: 0.001\n",
    "Elapsed <CUT>\n",
    "```\n",
    "\n",
    "Complete working example also can be found in `answers/step2_sharding.py`,\n",
    "you can run the application with the following shell command:\n",
    "\n",
    "`$ python3 step2_sharding.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe14bf0",
   "metadata": {},
   "source": [
    "Generate a profile report into directory `./profile_step2_sharding` with:\n",
    "\n",
    "`$ scripts/profile.sh step2_sharding.py`\n",
    "\n",
    "Use PopVision Graph Analyser to view the execution trace.\n",
    "\n",
    "You should see something like this (zoomed in to show a single mini-batch):\n",
    "\n",
    "![Execution trace](images/step2_sharding_execution_trace.png)\n",
    "\n",
    "The key points to note are:\n",
    "\n",
    "* IPU0 runs layers `flatten` to `dense64`.\n",
    "* IPU1 runs layers `dense32` to `softmax_ce` and the optimizer.\n",
    "* It is not efficient because execution is serialised (there is poor\n",
    "  utilisation).\n",
    "* Because `GradientAccumulationOptimizerV2` is being used, the gradient descent\n",
    "  is deferred until `BATCHES_TO_ACCUMULATE` mini-batches have been\n",
    "  processed.\n",
    "* In this specific captured example, the gradients are calculated entirely on\n",
    "  IPU1.\n",
    "\n",
    "Also, note that for a small model such as this one, that fits on a single IPU,\n",
    "sharding does not bring any performance advantages. In fact, because data\n",
    "exchange between IPUs is slower than data exchange within an IPU, the\n",
    "performance will be worse than that without sharding.\n",
    "\n",
    "The completed code for this step can be found\n",
    "here: [`answers/step2_sharding.py`](answers/step2_sharding.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf23be1",
   "metadata": {},
   "source": [
    "### Tutorial Step 3: Using pipelining for better IPU utilization\n",
    "\n",
    "Let's look at how we can modify the application to run the model on multiple\n",
    "IPUs **with** pipelining.\n",
    "\n",
    "This model outline shows how the operations will be defined as layers and\n",
    "allocated to stages:\n",
    "\n",
    "![Model Schematic (pipelined)](images/model_schematic_pipelined.png)\n",
    "\n",
    "First, let's take a look at the TensorFlow 1 pipelining API.\n",
    "\n",
    "#### The TensorFlow 1 Pipelining API\n",
    "\n",
    "Here is an example of how the pipelining API can be used. The parameters used\n",
    "here are for illustrative purposes only.\n",
    "\n",
    "```python\n",
    "from tensorflow.python.ipu.ops import pipelining_ops\n",
    "\n",
    "...\n",
    "pipeline_op = pipelining_ops.pipeline(\n",
    "    computational_stages=[stage1, stage2, stage3, stage4],\n",
    "    gradient_accumulation_count=8,\n",
    "    repeat_count=2,\n",
    "    inputs=[learning_rate],\n",
    "    infeed_queue=infeed_queue,\n",
    "    outfeed_queue=outfeed_queue,\n",
    "    optimizer_function=optimizer_function,\n",
    "    pipeline_schedule=PipelineSchedule.Grouped,\n",
    "    outfeed_loss=True,\n",
    "    name=\"Pipeline\")\n",
    "...\n",
    "```\n",
    "\n",
    "For convenience, the key parameters are described here inline:\n",
    "\n",
    "* `computation_stages`: a list of Python functions, where each function\n",
    "  represents a computational pipeline stage. Each function takes the outputs of\n",
    "  the previous pipeline stage as its inputs.\n",
    "* `gradient_accumulation_count`: the number of times each pipeline stage will\n",
    "  be executed.\n",
    "* `repeat_count`: the number of times the pipeline will be executed for each\n",
    "  session `run()` call.\n",
    "* `inputs`: arguments passed to the first pipeline stage.\n",
    "* `infeed_queue`: optional IPUInfeedQueue, if passed, it is dequeued and passed\n",
    "  as an input in the first pipeline stage.\n",
    "* `outfeed_queue`: IPUOutfeedQueue, required if the last computational stage\n",
    "  has any outputs. The outputs of these are enqueued to this queue and they can\n",
    "  be accessed on the host.\n",
    "* `optimizer_function`: optional Python function which takes the output of the\n",
    "  last computational stage as parameters and returns an instance\n",
    "  of `pipelining_ops.OptimizerFunctionOutput` in order to generate the\n",
    "  back-propagation and weight-update parts of the model suitable for training.\n",
    "* `pipeline_schedule`: which scheduling algorithm to use for pipeline lowering.\n",
    "  Defaults to `PipelineSchedule.Grouped` (\n",
    "  See [the Scheduling section](#scheduling).).\n",
    "* `outfeed_loss`: if True, the loss given by the optimizer_function will be\n",
    "  enqueued on the outfeed, instead of the outputs from the last computational\n",
    "  stage.\n",
    "\n",
    "See the TensorFlow 1 API documentation for\n",
    "details: [TensorFlow 1 Pipeline Operator](<https://docs.graphcore.ai/projects/tensorflow1-user-guide/en/latest/api.html#tensorflow.python.ipu.pipelining_ops.pipeline>)\n",
    "\n",
    "#### Scheduling\n",
    "\n",
    "The precise sequencing of forward and backward passes for the mini-batches can\n",
    "be adjusted using the scheduling parameter `pipeline_schedule`.  \n",
    "The differences are most significant when training and you may need to\n",
    "experiment to find which method works best for your model.\n",
    "\n",
    "Refer to the technical note on TensorFlow Model Parallelism for full\n",
    "details: [TensorFlow Model Parrallelism - Pipeline Scheduling](<https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html#pipeline-scheduling>)\n",
    "\n",
    "##### Sequential Scheduling\n",
    "\n",
    "When sequential scheduling is used, the pipelined stages are serialised so only\n",
    "one stage is being executed at any given point in time.  \n",
    "This removes the overlapped execution and so reduces IPU utilisation in the\n",
    "same way that sharded parallelism does. It can be used during debugging of the\n",
    "model.\n",
    "\n",
    "![Sequential Scheduling](images/sequential_scheduling.png)\n",
    "\n",
    "##### Interleaved Scheduling\n",
    "\n",
    "When interleaved scheduling is used, the pipelined stages run forward and\n",
    "backward passes in an alternating pattern. IPU utilisation may not be as\n",
    "optimal since a forward pass usually requires fewer cycles than a backwards\n",
    "pass.\n",
    "\n",
    "![Interleaved Scheduling](images/interleaved_scheduling.png)\n",
    "\n",
    "##### Grouped Scheduling\n",
    "\n",
    "When grouped scheduling is used, then at any given point in time, the pipelined\n",
    "stages are either all running their forward pass or all running their backward\n",
    "pass. IPU utilisation is more likely to be balanced compared to interleaved\n",
    "scheduling since all IPUs are running either forward pass compute or backward\n",
    "pass compute. Memory requirements may go up when compared to interleaved\n",
    "scheduling to cover an increase in 'in-flight' mini-batches.\n",
    "\n",
    "This is the default scheduling mode.\n",
    "\n",
    "![Grouped Scheduling](images/grouped_scheduling.png)\n",
    "\n",
    "To better understand the increase in memory requirements, see the Grouped\n",
    "Scheduling diagram here:\n",
    "\n",
    "![Grouped Scheduling](images/grouped_schedule_detail.png)\n",
    "\n",
    "Follow the first mini-batch from time-step 1 through the pipeline. Observe that\n",
    "it completes its backwards pass at time-step 11.\n",
    "\n",
    "Compare this with the Interleaved Scheduling diagram here:\n",
    "\n",
    "![Interleaved Scheduling](images/interleaved_schedule_detail.png)\n",
    "\n",
    "Follow the first mini-batch from time step 1 through the pipeline. Observe that\n",
    "it completes its backwards pass at time-step 8.\n",
    "\n",
    "This demonstrates the inherent increase of inflight mini-batches and therefore\n",
    "memory when using Grouped Scheduling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47eaf1fb",
   "metadata": {},
   "source": [
    "#### Tutorial Step 3: Code Changes\n",
    "Now, let's apply this to our application. We will modify the code from Step 2.\n",
    "Firstly, we break down the single model definition `model` into a series of layer\n",
    "definitions; the first layer is `layer1_flatten`.\n",
    "\n",
    "Remember, the pipelining API takes a series of Python functions\n",
    "as `computational_stages`; to support this, first we will make each layer its\n",
    "own Python function.  \n",
    "The first layer must take the explicit `learning_rate` (learning rate) argument\n",
    "and implicit (feed) arguments `images` and `labels` as arguments.  \n",
    "It should return the same `learning_rate`, layer output (`activations`), and \n",
    "`labels`. These will be fed to the next layer as input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29366bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer1_flatten(learning_rate, images, labels):\n",
    "    with tf.variable_scope(\"flatten\"):\n",
    "        activations = layers.Flatten()(images)\n",
    "        return learning_rate, activations, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a86d8f",
   "metadata": {},
   "source": [
    "Then, we add the next layer definition, `layer_dense256`.\n",
    "\n",
    "Each subsequent layer must take the outputs of the previous layer as\n",
    "arguments.  \n",
    "This dense layer should return the `learning_rate`, layer output \n",
    "(`activations`), and `labels`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80a1fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer2_dense256(learning_rate, activations, labels):\n",
    "    with tf.variable_scope(\"flatten\"):\n",
    "        activations = layers.Dense(256, activation=tf.nn.relu)(activations)\n",
    "        return learning_rate, activations, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a573c1",
   "metadata": {},
   "source": [
    "Continue similarly for layers 3,4,5 (`layer3_dense128`, `layer4_dense64`\n",
    ", `layer5_dense32`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer3_dense128(learning_rate, activations, labels):\n",
    "    with tf.variable_scope(\"dense128\"):\n",
    "        activations = layers.Dense(128, activation=tf.nn.relu)(activations)\n",
    "        return learning_rate, activations, labels\n",
    "\n",
    "\n",
    "def layer4_dense64(learning_rate, activations, labels):\n",
    "    with tf.variable_scope(\"dense64\"):\n",
    "        activations = layers.Dense(64, activation=tf.nn.relu)(activations)\n",
    "        return learning_rate, activations, labels\n",
    "\n",
    "\n",
    "def layer5_dense32(learning_rate, activations, labels):\n",
    "    with tf.variable_scope(\"dense32\"):\n",
    "        activations = layers.Dense(32, activation=tf.nn.relu)(activations)\n",
    "        return learning_rate, activations, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76cda557",
   "metadata": {},
   "source": [
    "Add layer 6 `layer6_logits`. Here we take activations and return logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03743bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer6_logits(learning_rate, activations, labels):\n",
    "    with tf.variable_scope(\"logits\"):\n",
    "        logits = layers.Dense(10)(activations)\n",
    "        return learning_rate, logits, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6019d2",
   "metadata": {},
   "source": [
    "For the cross entropy loss and mean, let's combine these into a single\n",
    "cross-entropy loss layer, `layer7_cel`.\n",
    "\n",
    "This layer must take the outputs from `layer6_logits` as arguments.  \n",
    "It should return just `learning_rate` and final `loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d03d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer7_cel(learning_rate, logits, labels):\n",
    "    with tf.variable_scope(\"softmax_ce\"):\n",
    "        cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels=labels, logits=logits)\n",
    "    with tf.variable_scope(\"mean\"):\n",
    "        loss = tf.reduce_mean(cross_entropy)\n",
    "        return learning_rate, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9cb36",
   "metadata": {},
   "source": [
    "We add an optimizer function, `optimizer_function`.\n",
    "\n",
    "This function must take the outputs from the last layer, `layer7_cel`, as\n",
    "arguments; these are `learning_rate` and `loss`.   \n",
    "Return a `tf.train.GradientDescentOptimizer` with argument\n",
    "`learning_rate=learning_rate`.  \n",
    "Return the optimizer and the loss wrapped\n",
    "with `ipu.pipelining_ops.OptimizerFunctionOutput`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4fc9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizer_function(learning_rate, loss):\n",
    "    # Optimizer function used by the pipeline to automatically set up\n",
    "    # the gradient accumulation and weight update steps\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "    return ipu.pipelining_ops.OptimizerFunctionOutput(optimizer, loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b0d3e9",
   "metadata": {},
   "source": [
    "When the pipeline API is used, we specify repeat count and gradient\n",
    "accumulation count so the explicit `loop_repeat_model` wrapper is redundant.  \n",
    "We will implement `pipelined_model` to define two stages and return these via\n",
    "the pipeline API.  \n",
    "The following shows what this should look like with inset comments:\n",
    "\n",
    "```python\n",
    "def pipelined_model(learning_rate):\n",
    "# Defines a pipelined model which is split accross two stages\n",
    "```\n",
    "\n",
    "Within `pipelined_model`, we need a definition for 'stage1', that must:\n",
    "\n",
    "- take the initial arguments (`learning_rate`, `images` and `labels`) as input.\n",
    "- issue the previously defined layers `layer1_flatten` through\n",
    "  to `layer4_dense64` in sequence.\n",
    "- pass the returns of each layer into the next layer.\n",
    "- return the results of the final layer.\n",
    "\n",
    "```python\n",
    "    def stage1(learning_rate, images, labels):\n",
    "    r = layer1_flatten(learning_rate, images, labels)\n",
    "    r = layer2_dense256(*r)\n",
    "    r = layer3_dense128(*r)\n",
    "    r = layer4_dense64(*r)\n",
    "    return r\n",
    "```\n",
    "\n",
    "Within `pipelined_model`, we need a definition for the next stage, 'stage2', that \n",
    "must:\n",
    "\n",
    "- take the outputs returned from 'stage1' as input.\n",
    "- issue the previously defined layers `layer5_dense32` through to `layer7_cel`\n",
    "  in sequence.\n",
    "- pass the returns of each layer into the next layer.\n",
    "- return the results of the final layer.\n",
    "\n",
    "```python\n",
    "    def stage2(*r):\n",
    "    r = layer5_dense32(*r)\n",
    "    r = layer6_logits(*r)\n",
    "    r = layer7_cel(*r)\n",
    "    return r\n",
    "```\n",
    "\n",
    "Within `pipelined_model`, we will add the pipeline operation itself using\n",
    "the `ipu.pipelining_ops.pipeline` API.  \n",
    "\n",
    "Complete code for this function is below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79171da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipelined_model(learning_rate):\n",
    "    # Defines a pipelined model which is split accross two stages\n",
    "\n",
    "    def stage1(learning_rate, images, labels):\n",
    "        r = layer1_flatten(learning_rate, images, labels)\n",
    "        r = layer2_dense256(*r)\n",
    "        r = layer3_dense128(*r)\n",
    "        r = layer4_dense64(*r)\n",
    "        return r\n",
    "\n",
    "    def stage2(*r):\n",
    "        r = layer5_dense32(*r)\n",
    "        r = layer6_logits(*r)\n",
    "        r = layer7_cel(*r)\n",
    "        return r\n",
    "\n",
    "    pipeline_op = ipu.pipelining_ops.pipeline(\n",
    "        computational_stages=[stage1, stage2],\n",
    "        gradient_accumulation_count=BATCHES_TO_ACCUMULATE,\n",
    "        repeat_count=REPEAT_COUNT,\n",
    "        inputs=[learning_rate],\n",
    "        infeed_queue=infeed_queue,\n",
    "        outfeed_queue=outfeed_queue,\n",
    "        optimizer_function=optimizer_function,\n",
    "        pipeline_schedule=ipu.pipelining_ops.PipelineSchedule.Grouped,\n",
    "        outfeed_loss=True,\n",
    "        name=\"Pipeline\")\n",
    "    return pipeline_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721a18c",
   "metadata": {},
   "source": [
    "Explanation of used parameters:\n",
    "\n",
    "`computational_stages` - specify the previously defined 'stage1' and '\n",
    "stage2'.  \n",
    "`gradient_accumulation_count` - specify the existing\n",
    "BATCHES_TO_ACCUMULATE.  \n",
    "`repeat_count` - specify the existing REPEAT_COUNT.  \n",
    "`inputs` - specify input arguments that are additional to those provided by the\n",
    "infeed queue; this is `learning_rate`.  \n",
    "`infeed_queue` - specify the existing infeed_queue.  \n",
    "`outfeed_queue` - specify the existing outfeed queue.  \n",
    "`optimizer_function` - specify the previously defined optimizer function.  \n",
    "`pipeline_schedule` - specify `ipu.pipelining_ops.PipelineSchedule.Grouped`.\n",
    "\n",
    "Notes:\n",
    "\n",
    "* The computational stages, which are executed in sequence, define the points\n",
    "  at which the model is split across IPUs. All model variables which are used\n",
    "  in multiple stages must be passed between IPUs. Consider this detail when\n",
    "  deciding where to put the split points for more complex pipelined models.\n",
    "* Infeeds and outfeeds are introduced\n",
    "  here: [Targeting the IPU from TensorFlow 1](<https://docs.graphcore.ai/projects/tensorflow1-user-guide/en/latest/perf_training.html#training-loops-data-sets-and-feed-queues>).\n",
    "* This tutorial demonstrates how an input, `learning_rate`, can be passed into\n",
    "  the pipeline and propagated through all layers/stages to where it is needed,\n",
    "  in this case, the optimizer. In reality, for this simple\n",
    "  application, `learning rate` is effectively static and could be removed and\n",
    "  hardcoded.\n",
    "  \n",
    "Moreover, we will update step calculation. The pipeline API encapsulates the \n",
    "repeat count and gradient accumulation count.\n",
    "With gradient accumulation count `BATCHES_TO_ACCUMULATE` and repeat count \n",
    "`REPEAT_COUNT`, each session.run() will process \n",
    "`BATCHES_TO_ACCUMULATE * REPEAT_COUNT` batches.\n",
    "\n",
    "Where `examples_per_step` was previously calculated as:\n",
    "\n",
    "```python\n",
    "# With batch size equal to `BATCH_SIZE` and repeat count set to `REPEAT_COUNT`,\n",
    "# at every step n = `BATCH_SIZE * REPEAT_COUNT` examples are used.\n",
    "# Ensure we process a whole multiple of the batch accumulation count.\n",
    "remainder = REPEAT_COUNT % BATCHES_TO_ACCUMULATE\n",
    "if remainder > 0:\n",
    "    REPEAT_COUNT += BATCHES_TO_ACCUMULATE - remainder\n",
    "    print(f'Rounding up repeat count to whole multiple of '\n",
    "          f'batches-to-accumulate (== {REPEAT_COUNT})')\n",
    "examples_per_step = BATCH_SIZE * REPEAT_COUNT\n",
    "```\n",
    "\n",
    "We replace it with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439cd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "REPEAT_COUNT = 10  # Before 160, while BATCHES_TO_ACCUMULATE=16\n",
    "examples_per_step = BATCH_SIZE * BATCHES_TO_ACCUMULATE * REPEAT_COUNT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6690e44f",
   "metadata": {},
   "source": [
    "Note that with batch size equal to `BATCH_SIZE`, gradient accumulation count \n",
    "`BATCHES_TO_ACCUMULATE` and repeat count `REPEAT_COUNT`,\n",
    "at every step `n = BATCH_SIZE * BATCHES_TO_ACCUMULATE * REPEAT_COUNT` examples \n",
    "are used. We also changed the default repeat count to 10 (from 160). The \n",
    "pipelined version will still process 160 batches each step \n",
    "(`BATCHES_TO_ACCUMULATE` 16 * `REPEAT_COUNT` 10 == 160).\n",
    "\n",
    "Compile the new `pipelined_model` instead of `loop_repeat_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078bcc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "infeed_queue = ipu.ipu_infeed_queue.IPUInfeedQueue(dataset)\n",
    "outfeed_queue = ipu.ipu_outfeed_queue.IPUOutfeedQueue()\n",
    "\n",
    "with ipu.scopes.ipu_scope(\"/device:IPU:0\"):\n",
    "    compiled_model = ipu.ipu_compiler.compile(pipelined_model,\n",
    "                                              inputs=[learning_rate])\n",
    "\n",
    "outfeed_op = outfeed_queue.dequeue()\n",
    "\n",
    "ipu.utils.move_variable_initialization_to_cpu()\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a6034e",
   "metadata": {},
   "source": [
    "Modify the IPU config. Add a line to the IPU configuration code to specify \n",
    "`selection_order`. Set it to SNAKE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b4a2d7",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "ipu_configuration = ipu.config.IPUConfig()\n",
    "ipu_configuration.auto_select_ipus = 2\n",
    "ipu_configuration.selection_order = ipu.utils.SelectionOrder.SNAKE\n",
    "ipu_configuration.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0344c248",
   "metadata": {},
   "source": [
    "This configures the logical IPU indexing so that logically adjacent IPUs are\n",
    "physically linked. This makes the exchange of data between these IPUs (pipeline\n",
    "stages) more efficient. The default `selection_order` is AUTO which\n",
    "automatically tries to select the best selection order given the model. You can\n",
    "override it, as we do in this tutorial, if you have a specific requirement and\n",
    "want to be sure it is used.\n",
    "\n",
    "For details, including other options, see\n",
    "the [TensorFlow 1 User Guide - Selection Order](<https://docs.graphcore.ai/projects/tensorflow1-user-guide/en/latest/api.html#tensorflow.python.ipu.config.SelectionOrder>).\n",
    "\n",
    "Now, we can run the modified application. You can also do that by running\n",
    "completed step 3 tutorial from answers directory with the following shell \n",
    "command:\n",
    "\n",
    "```\n",
    "$ python3 step3_pipelining.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5789e88",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "train()\n",
    "print(\"Stage 3 ran successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae9ea4e",
   "metadata": {},
   "source": [
    "You should see it train with output something like this below.\n",
    "\n",
    "```\n",
    "$ python3 step3_pipelining.py\n",
    "<CUT>\n",
    "Steps 586 x examples per step 5120 (== 3000320 training examples, 50.00 epochs of 60000 examples)\n",
    "<CUT>\n",
    "Step 0, Epoch 0.0, Mean loss: 2.174\n",
    "Step 10, Epoch 0.9, Mean loss: 0.425\n",
    "<CUT>\n",
    "Step 585, Epoch 49.9, Mean loss: 0.001\n",
    "Elapsed <CUT>\n",
    "```\n",
    "\n",
    "Generate a profile report into directory `./profile_step3_pipelining` with:\n",
    "\n",
    "`$ scripts/profile.sh step3_pipelining.py`\n",
    "\n",
    "Use PopVision Graph Analyser to view the execution trace.\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "The initial ramp-up phase where the first batches are pushed through the\n",
    "pipeline.  \n",
    "In this phase we can see it takes a while before both IPUs are working\n",
    "simultaneously.\n",
    "\n",
    "![Execution trace](images/step3_pipelining_execution_trace_ramp_up.png)\n",
    "\n",
    "The main execution phase where both IPUs are working simultaneously on\n",
    "different batches.\n",
    "\n",
    "![Execution trace](images/step3_pipelining_execution_trace_full.png)\n",
    "\n",
    "The final ramp-down phase where the gradient descent is applied and weights\n",
    "updated.\n",
    "\n",
    "![Execution trace](images/step3_pipelining_execution_trace_ramp_down.png)\n",
    "\n",
    "The key points to note are:\n",
    "\n",
    "* IPU0 runs layers `flatten` through to `dense64`.\n",
    "* IPU1 runs layers `dense32` through to `softmax_ce`.\n",
    "* The backward passes and weight updates for each layer are performed on the\n",
    "  same IPU as the forward passes.\n",
    "* It is better than sharding because execution is parallelised (utilization is\n",
    "  improved).\n",
    "* Because `ipu.pipelining_ops.pipeline` is being used, the gradient descent is\n",
    "  deferred to the end of the execution (until all mini-batches have been\n",
    "  processed).\n",
    "\n",
    "Also, note:\n",
    "\n",
    "* Although pipelining is better than sharding, the overheads from data exchange\n",
    "  between IPUs still have an impact.\n",
    "* The number of IPUs in a multi-IPU device must be a power of 2. For example,\n",
    "  if you try to select 3 IPUs to run 3 stages then you will see an\n",
    "  error: `Unsupported number of IPUs requested - could not find an IPU device with 3 IPUs`.\n",
    "\n",
    "During the development and tuning of your pipelined model, the general aim is\n",
    "to balance the execution cycles across all the IPUs in the main phase, so that\n",
    "utilisation is maximised.  \n",
    "With the simple example in this tutorial, the default split is already\n",
    "reasonably balanced, but try the extension exercises to see how the\n",
    "balance/behviour can be modified.  \n",
    "These tutorial examples keep the repeat count quite low. For production code\n",
    "you should increase the repeat count in order to maximise the time spent\n",
    "running on the IPU and minimise interactions with the host.\n",
    "\n",
    "#### Tutorial Step 3 : Extension\n",
    "We encourage you to test the training script more deeply using different \n",
    "parameters. You can do this by modifying the parameters in the notebook and \n",
    "running the appropriate cell, but an error-free approach would be to use the \n",
    "full script for training step 3 located in: [`answers/step3_pipelining.py`](answers/step3_pipelining.py).\n",
    "\n",
    "##### Pipeline Schedule\n",
    "\n",
    "Try changing `pipeline_schedule` to `PipelineSchedule.Interleaved`\n",
    "and/or `PipelineSchedule.Sequential`.  \n",
    "Recapture the profile reports and see how the execution trace changes in each\n",
    "case compared to the original version.\n",
    "\n",
    "##### Repeat count\n",
    "\n",
    "Run with different values of `--repeat-count` and observe how that changes\n",
    "performance.\n",
    "\n",
    "##### Stages\n",
    "\n",
    "Try modifying the layers assigned to each stage.  \n",
    "Try splitting the layers up over more stages.  \n",
    "Recapture the profile reports and see how the execution trace changes in each\n",
    "case compared to the original version.\n",
    "\n",
    "The code changes for this step can be found\n",
    "here: [`answers/step3_pipelining.py`](answers/step3_pipelining.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1346e",
   "metadata": {},
   "source": [
    "### Tutorial Step 4: Run-time Configurable Stages\n",
    "\n",
    "During the development and tuning of more complex models there are often\n",
    "multiple hyperparameters that can be adjusted. Even in this tutorial's simple\n",
    "pipelining application, we can adjust the batch size, the gradient accumulation\n",
    "count and the stages. It is easier to experiment with these options if they can\n",
    "be configured from the command line.\n",
    "\n",
    "Batch size and gradient accumulation count are already configurable from the\n",
    "command line (`--batch-size`, `--batches-to-accumulate`) in scripts or from\n",
    "the cell at top of this notebook. But the 'stages' are currently hardcoded by \n",
    "the series of 'stageN' functions defined locally in `pipelined_model`.\n",
    "\n",
    "In this next step we will add code to support run-time configurable stages.\n",
    "There are many ways this could be implemented, but we only demonstrate one\n",
    "method here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a152bad",
   "metadata": {},
   "source": [
    "#### Tutorial Step 4: Code Changes\n",
    "\n",
    "Add a new variable `SPLITS` with which you can specify the points\n",
    "to 'split' the model.\n",
    "\n",
    "This should take multiple string arguments where each argument defines the\n",
    "split point. This is the layer that will be assigned to the *next* stage. The\n",
    "implication is that for N `--split` arguments the model will run on N+1 stages\n",
    "and IPUs. Let's assume that will create a model_layers dictionary from which we\n",
    "can generate a list of layer ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b903ca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ['dense32']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff65164",
   "metadata": {},
   "source": [
    "Now, we add a function `discover_layers` that will return a sorted list of \n",
    "layers.\n",
    "\n",
    "This can use `globals()` to find all functions that match a specific syntax.\n",
    "\n",
    "- Find all functions that match `layer<idx>_<id>`. For example, this should\n",
    "  find `layer1_flatten` and return an entry with idx==1 and id==\"flatten\".\n",
    "- Make each returned layer in the list a dictionary with keys `func`, `id`\n",
    "  and `idx`.\n",
    "- Layers should be sorted by `idx` but do not need to be strictly sequential;\n",
    "  this is so 'gaps' can be left to support insertion of layers when\n",
    "  writing/developing your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a9f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discover_layers():\n",
    "    # This parses global functions with form \"layer<n>_<id>\"\n",
    "    # e.g. \"layer3_BlockA\"\n",
    "    # The layers will be sorted by <n>\n",
    "    # A layer list of dictionaries is returned with:\n",
    "    #   \"func\" : Function reference\n",
    "    #   \"id\"   : String name of layer (==<id>)\n",
    "    layers = []\n",
    "    global_symbols = globals()\n",
    "    prefix = \"layer\"\n",
    "    layer_funcs = [key for key in global_symbols if key.startswith(prefix)]\n",
    "    for layer_func in layer_funcs:\n",
    "        try:\n",
    "            idx_id = layer_func[len(prefix):]\n",
    "            idx, id = idx_id.split(\"_\")\n",
    "            layers.append({\"func\": global_symbols[layer_func],\n",
    "                           \"id\": id, \"idx\": int(idx)})\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def use_idx(e):\n",
    "        return e[\"idx\"]\n",
    "\n",
    "    layers.sort(key=use_idx)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d77286f",
   "metadata": {},
   "source": [
    "Then, we add a function that will take the layer list and `splits` argument and\n",
    "return a list of stages.\n",
    "\n",
    "Each stage should be a list of those layers in that stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8629fb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_layers_to_stages(layers, splits):\n",
    "    # Sequence layers into distinct groups (stages)\n",
    "    # according to splits.\n",
    "    def next_split_layer():\n",
    "        # Returns idx of layer matching id.\n",
    "        if stage >= len(splits):\n",
    "            return None\n",
    "        split = splits[stage]\n",
    "        try:\n",
    "            return next(layer for layer in layers if (layer[\"id\"] == split))\n",
    "        except:\n",
    "            print(\"Failed to match split layer with id \\\"{}\\\"\".format(split))\n",
    "            return None\n",
    "\n",
    "    stages = [[]]\n",
    "    stage = 0\n",
    "    idx = 0\n",
    "    next_split = next_split_layer()\n",
    "    while len(layers):\n",
    "        ly = layers.pop(0)\n",
    "        if ly == next_split:\n",
    "            stage += 1\n",
    "            stages.append([])\n",
    "            next_split = next_split_layer()\n",
    "        stages[stage].append(ly)\n",
    "    return stages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596b066d",
   "metadata": {},
   "source": [
    "Instead of using hard coded stage definitions from `pipelined_model`, we build \n",
    "a list of dynamically generated stages.\n",
    "\n",
    "Add a local helper to `pipelined_model` that will build a stage from the list\n",
    "of associated layers.  \n",
    "The arguments can be sequenced through the layers.\n",
    "```python\n",
    "def make_pipeline_stage(idx, stage):\n",
    "    # Helper that defines a single stage consisting of one or more layers\n",
    "    def _stage(*args):\n",
    "        for layer in stage:\n",
    "            with tf.variable_scope(\"stage\" + str(idx) + \"_\" + layer[\"id\"],\n",
    "                                   use_resource=True):\n",
    "                print(\"Issuing stage {} layer {}\".format(idx, layer[\"id\"]))\n",
    "                args = layer[\"func\"](*args)\n",
    "        return args\n",
    "\n",
    "    return _stage\n",
    "```\n",
    "\n",
    "Build a list of computational_stages that can be passed to the pipelining API.\n",
    "Make each stage (function) and add it to the computational stages:\n",
    "\n",
    "```python\n",
    "computational_stages = []\n",
    "for idx, stage in enumerate(stages):\n",
    "    f = make_pipeline_stage(idx, stage)\n",
    "    computational_stages.append(f)\n",
    "```\n",
    "\n",
    "Update the pipelining APIs `computational_stages` to use the dynamically\n",
    "generated list.\n",
    "\n",
    "```python\n",
    "computational_stages = computational_stages\n",
    "\n",
    "```\n",
    "\n",
    "Complete, updated `pipelined_model` looks like that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a60b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipelined_model(learning_rate):\n",
    "    # Helper that defines a single stage consisting of one or more layers\n",
    "    def make_pipeline_stage(idx, stage):\n",
    "        def _stage(*args):\n",
    "            for layer in stage:\n",
    "                with tf.variable_scope(\"stage\" + str(idx) + \"_\" + layer[\"id\"],\n",
    "                                       use_resource=True):\n",
    "                    print(\"Issuing stage {} layer {}\".format(idx, layer[\"id\"]))\n",
    "                    args = layer[\"func\"](*args)\n",
    "            return args\n",
    "\n",
    "        return _stage\n",
    "\n",
    "    # Make each stage (function) and add it to the computational stages\n",
    "    computational_stages = []\n",
    "    for idx, stage in enumerate(stages):\n",
    "        f = make_pipeline_stage(idx, stage)\n",
    "        computational_stages.append(f)\n",
    "\n",
    "    pipeline_op = ipu.pipelining_ops.pipeline(\n",
    "        computational_stages=computational_stages,\n",
    "        gradient_accumulation_count=BATCHES_TO_ACCUMULATE,\n",
    "        repeat_count=REPEAT_COUNT,\n",
    "        inputs=[learning_rate],\n",
    "        infeed_queue=infeed_queue,\n",
    "        outfeed_queue=outfeed_queue,\n",
    "        optimizer_function=optimizer_function,\n",
    "        pipeline_schedule=ipu.pipelining_ops.PipelineSchedule.Grouped,\n",
    "        outfeed_loss=True,\n",
    "        name=\"Pipeline\")\n",
    "    return pipeline_op"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05eead36",
   "metadata": {},
   "source": [
    "Update the script to use the new features. \n",
    "Add a line to call `discover_layers` and `move_layers_to_stages`;\n",
    "return as `stages`.  \n",
    "The following example also includes some additional logging and error\n",
    "reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_layers = discover_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28040ba8",
   "metadata": {},
   "source": [
    "Show final list of layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff8c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Layers:\")\n",
    "layer_list = [layer[\"id\"] for layer in model_layers]\n",
    "print(\" \" + (\", \".join(layer_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8f721d",
   "metadata": {},
   "source": [
    "Sequence layers into stage-groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8099c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "stages = move_layers_to_stages(model_layers, SPLITS)\n",
    "if (len(stages) != len(SPLITS) + 1):\n",
    "    print(\"Unexpected stage count - check splits are valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edb3998",
   "metadata": {},
   "source": [
    "Note, stage count has to be power of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b94e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_stages = len(stages)\n",
    "num_ipus = int(math.pow(2, math.ceil(math.log(num_stages, 2))))\n",
    "if num_stages != num_ipus:\n",
    "    print(\n",
    "        \"Stage count must be power2 (specified {} versus next power2 {})\".format(\n",
    "            num_stages, num_ipus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d1a922",
   "metadata": {},
   "source": [
    "Show final list of staged layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c7a149",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Stages:\")\n",
    "for idx, stage in enumerate(stages):\n",
    "    layer_list = [layer[\"id\"] for layer in stage]\n",
    "    print(\" \" + str(idx) + \". \" + (\"-\".join(layer_list)))\n",
    "    if len(layer_list) == 0:\n",
    "        print(\"Unexpected empty stage - check splits are valid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaa95a9",
   "metadata": {},
   "source": [
    "Compile new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236e94c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "infeed_queue = ipu.ipu_infeed_queue.IPUInfeedQueue(dataset)\n",
    "outfeed_queue = ipu.ipu_outfeed_queue.IPUOutfeedQueue()\n",
    "\n",
    "with ipu.scopes.ipu_scope(\"/device:IPU:0\"):\n",
    "    compiled_model = ipu.ipu_compiler.compile(pipelined_model,\n",
    "                                              inputs=[learning_rate])\n",
    "\n",
    "outfeed_op = outfeed_queue.dequeue()\n",
    "\n",
    "ipu.utils.move_variable_initialization_to_cpu()\n",
    "init_op = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2692b313",
   "metadata": {},
   "source": [
    "Update the IPU count to match the number of stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc45ee",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "ipu_configuration = ipu.config.IPUConfig()\n",
    "ipu_configuration.auto_select_ipus = len(SPLITS) + 1\n",
    "ipu_configuration.selection_order = ipu.utils.SelectionOrder.SNAKE\n",
    "ipu_configuration.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c3d5cf",
   "metadata": {},
   "source": [
    "Now, run the modified application. You can also do that by running python script\n",
    "with command:\n",
    "\n",
    "```\n",
    "$ python3 step4_configurable_stages.py\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a6e80",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "train()\n",
    "print(\"Stage 4 ran successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28148dd",
   "metadata": {},
   "source": [
    "You should see it train with output something like this below.\n",
    "\n",
    "```\n",
    "$ python3 step4_configurable_stages.py\n",
    "<CUT>\n",
    "Layers:\n",
    " flatten, dense256, dense128, dense64, dense32, logits, cel\n",
    "Stages:\n",
    " 0. flatten-dense256-dense128-dense64\n",
    " 1. dense32-logits-cel\n",
    "Steps 586 x examples per step 5120 (== 3000320 training examples, 50.00 epochs of 60000 examples)\n",
    "Issuing stage 0 layer flatten\n",
    "Issuing stage 0 layer dense256\n",
    "Issuing stage 0 layer dense128\n",
    "Issuing stage 0 layer dense64\n",
    "Issuing stage 1 layer dense32\n",
    "Issuing stage 1 layer logits\n",
    "Issuing stage 1 layer cel\n",
    "<CUT>\n",
    "Step 0, Epoch 0.0, Mean loss: 2.165\n",
    "Step 10, Epoch 0.9, Mean loss: 0.379\n",
    "<CUT>\n",
    "Step 585, Epoch 49.9, Mean loss: 0.001\n",
    "Elapsed <CUT>\n",
    "```\n",
    "\n",
    "Try specifying a different split point. For example, using the following shell\n",
    "command:\n",
    "\n",
    "`python3 step4_configurable_stages.py --splits dense128`\n",
    "\n",
    "Try specifying multiple splits. For example. using the following shell command:\n",
    "\n",
    "`python3 step4_configurable_stages.py --splits dense128 dense64 dense32`\n",
    "\n",
    "The code changes for this step can be found\n",
    "here: [`answers/step4_configurable_stages.py`](answers/step4_configurable_stages.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f68668",
   "metadata": {},
   "source": [
    "## Further Considerations\n",
    "\n",
    "This section calls out some additional features and parameters that can affect\n",
    "performance when pipelining.\n",
    "\n",
    "Always refer to the technical note on TensorFlow pipelining for the most\n",
    "up-to-date and detailed\n",
    "information: [TensorFlow Model Parallelism - Pipelining](<https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html#pipelining>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c061b00",
   "metadata": {},
   "source": [
    "### Recomputation\n",
    "\n",
    "When pipelining, the forward activations of each operation will be saved so\n",
    "that they are available to compute the gradients in the backwards pass. This\n",
    "may require significant memory. If IPU memory is limited, then _recomputation_\n",
    "can be used to reduce the memory used to store activations. With recomputation\n",
    "enabled, activations are only saved for a selected subset of the forward\n",
    "operations. The activations which were not saved will be recomputed as part of\n",
    "the backwards pass. This saves IPU memory at the expense of requiring more\n",
    "compute. Pipelining can be used with a single IPU in order to use\n",
    "recomputation, where using recomputation allows a model and mini-batch size to\n",
    "fit that would otherwise be out-of-memory on the IPU.\n",
    "\n",
    "Refer to the technical note on TensorFlow Model Parallelism for full\n",
    "details: [TensorFlow Model Parallelism - Pipelining Recomputation](<https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html#recomputation>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790dbb7a",
   "metadata": {},
   "source": [
    "### Variable Offloading\n",
    "\n",
    "If IPU memory is limited, then variable offloading can be used. This will store\n",
    "some variables and activations in Streaming Memory. This saves IPU memory at\n",
    "the expense of time to swap the data on and off the IPU.\n",
    "\n",
    "Refer to the technical note on TensorFlow Model Parallelism for full\n",
    "details: [TensorFlow Model Parallelism - Pipelining Variable Offloading](<https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html#variable-offloading>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b3a7d0",
   "metadata": {},
   "source": [
    "### Gradient Accumulation Buffer Data Type\n",
    "\n",
    "Gradients will be accumulated in a buffer when pipelining. The datatype of this\n",
    "buffer can be controlled using the API's `gradient_accumulation_dtype`\n",
    "argument:\n",
    "\n",
    "- `None`: Use an accumulator buffer of the same TensorFlow `DType` as each\n",
    "  variable.\n",
    "- A TensorFlow `DType`: Specify the type to use for all accumulation buffers (\n",
    "  for example, `tf.float16` or `tf.float32`).\n",
    "- A Python `callable`: User provided callback to support a different `DType`\n",
    "  for each variable.\n",
    "\n",
    "The default is `None` (use an accumulator buffer of the same `DType` as each\n",
    "variable). You may want to override this if you are training in float16 and you\n",
    "want to use a float32 accumulator buffer.\n",
    "\n",
    "See the TensorFlow 1 API documentation for\n",
    "details: [TensorFlow1 Pipelining API](<https://docs.graphcore.ai/projects/tensorflow1-user-guide/en/latest/api.html#tensorflow.python.ipu.pipelining_ops.pipeline>)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb21ece",
   "metadata": {},
   "source": [
    "### Data Parallelism\n",
    "\n",
    "It is possible to combine data parallelism with model parallelism by using the\n",
    "standard `CrossReplicaOptimizer`, which is used for training replicated models,\n",
    "in the pipeline optimiser function. In this case:\n",
    "\n",
    "`effective batch size = replication factor * mini-batch size * gradient accumulation count`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48700a55",
   "metadata": {},
   "source": [
    "### IPUPipelineEstimator\n",
    "\n",
    "`IPUPipelineEstimator` is a version of `IPUEstimator` that supports pipelining.\n",
    "These APIs handle many of the details of running on IPUs, such as placement of\n",
    "operations and tensors, graph compilation and usage of data feeds. The model\n",
    "function provided to the `IPUEstimator` API must return an instance\n",
    "of `IPUPipelineEstimatorSpec` that contains the information needed for\n",
    "execution including pipelining specific details such as the computational\n",
    "stages and the gradient accumulation count.\n",
    "\n",
    "The following example is derived from `step3_pipelining.py` and further\n",
    "simplified:\n",
    "\n",
    "```python\n",
    "ipu_estimator = ipu.ipu_pipeline_estimator.IPUPipelineEstimator(\n",
    "config=config,\n",
    "model_fn=model_fn,\n",
    "params={\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"gradient_accumulation_count\": BATCHES_TO_ACCUMULATE\n",
    "    },\n",
    ")\n",
    "\n",
    "ipu_estimator.train(input_fn=input_fn, steps=steps)\n",
    "```\n",
    "\n",
    "Where `input_fn` provides the dataset and `model_fn` provides the model\n",
    "definition. The `model_fn` might look like this:\n",
    "```python\n",
    "def model_fn(mode, params):\n",
    "    if not mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        raise NotImplementedError(mode)\n",
    "\n",
    "    # Defines a pipelined model which is split accross two stages\n",
    "    def stage1(images, labels):\n",
    "        r = layer1_flatten(images, labels)\n",
    "        r = layer2_dense256(*r)\n",
    "        r = layer3_dense128(*r)\n",
    "        r = layer4_dense64(*r)\n",
    "        return r\n",
    "\n",
    "    def stage2(*r):\n",
    "        r = layer5_dense32(*r)\n",
    "        r = layer6_logits(*r)\n",
    "        loss = layer7_cel(*r)\n",
    "        return loss\n",
    "\n",
    "    def optimizer_function(loss):\n",
    "        # Optimizer function used by the pipeline to automatically set up\n",
    "        # the gradient accumulation and weight update steps\n",
    "        optimizer = tf.train.GradientDescentOptimizer(\n",
    "            learning_rate=params[\"learning_rate\"])\n",
    "        return ipu.pipelining_ops.OptimizerFunctionOutput(optimizer, loss)\n",
    "\n",
    "    return ipu.ipu_pipeline_estimator.IPUPipelineEstimatorSpec(\n",
    "        mode,\n",
    "        computational_stages=[stage1, stage2],\n",
    "        optimizer_function=optimizer_function,\n",
    "        gradient_accumulation_count=params[\"gradient_accumulation_count\"])\n",
    "```\n",
    "\n",
    "A complete example is provided\n",
    "here : [`answers/ipu_pipeline_estimator.py`](answers/ipu_pipeline_estimator.py)\n",
    "\n",
    "Run it with `$ python3 answers/ipu_pipeline_estimator.py`\n",
    "\n",
    "See the TensorFlow 1 API documentation for details:  \n",
    "[TensorFlow IPUPipelineEstimator API](<https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#ipupipelineestimator>)  \n",
    "[TensorFlow IPUPipelineEstimatorSpec](<https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#tensorflow.python.ipu.ipu_pipeline_estimator.IPUPipelineEstimatorSpec>)  \n",
    "[TensorFlow IPUPipelineEstimator Example](<https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/ipu_pipeline_estimator_example.html>)  "
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
