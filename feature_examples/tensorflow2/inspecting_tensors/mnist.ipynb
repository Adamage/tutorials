{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "985acebc",
   "metadata": {},
   "source": [
    "Copyright (c) 2020 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2458e456",
   "metadata": {},
   "source": [
    "# TensorFlow 2: Tensor Inspection Techniques\n",
    "\n",
    "In this tutorial you will train a selection of simple fully connected models\n",
    "on the MNIST numeral data set and see how tensors (containing activations\n",
    "and gradients) can be returned to the host via outfeeds for inspection.\n",
    "\n",
    "An outfeed is the counterpart to an infeed and manages the transfer of data \n",
    "(like tensors, tuples or dictionaries of tensors) from the IPU to the host. \n",
    "To learn more about using outfeeds, see [outfeed queues](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#outfeed-queue).\n",
    "\n",
    "Outfeeds can be useful for debugging, but can significantly increase the amount\n",
    "of memory required on the IPU(s). When pipelining, you could use a smaller\n",
    "value for the gradient accumulation count to mitigate this. Also consider using\n",
    "a small number of [steps per execution](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/keras/engine/training.py#L537)\n",
    "to reduce memory footprint. \n",
    "\n",
    "In this demo, filters can be used to return only a subset of the activations\n",
    "and gradients. The outfed information can be returned to a variable or can be\n",
    "printed to the standard output. In [`outfeed_callback.py`](https://github.com/graphcore/tutorials/blob/master/feature_examples/tensorflow2/inspecting_tensors/outfeed_callback.py)\n",
    "the implementation is to print this information to the standard output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523d536c",
   "metadata": {},
   "source": [
    "## How to use this demo\n",
    "\n",
    "### File structure and local imports\n",
    "\n",
    "* `mnist.py` The main Python script.\n",
    "* `mnist_code_only.py` Autogenerated script without any comments.\n",
    "* `mnist.ipynb` Autogenerated interactive Jupyter Notebook tutorial.\n",
    "* `outfeed_callback.py` Contains a custom callback that dequeues an outfeed \n",
    "  queue at the end of every epoch.\n",
    "* `outfeed_layers.py` Custom layers that (selectively) add the inputs \n",
    "  (for example, activations from the previous layer) to a dict that will be \n",
    "  enqueued on an outfeed queue.\n",
    "* `outfeed_optimizer.py` Custom optimizer that outfeeds the gradients generated\n",
    "  by a wrapped optimizer.\n",
    "* `outfeed_wrapper.py` Contains the `MaybeOutfeedQueue` class, see below.\n",
    "* `README.md` Markdown autogenerated file.\n",
    "* `requirements.txt` Required packages for this tutorial\n",
    "* `tests` Subdirectory containing test scripts.\n",
    "\n",
    "### Custom classes descriptions\n",
    "\n",
    "This tutorial uses the following classes, which are implemented in separate\n",
    "[Python files](https://github.com/graphcore/tutorials/tree/master/feature_examples/tensorflow2/inspecting_tensors):\n",
    "\n",
    "* `outfeed_wrapper.MaybeOutfeedQueue` - a wrapper for an IPUOutfeedQueue that \n",
    "  allows key-value pairs to be selectively added to a dictionary that can then \n",
    "  be enqueued.\n",
    "* `outfeed_optimizer.OutfeedOptimizer` - a custom optimizer that enqueues \n",
    "  gradients using a `MaybeOutfeedQueue`, with the choice of whether to enqueue \n",
    "  the gradients after they are computed (the pre-accumulated gradients) or \n",
    "  before they are applied (the accumulated gradients).\n",
    "* `outfeed_layers.Outfeed` - a Keras layer that puts the inputs into \n",
    "  a dictionary and enqueues it on an IPUOutfeedQueue.\n",
    "* `outfeed_layers.MaybeOutfeed` - a Keras layer that uses a MaybeOutfeedQueue \n",
    "  to selectively put the inputs into a dict and optionally enqueues the dict. \n",
    "  At the moment, this layer cannot be used with non-pipelined Sequential models.\n",
    "* `outfeed_callback.OutfeedCallback` - a Keras callback to dequeue an outfeed\n",
    "  queue at the end of every epoch, printing some statistics about the tensors.\n",
    "\n",
    "### Environment preparation\n",
    "\n",
    "Install the Poplar SDK following the instructions in the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html)\n",
    "guide for your IPU system. Make sure to run the `enable.sh` scripts for Poplar \n",
    "and PopART and activate a Python3 virtualenv with PopTorch installed.\n",
    "Then install the package requirements:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187ae5c1",
   "metadata": {},
   "source": [
    "### Required imports\n",
    ">**Note**\n",
    ">The Graphcore TensorFlow 2 wheel is bundled with Graphcore Poplar SDK. Please\n",
    ">ensure you install this wheel rather than the default public wheel, as it \n",
    ">contains IPU specific functionality in the `ipu` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc56bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python import ipu\n",
    "\n",
    "from outfeed_callback import OutfeedCallback\n",
    "from outfeed_optimizer import OutfeedOptimizer, OutfeedOptimizerMode\n",
    "import outfeed_layers\n",
    "from outfeed_wrapper import MaybeOutfeedQueue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e28be82",
   "metadata": {},
   "source": [
    "## General approach to code in this tutorial\n",
    "\n",
    "You will notice that a lot of code has been extracted to functions. This is \n",
    "mainly because when running in a Jupyter notebook most of the code has to be \n",
    "executed in the same Python context manager (which is scoped per cell). To \n",
    "avoid giant Jupyter notebook cells, you will only find invocations of functions\n",
    "later once the Tensorflow IPU context has been used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad4a76da",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We need to load the dataset and perform some normalization of values. Below\n",
    "you will find a helper function to use inside IPU context, which will load\n",
    "the input data with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de40c686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    mnist = keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    # Add a channels dimension.\n",
    "    x_train = tf.expand_dims(x_train, -1)\n",
    "    x_test = tf.expand_dims(x_test, -1)\n",
    "\n",
    "    train_ds = tf.data.Dataset \\\n",
    "        .from_tensor_slices((x_train, y_train)) \\\n",
    "        .shuffle(len(x_train)) \\\n",
    "        .batch(32, drop_remainder=True)\n",
    "\n",
    "    train_ds = train_ds.map(\n",
    "        lambda d, l: (tf.cast(d, tf.float32), tf.cast(l, tf.float32))\n",
    "    )\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4affc",
   "metadata": {},
   "source": [
    "## General description of the model\n",
    "\n",
    "By default, the tutorial runs a three layer fully connected model, pipelined \n",
    "over two IPUs. Gradients for one of the layers, and activations for two of \n",
    "the layers, are returned for inspection on the host. This can be changed using \n",
    "options.\n",
    "\n",
    "The gradient accumulation count (`gradient_accumulation_steps_per_replica`)\n",
    "determines the pipeline depth, so the number of activations and gradients \n",
    "added to the outfeed queues will be proportional to the gradient accumulation \n",
    "value. Additionally, the outfeed callback is called at the end of the epoch, \n",
    "so the number of steps per epoch will also affect the amount of data \n",
    "in the queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f85bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_sequential_model(multi_activations_outfeed_queue):\n",
    "    seq_model = keras.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation='relu', name=\"Dense_256\"),\n",
    "        keras.layers.Dense(128, activation='relu', name=\"Dense_128\"),\n",
    "        outfeed_layers.MaybeOutfeed(multi_activations_outfeed_queue,\n",
    "                                    final_outfeed=False,\n",
    "                                    name=\"Dense_128_acts\"),\n",
    "        keras.layers.Dense(10, activation='softmax', name=\"Dense_10\"),\n",
    "        outfeed_layers.MaybeOutfeed(multi_activations_outfeed_queue,\n",
    "                                    final_outfeed=True,\n",
    "                                    name=\"Dense_10_acts\")\n",
    "    ])\n",
    "    seq_model.set_pipelining_options(gradient_accumulation_steps_per_replica=4)\n",
    "    seq_model.set_pipeline_stage_assignment([0, 0, 1, 1, 1, 1])\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c4a493",
   "metadata": {},
   "source": [
    "## Configuring the demo\n",
    "\n",
    "Choose values for the following variables that hold parameters.\n",
    "If you change them for experimentation in a Jupyter notebook, re-run all\n",
    "the cells below including this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6931bcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [boolean] Should the code outfeed the pre-accumulated gradients, rather than\n",
    "# accumulated gradients? Only makes a difference when using gradient\n",
    "# accumulation, which is always the case when pipelining is enabled.\n",
    "outfeed_pre_accumulated_gradients = False\n",
    "\n",
    "# Number of steps to run per execution. The number of batches to run for\n",
    "# each TensorFlow function call. At most it would execute a full epoch.\n",
    "steps_per_execution = 500\n",
    "\n",
    "# Number of steps per epoch. The total number of steps (batches of samples)\n",
    "# for one epoch to finish and starting the next one. The default `None` is\n",
    "# equal to the number of samples divided by the batch size.\n",
    "steps_per_epoch = steps_per_execution\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# [List] String values representing which gradients to add to the dictionary\n",
    "# that is enqueued on the outfeed queue. Pass `[none]` to disable filtering.\n",
    "gradients_filters = ['Dense_128']\n",
    "\n",
    "# [List] Activation filters - strings representing which activations in the\n",
    "# second `PipelineStage` to add to the dictionary that is enqueued on the\n",
    "# outfeed queue. Pass `[none]` to disable filtering. Applicable only for\n",
    "# pipelined models.\n",
    "activations_filters = ['none']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7082084",
   "metadata": {},
   "source": [
    "If the above `outfeed_pre_accumulated_gradients` is set to `True`, then\n",
    "modify the outfeed optimizer mode. You can read more about this in [outfeed_optimizer](https://github.com/graphcore/tutorials/blob/master/feature_examples/tensorflow2/inspecting_tensors/outfeed_optimizer.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7dbe37",
   "metadata": {},
   "outputs": [],
   "source": [
    "if outfeed_pre_accumulated_gradients:\n",
    "    outfeed_optimizer_mode = OutfeedOptimizerMode.AFTER_COMPUTE\n",
    "else:\n",
    "    outfeed_optimizer_mode = OutfeedOptimizerMode.BEFORE_APPLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87911444",
   "metadata": {},
   "source": [
    "Define a helper function to parse user input for filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af061e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filters(filters_input):\n",
    "    if len(filters_input) == 1 and filters_input[0].lower() == \"none\":\n",
    "        return None\n",
    "    return filters_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d87002",
   "metadata": {},
   "source": [
    "Next we define a helper function to create the Keras model with callbacks.\n",
    "Inside, multiple outfeed queues and callbacks are created based on the user\n",
    "prepared lists of layers in variables `gradients_filters` and \n",
    "`activations_filters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338e5153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_callbacks(gradients_filters, activations_filters):\n",
    "    optimizer_q = MaybeOutfeedQueue(filters=process_filters(gradients_filters))\n",
    "    act_q = MaybeOutfeedQueue(filters=process_filters(activations_filters))\n",
    "\n",
    "    gradients_cb = OutfeedCallback(outfeed_queue=optimizer_q,\n",
    "                                   name=\"Gradients callback\")\n",
    "    multi_layer_cb = OutfeedCallback(outfeed_queue=act_q,\n",
    "                                     name=\"Multi-layer activations callback\")\n",
    "\n",
    "    callbacks = [gradients_cb, multi_layer_cb]\n",
    "    seq_model = create_pipeline_sequential_model(act_q)\n",
    "    return seq_model, callbacks, optimizer_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f6027",
   "metadata": {},
   "source": [
    "Initialise IPU configuration - more details [here](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#tensorflow.python.ipu.config.IPUConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ipu.config.IPUConfig()\n",
    "cfg.auto_select_ipus = 2\n",
    "cfg.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02ee6c7",
   "metadata": {},
   "source": [
    "## Training the model on an IPU\n",
    "\n",
    "If you are using Keras, you must instantiate your Keras model inside of \n",
    "a strategy scope, which is a Python context manager.\n",
    "More details about the `IPUStrategy` API can be found [here](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/targeting_tf2.html#ipustrategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc86dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = ipu.ipu_strategy.IPUStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3568c58",
   "metadata": {},
   "source": [
    "Use the `strategy.scope()` context to ensure that everything within that \n",
    "context will be compiled for the IPU device. You should do this instead of \n",
    "using the `tf.device` context that is used in TensorFlow1.\n",
    "\n",
    "This tutorial uses queues for handling of outfeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2f3f05",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    seq_model, callbacks, optimizer_outfeed_queue = \\\n",
    "        model_with_callbacks(gradients_filters, activations_filters)\n",
    "\n",
    "    # Build the graph passing an OutfeedOptimizer to enqueue selected gradients\n",
    "    seq_model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer=OutfeedOptimizer(\n",
    "            wrapped_optimizer=keras.optimizers.SGD(),\n",
    "            outfeed_queue=optimizer_outfeed_queue,\n",
    "            outfeed_optimizer_mode=outfeed_optimizer_mode,\n",
    "            model=seq_model\n",
    "        ),\n",
    "        steps_per_execution=steps_per_execution\n",
    "    )\n",
    "\n",
    "    # Train the model passing the callbacks to see the gradients\n",
    "    # and activations stats\n",
    "    seq_model.fit(\n",
    "        create_dataset(),\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d39e0e",
   "metadata": {},
   "source": [
    "Example callback outfeed print would look a bit like this:\n",
    "```\n",
    "Gradients callback\n",
    "key: Dense_128/bias:0_grad shape: (125, 128)\n",
    "key: Dense_128/kernel:0_grad shape: (125, 256, 128)\n",
    "Epoch 3 - Summary Stats\n",
    "Index Name                         Mean         Std          Minimum      Maximum      NaNs    infs   \n",
    "0     Dense_128/bias:0_grad        -0.000663    0.021037     -0.108830    0.111534     False   False  \n",
    "1     Dense_128/kernel:0_grad      -0.000120    0.012575     -0.186476    0.183576     False   False  \n",
    "\n",
    "Single layer activations callback\n",
    "No data enqueued\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
