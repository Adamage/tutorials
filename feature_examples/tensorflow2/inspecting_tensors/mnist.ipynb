{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dde2119",
   "metadata": {},
   "source": [
    "Copyright (c) 2020 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89568326",
   "metadata": {},
   "source": [
    "# TensorFlow 2: Tensor Inspection Techniques\n",
    "\n",
    "In this tutorial you will train a selection of simple fully connected models\n",
    "on the MNIST numeral data set and see how tensors (containing activations\n",
    "and gradients) can be returned to the host via outfeeds for inspection.\n",
    "\n",
    "An outfeed is the counterpart to an infeed and manages the transfer of data \n",
    "(like tensors, tuples or dictionaries of tensors) from the IPU graph to \n",
    "the host. Too learn more about using feeds, see [outfeed queues](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#outfeed-queue).\n",
    "\n",
    "Outfeeds can be useful for debugging, but can significantly increase the amount\n",
    "of memory required on the IPU(s). When pipelining, you could use a smaller\n",
    "value for the gradient accumulation count to mitigate this. Also consider using\n",
    "a small number of steps per execution to reduce memory footprint. \n",
    "\n",
    "In this demo, filters can be used to return only a subset of the activations\n",
    "and gradients. The outfed information can be returned to a variable or can be\n",
    "printed to the standard output. In [`outfeed_callback.py`](./outfeed_callback.py)\n",
    "the implementation is to print this information to the standard output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944087e6",
   "metadata": {},
   "source": [
    "## How to use this demo\n",
    "\n",
    "### File structure and local imports\n",
    "\n",
    "* `mnist.py` The main Python script.\n",
    "* `mnist_code_only.py` Autogenerated script without any comments.\n",
    "* `mnist.ipynb` Autogenerated interactive Jupyter Notebook tutorial.\n",
    "* `outfeed_callback.py` Contains a custom callback that dequeues an outfeed \n",
    "  queue at the end of every epoch.\n",
    "* `outfeed_layers.py` Custom layers that (selectively) add the inputs \n",
    "  (for example, activations from the previous layer) to a dict that will be \n",
    "  enqueued on an outfeed queue.\n",
    "* `outfeed_optimizer.py` Custom optimizer that outfeeds the gradients generated\n",
    "  by a wrapped optimizer.\n",
    "* `outfeed_wrapper.py` Contains the `MaybeOutfeedQueue` class, see below.\n",
    "* `README.md` Markdown autogenerated file.\n",
    "* `requirements.txt` Required packages for this tutorial\n",
    "* `tests` Subdirectory containing test scripts.\n",
    "\n",
    "### Custom classes descriptions\n",
    "\n",
    "This tutorial uses the following classes, which are implemented in libraries:\n",
    "\n",
    "* `outfeed_wrapper.MaybeOutfeedQueue` - a wrapper for an IPUOutfeedQueue that \n",
    "  allows key-value pairs to be selectively added to a dictionary that can then \n",
    "  be enqueued.\n",
    "* `outfeed_optimizer.OutfeedOptimizer` - a custom optimizer that enqueues \n",
    "  gradients using a `MaybeOutfeedQueue`, with the choice of whether to enqueue \n",
    "  the gradients after they are computed (the pre-accumulated gradients) or \n",
    "  before they are applied (the accumulated gradients).\n",
    "* `outfeed_layers.Outfeed` - a Keras layer that puts the inputs into \n",
    "  a dictionary and enqueues it on an IPUOutfeedQueue.\n",
    "* `outfeed_layers.MaybeOutfeed` - a Keras layer that uses a MaybeOutfeedQueue \n",
    "  to selectively put the inputs into a dict and optionally enqueues the dict. \n",
    "  At the moment, this layer cannot be used with non-pipelined Sequential models.\n",
    "* `outfeed_callback.OutfeedCallback` - a Keras callback to dequeue an outfeed\n",
    "  queue at the end of every epoch, printing some statistics about the tensors.\n",
    "\n",
    "### Environment preparation\n",
    "\n",
    "Install the Poplar SDK following the instructions in the [Getting Started](https://docs.graphcore.ai/en/latest/getting-started.html)\n",
    "guide for your IPU system. Make sure to run the `enable.sh` scripts for Poplar \n",
    "and PopART and activate a Python3 virtualenv with PopTorch installed.\n",
    "Then install the package requirements:\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f3fc36",
   "metadata": {},
   "source": [
    "### Required imports\n",
    ">**Note**\n",
    ">The Graphcore TensorFlow 2 wheel is bundled with Graphcore Poplar SDK. Please\n",
    ">ensure you install this wheel rather than the default public wheel, as it \n",
    ">contains IPU specific functionality in the `ipu` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c918c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python import ipu\n",
    "\n",
    "from outfeed_callback import OutfeedCallback\n",
    "from outfeed_optimizer import OutfeedOptimizer, OutfeedOptimizerMode\n",
    "import outfeed_layers\n",
    "from outfeed_wrapper import MaybeOutfeedQueue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85784756",
   "metadata": {},
   "source": [
    "## General approach to code in this tutorial\n",
    "\n",
    "You will notice that a lot of code has been extracted to functions. This is \n",
    "mainly because when running in a Jupyter notebook most of the code has to be \n",
    "executed in the same Python context manager (which is scoped per cell). To \n",
    "avoid giant Jupyter notebook cells, you will only find invocations of functions\n",
    "later once the Tensorflow IPU context has been used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7564ec",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We need to load the dataset and perform some normalization of values. Below\n",
    "you will find a helper function to use inside IPU context, which will load\n",
    "the input data with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80faf4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    mnist = keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    # Add a channels dimension.\n",
    "    x_train = tf.expand_dims(x_train, -1)\n",
    "    x_test = tf.expand_dims(x_test, -1)\n",
    "\n",
    "    train_ds = tf.data.Dataset \\\n",
    "        .from_tensor_slices((x_train, y_train)) \\\n",
    "        .shuffle(len(x_train)) \\\n",
    "        .batch(32, drop_remainder=True)\n",
    "\n",
    "    train_ds = train_ds.map(\n",
    "        lambda d, l: (tf.cast(d, tf.float32), tf.cast(l, tf.float32))\n",
    "    )\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be57c342",
   "metadata": {},
   "source": [
    "## General description of the model\n",
    "\n",
    "By default, the tutorial runs a three layer fully connected model, pipelined \n",
    "over two IPUs. Gradients for one of the layers, and activations for two of \n",
    "the layers, are returned for inspection on the host. This can be changed using \n",
    "options.\n",
    "\n",
    "The gradient accumulation count `gradient_accumulation_steps_per_replica`\n",
    "determines the pipeline depth, so the number of activations and gradients \n",
    "added to the outfeed queues will be proportional to the gradient accumulation \n",
    "value. Additionally, the outfeed callback is called at the end of the epoch, \n",
    "so the number of steps per epoch will also affect the amount of data \n",
    "in the queues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e644b0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_sequential_model(multi_activations_outfeed_queue):\n",
    "    seq_model = keras.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation='relu', name=\"Dense_256\"),\n",
    "        keras.layers.Dense(128, activation='relu', name=\"Dense_128\"),\n",
    "        outfeed_layers.MaybeOutfeed(multi_activations_outfeed_queue,\n",
    "                                    final_outfeed=False,\n",
    "                                    name=\"Dense_128_acts\"),\n",
    "        keras.layers.Dense(10, activation='softmax', name=\"Dense_10\"),\n",
    "        outfeed_layers.MaybeOutfeed(multi_activations_outfeed_queue,\n",
    "                                    final_outfeed=True,\n",
    "                                    name=\"Dense_10_acts\")\n",
    "    ])\n",
    "    seq_model.set_pipelining_options(gradient_accumulation_steps_per_replica=4)\n",
    "    seq_model.set_pipeline_stage_assignment([0, 0, 1, 1, 1, 1])\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24d26efb",
   "metadata": {},
   "source": [
    "## Configuring the demo\n",
    "\n",
    "Choose values for the following variables that hold parameters.\n",
    "If you change them for experimentation in a Jupyter notebook, re-run all t\n",
    "he cells below including this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31701762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [boolean] Should the code outfeed the pre-accumulated gradients, rather than\n",
    "# accumulated gradients? Only makes a difference when using gradient\n",
    "# accumulation, which is always the case when pipelining is enabled.\n",
    "outfeed_pre_accumulated_gradients = False\n",
    "\n",
    "# Number of steps to run per execution. The number of batches to run for\n",
    "# each TensorFlow function call. At most it would execute a full epoch.\n",
    "steps_per_execution = 500\n",
    "\n",
    "# Number of steps per epoch. The total number of steps (batches of samples)\n",
    "# for one epoch to finish and starting the next one. The default `None` is\n",
    "# equal to the number of samples divided by the batch size.\n",
    "steps_per_epoch = steps_per_execution\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# [List] String values representing which gradients to add to the dictionary\n",
    "# that is enqueued on the outfeed queue. Pass `[none]` to disable filtering.\n",
    "gradients_filters = ['Dense_128']\n",
    "\n",
    "# [List] Activation filters - strings representing which activations in the\n",
    "# second `PipelineStage` to add to the dictionary that is enqueued on the\n",
    "# outfeed queue. Pass `[none]` to disable filtering. Applicable only for\n",
    "# pipelined models.\n",
    "activations_filters = ['none']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a87d9d3",
   "metadata": {},
   "source": [
    "Automatically set these parameters based on user input and configure the IPU\n",
    "system.\n",
    "Outfeed optimizer mode documentation can be found in [outfeed_optimizer](./outfeed_optimizer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a7fb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "if outfeed_pre_accumulated_gradients:\n",
    "    outfeed_optimizer_mode = OutfeedOptimizerMode.AFTER_COMPUTE\n",
    "else:\n",
    "    outfeed_optimizer_mode = OutfeedOptimizerMode.BEFORE_APPLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7661f2a",
   "metadata": {},
   "source": [
    "Define a helper function to parse user input for filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5387a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filters(filters_input):\n",
    "    if len(filters_input) == 1 and filters_input[0].lower() == \"none\":\n",
    "        return None\n",
    "    return filters_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7968af",
   "metadata": {},
   "source": [
    "Next we define a helper function to create the Keras model with callbacks.\n",
    "Inside, multiple outfeed queues and callbacks are created based on the user\n",
    "prepared lists of layers in variables `gradients_filters` and \n",
    "`activations_filters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32362341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_with_callbacks(gradients_filters, activations_filters):\n",
    "    optimizer_q = MaybeOutfeedQueue(filters=process_filters(gradients_filters))\n",
    "    act_q = MaybeOutfeedQueue(filters=process_filters(activations_filters))\n",
    "\n",
    "    gradients_cb = OutfeedCallback(outfeed_queue=optimizer_q,\n",
    "                                   name=\"Gradients callback\")\n",
    "    multi_layer_cb = OutfeedCallback(outfeed_queue=act_q,\n",
    "                                     name=\"Multi-layer activations callback\")\n",
    "\n",
    "    callbacks = [gradients_cb, multi_layer_cb]\n",
    "    seq_model = create_pipeline_sequential_model(act_q)\n",
    "    return seq_model, callbacks, optimizer_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62433e07",
   "metadata": {},
   "source": [
    "Initialise IPU configuration - more details [here](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#tensorflow.python.ipu.config.IPUConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2a1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ipu.config.IPUConfig()\n",
    "cfg.auto_select_ipus = 2\n",
    "cfg.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41a4bad",
   "metadata": {},
   "source": [
    "## Training the model on an IPU\n",
    "\n",
    "If you are using Keras, you must instantiate your Keras model inside of \n",
    "a strategy scope, which is a Python context manager.\n",
    "More details about the `IPUStrategy` [here](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/targeting_tf2.html#ipustrategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17962a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = ipu.ipu_strategy.IPUStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d7c996",
   "metadata": {},
   "source": [
    "Use the `strategy.scope()` context to ensure that everything within that \n",
    "context will be compiled for the IPU device. You should do this instead of \n",
    "using the `tf.device` context.\n",
    "\n",
    "This tutorial uses queues for handling of outfeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75d6e2",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    seq_model, callbacks, optimizer_outfeed_queue = \\\n",
    "        model_with_callbacks(gradients_filters, activations_filters)\n",
    "\n",
    "    # Build the graph passing an OutfeedOptimizer to enqueue selected gradients\n",
    "    seq_model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer=OutfeedOptimizer(\n",
    "            wrapped_optimizer=keras.optimizers.SGD(),\n",
    "            outfeed_queue=optimizer_outfeed_queue,\n",
    "            outfeed_optimizer_mode=outfeed_optimizer_mode,\n",
    "            model=seq_model\n",
    "        ),\n",
    "        steps_per_execution=steps_per_execution\n",
    "    )\n",
    "\n",
    "    # Train the model passing the callbacks to see the gradients\n",
    "    # and activations stats\n",
    "    seq_model.fit(\n",
    "        create_dataset(),\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cd681b",
   "metadata": {},
   "source": [
    "Example callback outfeed print would look a bit like this:\n",
    "```\n",
    "Gradients callback\n",
    "key: Dense_128/bias:0_grad shape: (125, 128)\n",
    "key: Dense_128/kernel:0_grad shape: (125, 256, 128)\n",
    "Epoch 3 - Summary Stats\n",
    "Index Name                         Mean         Std          Minimum      Maximum      NaNs    infs   \n",
    "0     Dense_128/bias:0_grad        -0.000663    0.021037     -0.108830    0.111534     False   False  \n",
    "1     Dense_128/kernel:0_grad      -0.000120    0.012575     -0.186476    0.183576     False   False  \n",
    "\n",
    "Single layer activations callback\n",
    "No data enqueued\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c015d5cf",
   "metadata": {},
   "source": [
    "## Profiling and Visualising for pipelining\n",
    "\n",
    "If you execute this code with environmental variable:\n",
    "```bash\n",
    "POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\"}'\n",
    "```\n",
    "\n",
    "For example like this:\n",
    "```bash\n",
    "POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\"}' python3 mnist.py\n",
    "```\n",
    "\n",
    "Or set this variable inside Jupyter Notebook:\n",
    "```python\n",
    "import os\n",
    "os.environ['POPLAR_ENGINE_OPTIONS']='{\"autoReport.all\":\"true\"}'\n",
    "```\n",
    "\n",
    "Then you could use the generated report, which for this tutorial might look\n",
    "like this:\n",
    "```bash\n",
    "ls .\n",
    "```\n",
    "> ./tf_report__2021-10-06__02-24-24.631__70052:\n",
    "> archive.a\n",
    "> debug.cbor\n",
    "> framework.json\n",
    "> profile.pop\n",
    "> profile.pop_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c268f",
   "metadata": {},
   "source": [
    "## PopVision - reading the reports\n",
    "\n",
    "When you open such a report, you could navigate to multiple reports.\n",
    "Here you can see example views generated for a set of parameters in this\n",
    "tutorial: pipelining enabled, for 2 IPUs with gradient accumulation enabled.\n",
    "\n",
    "![Execution Trace](static/popvision_multiple_ipus.png)\n",
    "\n",
    "Also when you go to the Operations Summary and in the top left corner you \n",
    "select Tensorflow Layer, you can observe that SGD and gradient accumulation\n",
    "was performed.\n",
    "![Execution Trace](static/enabled_gradient_accumulator_and_SGD.png)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
