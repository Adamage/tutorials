{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d20138c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2020 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3be9b3",
   "metadata": {},
   "source": [
    "# TensorFlow 2: Pipelining and Tensor Inspection Techniques\n",
    "\n",
    "In this tutorial you will train a selection of simple fully connected models\n",
    "on the MNIST numeral data set and see how tensors (containing activations\n",
    "and gradients) can be returned to the host via outfeeds for inspection.\n",
    "\n",
    "Outfeeds can be useful for debugging, but can significantly increase the amount\n",
    "of memory required on the IPU(s). When pipelining, you could use a smaller\n",
    "value for the gradient accumulation count to mitigate this. Also consider using\n",
    "a small number of steps per execution to reduce memory footprint. Filters can\n",
    "be used to only return a subset of the activations and gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d2c5ab",
   "metadata": {},
   "source": [
    "## Required imports.\n",
    ">**Note**\n",
    ">The Graphcore TensorFlow 2 wheel is bundled with Graphcore Poplar SDK. Please\n",
    ">ensure you install this wheel rather than the default public wheel, as it \n",
    ">contains IPU specific functionality in the `ipu` submodule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc9747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python import ipu\n",
    "\n",
    "from outfeed_callback import OutfeedCallback\n",
    "from outfeed_optimizer import OutfeedOptimizer, OutfeedOptimizerMode\n",
    "import outfeed_layers\n",
    "from outfeed_wrapper import MaybeOutfeedQueue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988869e8",
   "metadata": {},
   "source": [
    "## General approach to code in this tutorial\n",
    "\n",
    "You will notice that a lot of code has been extracted to functions. This is \n",
    "mainly because when running in a Jupyter notebook most of the code has to be \n",
    "executed in the same Python context manager (which is scoped per cell). To \n",
    "avoid giant Jupyter notebook cells, you will only find invocations of functions\n",
    "later once the Tensorflow IPU context has been used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2893f6d1",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "\n",
    "We need to load the dataset and perform some normalization of values. Below\n",
    "you will find a helper function to use inside IPU context, which will load\n",
    "the input data with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb969238",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    mnist = keras.datasets.mnist\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    # Add a channels dimension.\n",
    "    x_train = tf.expand_dims(x_train, -1)\n",
    "    x_test = tf.expand_dims(x_test, -1)\n",
    "\n",
    "    train_ds = tf.data.Dataset \\\n",
    "        .from_tensor_slices((x_train, y_train)) \\\n",
    "        .shuffle(len(x_train)) \\\n",
    "        .batch(32, drop_remainder=True)\n",
    "\n",
    "    train_ds = train_ds.map(\n",
    "        lambda d, l: (tf.cast(d, tf.float32), tf.cast(l, tf.float32))\n",
    "    )\n",
    "    return train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb8393f",
   "metadata": {},
   "source": [
    "## Pipelining features\n",
    "\n",
    "In this tutorial, we will create models using the Keras Model class and IPU \n",
    "pipelining features.\n",
    "We are going to use Pipeline Stages to assign operations to devices and to\n",
    "configure parallelism. Additionally, we will use the functionality of outfeed\n",
    "queues for IPUs to outfeed the activations for multiple layers for a specific\n",
    "stage.\n",
    "If you're interested you can read more about outfeed queues [here](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/perf_training.html#accessing-outfeed-queue-results-during-execution)\n",
    "\n",
    "In the following graphics, FWD and BWD refer to forward and backward passes.\n",
    "\n",
    "The computational stages can be interleaved on the devices in three different \n",
    "ways as described by the `pipeline_schedule` parameter. By default the API \n",
    "will use the `PipelineSchedule.Grouped` mode, where the forward passes are \n",
    "grouped together, and the backward passes are grouped together. \n",
    "![Grouped pipeline](static/grouped_pipeline.png)\n",
    "\n",
    "The main alternative is the `PipelineSchedule.Interleaved` mode, where the \n",
    "forward and backward passes are interleaved, so that fewer activations need \n",
    "to be stored. \n",
    "![Interleaved pipeline](static/interleaved_pipeline.png)\n",
    "\n",
    "Additionally, the `PipelineSchedule.Sequential` mode, where the pipeline is \n",
    "scheduled in the same way as if it were a sharded model, may be useful when \n",
    "debugging your model.\n",
    "![Sharded pipeline](static/sharded_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11363b",
   "metadata": {},
   "source": [
    "## File structure and local imports\n",
    "\n",
    "* `mnist.py` The main Python script.\n",
    "* `outfeed_callback.py` Contains a custom callback that dequeues an outfeed \n",
    "  queue at the end of every epoch.\n",
    "* `outfeed_layers.py` Custom layers that (selectively) add the inputs \n",
    "  (for example, activations from the previous layer) to a dict that will be \n",
    "  enqueued on an outfeed queue.\n",
    "* `outfeed_optimizer.py` Custom optimizer that outfeeds the gradients generated\n",
    "  by a wrapped optimizer.\n",
    "* `outfeed_wrapper.py` Contains the `MaybeOutfeedQueue` class, see below.\n",
    "* `README.md` Markdown autogenerated file.\n",
    "* `requirements.txt` Required packages for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6529a4d0",
   "metadata": {},
   "source": [
    "## Custom classes descriptions\n",
    "\n",
    "This tutorial uses the following classes, which are implemented in libraries:\n",
    "\n",
    "* `outfeed_wrapper.MaybeOutfeedQueue` - a wrapper for an IPUOutfeedQueue that \n",
    "  allows key-value pairs to be selectively added to a dictionary that can then \n",
    "  be enqueued.\n",
    "* `outfeed_optimizer.OutfeedOptimizer` - a custom optimizer that enqueues \n",
    "  gradients using a `MaybeOutfeedQueue`, with the choice of whether to enqueue \n",
    "  the gradients after they are computed (the pre-accumulated gradients) or \n",
    "  before they are applied (the accumulated gradients).\n",
    "* `outfeed_layers.Outfeed` - a Keras layer that puts the inputs into \n",
    "  a dictionary and enqueues it on an IPUOutfeedQueue.\n",
    "* `outfeed_layers.MaybeOutfeed` - a Keras layer that uses a MaybeOutfeedQueue \n",
    "  to selectively put the inputs into a dict and optionally enqueues the dict. \n",
    "  At the moment, this layer cannot be used with non-pipelined Sequential models.\n",
    "* `outfeed_callback.OutfeedCallback` - a Keras callback to dequeue an outfeed\n",
    "  queue at the end of every epoch, printing some statistics about the tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c49d221",
   "metadata": {},
   "source": [
    "## General description for used model\n",
    "\n",
    "By default, the example runs a three layer fully connected model, pipelined \n",
    "over two IPUs. Gradients for one of the layers, and activations for two of \n",
    "the layers, are returned for inspection on the host. This can be changed using \n",
    "options.\n",
    "\n",
    "For the single IPU models (Model and Sequential), gradients and activations are\n",
    "returned for one layer.\n",
    "\n",
    ">What follows next are helper functions which act as factories for instances\n",
    ">of Keras models. After the section where they are defined, there is an option\n",
    ">to **choose one of them** for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ebd19",
   "metadata": {},
   "source": [
    "## Option 1 - Keras `Functional` model without pipelining\n",
    "\n",
    "Create the model using the Keras Model class.\n",
    "Outfeed the activations for a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c4ca84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(\n",
    "        activations_outfeed_queue,\n",
    "        gradient_accumulation_steps_per_replica\n",
    "):\n",
    "    input_layer = keras.layers.Input(\n",
    "        shape=(28, 28, 1),\n",
    "        dtype=tf.float32,\n",
    "        batch_size=32\n",
    "    )\n",
    "    x = keras.layers.Flatten()(input_layer)\n",
    "    x = keras.layers.Dense(128, activation='relu', name=\"Dense_128\")(x)\n",
    "\n",
    "    # Outfeed the activations for a single layer:\n",
    "    x = outfeed_layers.Outfeed(\n",
    "        activations_outfeed_queue,\n",
    "        name=\"Dense_128_acts\")(x)\n",
    "\n",
    "    x = keras.layers.Dense(10, activation='softmax',  name=\"Dense_10\")(x)\n",
    "\n",
    "    keras_model = keras.Model(input_layer, x)\n",
    "    keras_model.set_gradient_accumulation_options(\n",
    "        gradient_accumulation_steps_per_replica=\n",
    "        gradient_accumulation_steps_per_replica\n",
    "    )\n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ae7a80",
   "metadata": {},
   "source": [
    "##  Option 2 - Keras `Functional` model with pipelining for two separate Stages\n",
    "The usage of Pipeline Stages can be found here:\n",
    "[pipelined training](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/perf_training.html#pipelined-training)\n",
    "\n",
    "To pipeline a `Functional` model you are writing yourself, each layer call must\n",
    "happen within the scope of an `ipu.keras.PipelineStage` context.\n",
    "In the function below, we assign layers to two different stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70255beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_model(\n",
    "        multi_activations_outfeed_queue,\n",
    "        gradient_accumulation_steps_per_replica\n",
    "):\n",
    "    input_layer = keras.layers.Input(shape=(28, 28, 1),\n",
    "                                     dtype=tf.float32,\n",
    "                                     batch_size=32)\n",
    "\n",
    "    with ipu.keras.PipelineStage(0):\n",
    "        x = keras.layers.Flatten()(input_layer)\n",
    "        x = keras.layers.Dense(256, activation='relu', name=\"Dense_256\")(x)\n",
    "\n",
    "    with ipu.keras.PipelineStage(1):\n",
    "        x = keras.layers.Dense(128, activation='relu', name=\"Dense_128\")(x)\n",
    "        x = outfeed_layers.MaybeOutfeed(multi_activations_outfeed_queue,\n",
    "                                        final_outfeed=False,\n",
    "                                        name=\"Dense_128_acts\")(x)\n",
    "        x = keras.layers.Dense(10, activation='softmax',  name=\"Dense_10\")(x)\n",
    "        x = outfeed_layers.MaybeOutfeed(multi_activations_outfeed_queue,\n",
    "                                        final_outfeed=True,\n",
    "                                        name=\"Dense_10_acts\")(x)\n",
    "    model = keras.Model(input_layer, x)\n",
    "    model.set_pipelining_options(gradient_accumulation_steps_per_replica=\n",
    "                                 gradient_accumulation_steps_per_replica)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ac9c7a",
   "metadata": {},
   "source": [
    "##  Option 3 - Keras `Sequential model` without pipelining\n",
    "\n",
    "This function creates the model using the Keras `Sequential` class. This class\n",
    "groups a linear stack of layers into a `tf.Keras.Model`. Then, `Sequential`\n",
    "provides training and inference features on this model.\n",
    " \n",
    "We outfeed the activations for a single layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5773bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_model(\n",
    "        activations_outfeed_queue,\n",
    "        gradient_accumulation_steps_per_replica\n",
    "):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(128, activation='relu', name=\"Dense_128\"),\n",
    "        outfeed_layers.Outfeed(activations_outfeed_queue, name=\"Dense_128_acts\"),\n",
    "        keras.layers.Dense(10, activation='softmax', name=\"Dense_10\")\n",
    "    ])\n",
    "    model\\\n",
    "        .set_gradient_accumulation_options(\n",
    "          gradient_accumulation_steps_per_replica=\n",
    "          gradient_accumulation_steps_per_replica\n",
    "        )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eca726",
   "metadata": {},
   "source": [
    "##  Option 4 - Keras `Sequential model` with pipelining\n",
    "\n",
    "This function reate the model using the Keras Sequential class. We can pipeline\n",
    "the model by assigning layers to stages through \n",
    "`set_pipeline_stage_assignment`. We outfeed the activations for multiple \n",
    "layers in the second stage.\n",
    "\n",
    "Below you will see pipeline stage assignment like this:\n",
    "`model.set_pipeline_stage_assignment([0, 0, 1, 1, 1, 1])`\n",
    "which means that first two layers of `Sequential` model are assigned to\n",
    "the first stage, and the remaining four layers to the second stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4174fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_sequential_model(\n",
    "        multi_activations_outfeed_queue,\n",
    "        gradient_accumulation_steps_per_replica\n",
    "):\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dense(256, activation='relu', name=\"Dense_256\"),\n",
    "        keras.layers.Dense(128, activation='relu', name=\"Dense_128\"),\n",
    "        outfeed_layers.MaybeOutfeed(multi_activations_outfeed_queue,\n",
    "                                    final_outfeed=False,\n",
    "                                    name=\"Dense_128_acts\"),\n",
    "        keras.layers.Dense(10, activation='softmax', name=\"Dense_10\"),\n",
    "        outfeed_layers.MaybeOutfeed(multi_activations_outfeed_queue,\n",
    "                                    final_outfeed=True,\n",
    "                                    name=\"Dense_10_acts\")\n",
    "    ])\n",
    "    model.set_pipelining_options(\n",
    "        gradient_accumulation_steps_per_replica=\n",
    "        gradient_accumulation_steps_per_replica\n",
    "    )\n",
    "    model.set_pipeline_stage_assignment([0, 0, 1, 1, 1, 1])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001c35e4",
   "metadata": {},
   "source": [
    "## Configuring pipelining\n",
    "\n",
    "Choose values for the following variables that hold parameters.\n",
    "If you change them for experimentation, re-run all the cells below including\n",
    "this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7c2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Type can be \"Model\" or \"Sequential\"\n",
    "SEQUENTIAL_MODEL = \"Sequential\"\n",
    "REGULAR_MODEL = \"Model\"\n",
    "model_type = SEQUENTIAL_MODEL\n",
    "\n",
    "# [boolean] Should IPU pipelining be disabled?\n",
    "no_pipelining = False\n",
    "\n",
    "# [boolean] Should gradient accumulation be enabled? It's always enabled\n",
    "# for pipelined models.\n",
    "use_gradient_accumulation = True\n",
    "\n",
    "# [boolean] Should the code outfeed the pre-accumulated gradients, rather than\n",
    "# accumulated gradients? Only makes a difference when using gradient\n",
    "# accumulation, which is always the case when pipelining is enabled.\n",
    "outfeed_pre_accumulated_gradients = False\n",
    "\n",
    "# Number of steps to run per execution.\n",
    "steps_per_execution = 500\n",
    "\n",
    "# Number of epochs\n",
    "epochs = 3\n",
    "\n",
    "# [List] String values representing which gradients to add to the dictionary\n",
    "# that is enqueued on the outfeed queue. Pass `[none]` to disable filtering.\n",
    "gradients_filters = ['Dense_128']\n",
    "\n",
    "\n",
    "# [List] Activation filters - strings representing which activations in the\n",
    "# second `PipelineStage` to add to the dictionary that is enqueued on the\n",
    "# outfeed queue. Pass `[none]` to disable filtering. Applicable only for\n",
    "# pipelined models.\n",
    "activations_filters = ['none']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b17bf7",
   "metadata": {},
   "source": [
    "Automatically set these parameters based on user input and configure the IPU\n",
    "system.\n",
    "Outfeed optimizer mode documentation can be found in [outfeed_optimizer](./outfeed_optimizer.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cdf2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if no_pipelining:\n",
    "    num_ipus = 1\n",
    "else:\n",
    "    num_ipus = 2\n",
    "    use_gradient_accumulation = True\n",
    "\n",
    "gradient_accumulation_steps_per_replica = 4\n",
    "\n",
    "if outfeed_pre_accumulated_gradients:\n",
    "    outfeed_optimizer_mode = OutfeedOptimizerMode.AFTER_COMPUTE\n",
    "else:\n",
    "    outfeed_optimizer_mode = OutfeedOptimizerMode.BEFORE_APPLY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0ca2e",
   "metadata": {},
   "source": [
    "Define a helper function to parse user input for filters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_filters(filters_input):\n",
    "    if len(filters_input) == 1 and filters_input[0].lower() == \"none\":\n",
    "        return None\n",
    "    return filters_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4e986d",
   "metadata": {},
   "source": [
    "Define a helper function to use user selected values and inject a model into\n",
    "an IPU-aware context where it's invoked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85e036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_selected_model_type(\n",
    "        gradient_accumulation_steps_per_replica,\n",
    "        no_pipelining,\n",
    "        use_gradient_accumulation,\n",
    "        model_type,\n",
    "        gradients_filters,\n",
    "        activations_filters\n",
    "):\n",
    "    # Create the outfeed queue for selected gradients.\n",
    "    # Remove the filters to get the gradients for all layers or pass different\n",
    "    # strings to the argument to select other layer(s)\n",
    "    optimizer_q = MaybeOutfeedQueue(filters=process_filters(gradients_filters))\n",
    "\n",
    "    # Create a callback for the gradients.\n",
    "    gradients_cb = OutfeedCallback(optimizer_q, name=\"Gradients callback\")\n",
    "\n",
    "    # Create callbacks for the activations in the custom layers.\n",
    "    activations_q = ipu.ipu_outfeed_queue.IPUOutfeedQueue()\n",
    "    layer_cb = OutfeedCallback(activations_q,\n",
    "                               name=\"Single layer activations callback\")\n",
    "\n",
    "    multi_activations_q = MaybeOutfeedQueue(filters=process_filters(\n",
    "                                                      activations_filters))\n",
    "    multi_layer_cb = OutfeedCallback(multi_activations_q,\n",
    "                                     name=\"Multi-layer activations callback\")\n",
    "\n",
    "    callbacks = [gradients_cb]\n",
    "\n",
    "    model = None\n",
    "    if not no_pipelining:\n",
    "        if model_type == REGULAR_MODEL:\n",
    "            model = create_pipeline_model(\n",
    "                multi_activations_q, gradient_accumulation_steps_per_replica\n",
    "            )\n",
    "        elif model_type == SEQUENTIAL_MODEL:\n",
    "            model = create_pipeline_sequential_model(\n",
    "                multi_activations_q, gradient_accumulation_steps_per_replica\n",
    "            )\n",
    "        callbacks += [multi_layer_cb]\n",
    "    else:\n",
    "        if not use_gradient_accumulation:\n",
    "            gradient_accumulation_steps_per_replica = 1\n",
    "\n",
    "        if model_type == SEQUENTIAL_MODEL:\n",
    "            model = create_sequential_model(\n",
    "                activations_q, gradient_accumulation_steps_per_replica\n",
    "            )\n",
    "        elif model_type == REGULAR_MODEL:\n",
    "            model = create_model(activations_q,\n",
    "                                 gradient_accumulation_steps_per_replica)\n",
    "        callbacks += [layer_cb]\n",
    "\n",
    "    if not model:\n",
    "        raise Exception(\"Please select proper model_type!\")\n",
    "\n",
    "    return model, callbacks, optimizer_q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14c7cd1",
   "metadata": {},
   "source": [
    "Initialise IPU configuration - more details here [`IPUConfig`](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#tensorflow.python.ipu.config.IPUConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b296477",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ipu.config.IPUConfig()\n",
    "cfg.auto_select_ipus = num_ipus\n",
    "cfg.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2615bc4c",
   "metadata": {},
   "source": [
    "## Placing and training the model on IPU\n",
    "\n",
    "If you are using Keras, you must instantiate your Keras model inside of \n",
    "the strategy scope, which is a Python context manager.\n",
    "More details here: [`IPUStrategy`](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/targeting_tf2.html#ipustrategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def7c503",
   "metadata": {},
   "outputs": [],
   "source": [
    "strategy = ipu.ipu_strategy.IPUStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05e6234",
   "metadata": {},
   "source": [
    "Use the `strategy.scope()` context to ensure that everything within that \n",
    "context will be compiled for the IPU device. You should do this instead of \n",
    "using the `tf.device` context.\n",
    "\n",
    "This tutorial uses queues for handling of outfeeds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ef9658",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    model, callbacks, optimizer_outfeed_queue = \\\n",
    "        instantiate_selected_model_type(\n",
    "            gradient_accumulation_steps_per_replica=\n",
    "            gradient_accumulation_steps_per_replica,\n",
    "            no_pipelining=no_pipelining,\n",
    "            use_gradient_accumulation=use_gradient_accumulation,\n",
    "            model_type=model_type,\n",
    "            gradients_filters=gradients_filters,\n",
    "            activations_filters=activations_filters\n",
    "        )\n",
    "\n",
    "    # Build the graph passing an OutfeedOptimizer to enqueue selected gradients\n",
    "    model.compile(\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        optimizer=OutfeedOptimizer(\n",
    "            wrapped_optimizer=keras.optimizers.SGD(),\n",
    "            outfeed_queue=optimizer_outfeed_queue,\n",
    "            outfeed_optimizer_mode=outfeed_optimizer_mode,\n",
    "            model=model\n",
    "        ),\n",
    "        steps_per_execution=steps_per_execution\n",
    "    )\n",
    "\n",
    "    # Train the model passing the callbacks to see the gradients\n",
    "    # and activations stats\n",
    "    model.fit(\n",
    "        create_dataset(),\n",
    "        callbacks=callbacks,\n",
    "        steps_per_epoch=steps_per_execution,\n",
    "        epochs=epochs\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583e367",
   "metadata": {},
   "source": [
    "Example callback outfeed print would look a bit like this:\n",
    "```\n",
    "Gradients callback\n",
    "key: Dense_128/bias:0_grad shape: (125, 128)\n",
    "key: Dense_128/kernel:0_grad shape: (125, 256, 128)\n",
    "Epoch 3 - Summary Stats\n",
    "Index Name                         Mean         Std          Minimum      Maximum      NaNs    infs   \n",
    "0     Dense_128/bias:0_grad        -0.000663    0.021037     -0.108830    0.111534     False   False  \n",
    "1     Dense_128/kernel:0_grad      -0.000120    0.012575     -0.186476    0.183576     False   False  \n",
    "\n",
    "Single layer activations callback\n",
    "No data enqueued\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d9aa87",
   "metadata": {},
   "source": [
    "## Profiling and Visualising for pipelining\n",
    "\n",
    "If you execute this code with environmental variable:\n",
    "```bash\n",
    "POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\"}'\n",
    "```\n",
    "\n",
    "For example like this:\n",
    "```bash\n",
    "POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\"}' python3 mnist.py\n",
    "```\n",
    "\n",
    "Or set this variable inside Jupyter Notebook:\n",
    "```python\n",
    "import os\n",
    "os.environ['POPLAR_ENGINE_OPTIONS']='{\"autoReport.all\":\"true\"}'\n",
    "```\n",
    "\n",
    "Then you could use the generated report, which for this tutorial might look\n",
    "like this:\n",
    "```bash\n",
    "ls .\n",
    "```\n",
    "> ./tf_report__2021-10-06__02-24-24.631__70052:\n",
    "> archive.a\n",
    "> debug.cbor\n",
    "> framework.json\n",
    "> profile.pop\n",
    "> profile.pop_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787cf4f0",
   "metadata": {},
   "source": [
    "## PopVision - reading the reports\n",
    "\n",
    "When you open such a report, you could navigate to multiple reports.\n",
    "Here you can see example views generated for a set of parameters in this\n",
    "tutorial: pipelining enabled, for 2 IPUs with gradient accumulation enabled.\n",
    "\n",
    "![Execution Trace](static/popvision_multiple_ipus.png)\n",
    "\n",
    "Also when you go to the Operations Summary and in the top left corner you \n",
    "select Tensorflow Layer, you can observe that SGD and gradient accumulation\n",
    "was performed.\n",
    "![Execution Trace](static/enabled_gradient_accumulator_and_SGD.png)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
