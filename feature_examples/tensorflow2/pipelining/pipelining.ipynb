{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "406a984c",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef075ba3",
   "metadata": {},
   "source": [
    "# TensorFlow 2: Model Parallelism with IPU Pipelining\n",
    "\n",
    "In this tutorial you will train a selection of simple fully connected models\n",
    "on the MNIST numeral data set and see how training can be parallelised over\n",
    "multiple IPU devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ed7d58",
   "metadata": {},
   "source": [
    "## Model Parallelism With Pipelining\n",
    "\n",
    "With pipelining, as with sharding, the model is split into stages where each\n",
    "stage can fit and be run on a single IPU. However, unlike sharding, the compute\n",
    "for separate batches is overlapped so that execution of the model\n",
    "is parallelised. That is, each stage (part of the original model) is executed\n",
    "on its IPU while the IPUs allocated to previous stages are already working on\n",
    "subsequent batches. This provides improved utilisation compared to sharding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a9628c",
   "metadata": {},
   "source": [
    "![Pipelining outline](static/pipelining_outline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e147f6",
   "metadata": {},
   "source": [
    "Refer to the technical note on TensorFlow Model Parallelism for full details:\n",
    "[TensorFlow Model Parallelism - Pipelining](<https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html#pipelining>)/\n",
    "\n",
    "Pipelining provides a method to run larger models that is conceptually less\n",
    "straightforward compared to sharding. However, it offers better utilisation of\n",
    "the allocated IPU resource and, for this reason, pipelining is recommended\n",
    "where performance is critical.\n",
    "This tutorial focuses on how to apply pipelining in TensorFlow 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcefb62",
   "metadata": {},
   "source": [
    "### Pipeline Execution Phases\n",
    "It is important to understand the key phases of pipeline execution:\n",
    "\n",
    "1. Ramp up -  the pipeline is being filled; work is flowing into each stage \n",
    "until all stages are filled (all IPUs are busy).\n",
    "2. Main execution - all stages are filled and IPU utilisation is maximised.\n",
    "3. Ramp down - the pipeline is being drained; work is flowing out of each stage \n",
    "until all stages are empty (no IPUs are busy).\n",
    "4. Weight updates - all pipeline batches have been processed, so accumulated \n",
    "gradients can be processed (gradient descent) and weights updated.\n",
    "Note:  \n",
    "* Each individual batch passed through the pipeline is called a **mini-batch**.  \n",
    "* Weights are updated only once a set of mini-batches has been fully processed.  \n",
    "* Gradients are accumulated across a set of mini-batches.  \n",
    "* Weight updates are applied once all the complete set of mini-batches are \n",
    "processed.  \n",
    "\n",
    "In short, pipelining enforces **gradient accumulation** where:  \n",
    "`effective batch size = mini-batch size * gradient accumulation count`  \n",
    "Performing gradient accumulation is still valid because summing the gradients \n",
    "across all the examples in a batch immediately and accumulating them over \n",
    "several steps are equivalent.  \n",
    "Increasing the gradient accumulation count has these benefits:\n",
    "1. A smaller proportion of time is spent in the ramp up and ramp down - that is, \n",
    "more time is spent in the main execution phase where maximum utilisation of the \n",
    "IPUs is made.\n",
    "2. Fewer overall weight updates are made, which saves compute time.\n",
    "Here is the pipeline outline extended to show the progress of 16 mini-batches \n",
    "followed by a weight update. Notice that the best utilization of the IPUs is \n",
    "during the main phase and that this is sustained until the last mini-batch enters \n",
    "the pipeline, following which the ramp down begins. Also notice that weight \n",
    "updates are only applied once, following the ramp down (after the pipeline has \n",
    "been drained of all mini-batches)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991a10d",
   "metadata": {},
   "source": [
    "## Pipelining Schedules\n",
    "\n",
    "In this tutorial, we will create models using the Keras Model class and IPU \n",
    "pipelining features. We are going to use Pipeline Stages to assign operations\n",
    "to devices and to configure parallelism.\n",
    "\n",
    "In the following graphics, FWD and BWD refer to forward and backward passes.\n",
    "\n",
    "The computational stages can be interleaved on the devices in three different \n",
    "ways as described by the `pipeline_schedule` parameter. By default the API \n",
    "will use the `PipelineSchedule.Grouped` mode, where the forward passes are \n",
    "grouped together, and the backward passes are grouped together. \n",
    "![Grouped pipeline](static/grouped_pipeline.png)\n",
    "\n",
    "The main alternative is the `PipelineSchedule.Interleaved` mode, where the \n",
    "forward and backward passes are interleaved, so that fewer activations need \n",
    "to be stored. \n",
    "![Interleaved pipeline](static/interleaved_pipeline.png)\n",
    "\n",
    "Additionally, the `PipelineSchedule.Sequential` mode, where the pipeline is \n",
    "scheduled in the same way as if it were a sharded model, may be useful when \n",
    "debugging your model.\n",
    "![Sharded pipeline](static/sharded_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd86591",
   "metadata": {},
   "source": [
    "## Upgrading to TensorFlow 2\n",
    "\n",
    "Considering that IPU computation can be enabled on both TensorFlow 1 \n",
    "and Tensorflow 2 it is necessary to explain the major differences between them\n",
    "and how it affects implementation of IPU specific code.\n",
    "\n",
    "### Device scopes\n",
    "In IPU APIs for TF2, the scope context `ipu.scopes.ipu_scope(device_id)` was\n",
    "replaced with a strategy context `ipu.ipu_strategy.IPUStrategy().scope()`.\n",
    "\n",
    "### Training loop\n",
    "Since TF2 moved in the direction of eager execution, we no longer are required\n",
    "to create sessions and use them as context (`with tf.Session()...`). Instead, \n",
    "when using the Keras API, we can use the model instance directly and invoke\n",
    "`model.compile()`, `model.fit()`, and `model.predict()` methods without\n",
    "specifing explicitly the training loop. To enable IPUs, it is just required\n",
    "that these invocations are executed under `IPUStrategy` scope.\n",
    "\n",
    "### Keras extensions to facilitate IPU computing\n",
    "You can find the main documentation on the [GraphCore Keras for IPU](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/keras_tf2.html) page.\n",
    "The framework has been extended to enable IPU devices usage and configuration.\n",
    "All the new code can be found within the `tensorflow.python.ipu` package.\n",
    "\n",
    "### TF2 specific changes\n",
    "There is a guide prepared by the TensorFlow team to conduct migration between\n",
    "versions of TensorFlow library, which you can study [here](https://www.tensorflow.org/guide/migrate).\n",
    "\n",
    "A very exhaustive comparison of both versions can be found [here](https://www.tensorflow.org/guide/migrate/tf1_vs_tf2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d63de1",
   "metadata": {},
   "source": [
    "## Tutorial Walkthrough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6197680",
   "metadata": {},
   "source": [
    "This cell contains the constants applied to the whole tutorial. When running\n",
    "this tutorial in a Jupyter Notebook, make sure all the cells below \n",
    "are re-run (including this one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf1257",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per batch.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Number of steps to run per execution. The number of batches to run for\n",
    "# each TensorFlow function call. At most it would execute a full epoch.\n",
    "STEPS_PER_EXECUTION = 500\n",
    "\n",
    "# Number of steps per epoch. The total number of steps (batches of samples)\n",
    "# for one epoch to finish and starting the next one. The default `None` is\n",
    "# equal to the number of samples divided by the batch size.\n",
    "STEPS_PER_EPOCH = STEPS_PER_EXECUTION\n",
    "\n",
    "# Number of epochs\n",
    "EPOCHS = 4\n",
    "\n",
    "# Optimizer parameters.\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Number of devices that will be attached to this model for training and\n",
    "# inference.\n",
    "NUM_IPUS = 2\n",
    "\n",
    "# Number of steps for which the gradients should be accumulated, for each\n",
    "# configured replica.\n",
    "STEPS_PER_REPLICA = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd1df2",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b891ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python import ipu\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.python.keras.engine.sequential import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af3fc0",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "We need to load the dataset and perform some normalization of values. Below\n",
    "you will find a helper function to use inside IPU context, which will load\n",
    "the input data with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cbd201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(batch_size: int, repeat=True):\n",
    "    mnist = keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    train_ds = train_ds.map(\n",
    "        lambda d, l: (tf.cast(d, tf.float32), tf.cast(l, tf.float32))\n",
    "    )\n",
    "    if repeat:\n",
    "        return train_ds.repeat()\n",
    "    else:\n",
    "        return train_ds\n",
    "\n",
    "\n",
    "train_ds = create_dataset(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81c2913",
   "metadata": {},
   "source": [
    "Initialise the IPU configuration - more details can be found in `IPUConfig`\n",
    "[documentation](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#tensorflow.python.ipu.config.IPUConfig).\n",
    "Creating new instance of `IPUConfig` and running `configure_ipu_system` always\n",
    "reattaches the devices, freeing the resources if they were occupied by this\n",
    "process. It is important when a Jupyter Notebook is used and the kernel\n",
    "is still running, and it does not release IPU devices automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6eab90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_ipus(\n",
    "        num_ipus: int,\n",
    "        selection_order: Optional[ipu.utils.SelectionOrder] = None\n",
    ") -> ipu.config.IPUConfig:\n",
    "\n",
    "    ipu_configuration = ipu.config.IPUConfig()\n",
    "    ipu_configuration.auto_select_ipus = num_ipus\n",
    "\n",
    "    if selection_order:\n",
    "        ipu_configuration.selection_order = selection_order\n",
    "\n",
    "    ipu_configuration.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0749f8e5",
   "metadata": {},
   "source": [
    "This will be the training function reused by all the kinds of models and modes\n",
    "of pipelining.\n",
    "> Note: model creation needs to be processed under the `IPUStrategy().scope()`,\n",
    "> hence this function accepts only the reference to the function which performs\n",
    "> the model creation, not the model instance (as `model_factory` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea46b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(strategy,\n",
    "          model_factory,\n",
    "          train_ds,\n",
    "          steps_per_replica: int = STEPS_PER_REPLICA,\n",
    "          steps_per_execution: int = STEPS_PER_EXECUTION,\n",
    "          steps_per_epoch: int = STEPS_PER_EPOCH,\n",
    "          epochs: int = 4):\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = model_factory()\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True\n",
    "            ),\n",
    "            optimizer=tf.keras.optimizers.SGD(\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                momentum=MOMENTUM\n",
    "            ),\n",
    "            steps_per_execution=steps_per_execution\n",
    "        )\n",
    "\n",
    "        if steps_per_replica:\n",
    "            model.set_pipelining_options(\n",
    "                gradient_accumulation_steps_per_replica=steps_per_replica\n",
    "            )\n",
    "\n",
    "        model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a81839f",
   "metadata": {},
   "source": [
    "## Training a Keras `Functional` model on a single IPU\n",
    "\n",
    "Next let's define a function which returns a `Functional` Keras model. This\n",
    "implementation looks very similar to a regular non-IPU Keras model definition.\n",
    "\n",
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a19efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_functional_model(batch_size=BATCH_SIZE):\n",
    "    input_layer = Input(\n",
    "        shape=(28, 28, 1),\n",
    "        dtype=tf.float32,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    x = Flatten(name='flatten')(input_layer)\n",
    "    x = Dense(256, activation='relu', name=\"dense256\")(x)\n",
    "    x = Dense(128, activation='relu', name=\"dense128\")(x)\n",
    "    x = Dense(64, activation='relu', name=\"dense64\")(x)\n",
    "    x = Dense(32, activation='relu', name=\"dense32\")(x)\n",
    "    x = Dense(10, name=\"logits\")(x)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=input_layer,\n",
    "        outputs=x,\n",
    "        name=\"singleIPU\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ffb4b8",
   "metadata": {},
   "source": [
    "### Execute Training\n",
    "\n",
    "It is essential to create a fresh instance of `IPUConfig` and `IPUStrategy`\n",
    "before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d09d9e8",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "configure_ipus(num_ipus=1)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_functional_model,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a0ae7",
   "metadata": {},
   "source": [
    "## Training a Keras `Sequential` model on a single IPU\n",
    "\n",
    "Let us organize the same layers using the `Sequential` Keras model API.\n",
    "This class groups a linear stack of layers into a `tf.Keras.Model`. \n",
    "Then, `Sequential` provides training and inference features on this model.\n",
    "\n",
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91a980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_model():\n",
    "    seq_model = Sequential(\n",
    "        layers=[\n",
    "            Flatten(name='flatten'),\n",
    "            Dense(256, activation='relu', name=\"dense256\"),\n",
    "            Dense(128, activation='relu', name=\"dense128\"),\n",
    "            Dense(64, activation='relu', name=\"dense64\"),\n",
    "            Dense(32, activation='relu', name=\"dense32\"),\n",
    "            Dense(10, activation='softmax', name=\"logits\")\n",
    "        ],\n",
    "        name=\"singleIPU\"\n",
    "    )\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781ade11",
   "metadata": {},
   "source": [
    "### Execute Training\n",
    "\n",
    "Next we refresh IPU device configuration and train again with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222b60e",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "configure_ipus(num_ipus=1)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_sequential_model,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c6c38a",
   "metadata": {},
   "source": [
    "##  Training a Keras `Functional` model with pipelining for two devices\n",
    "\n",
    "The documentation of Pipeline Stages can be found [here](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/perf_training.html#pipelined-training).\n",
    "There are two ways to enable IPU pipelining for a Keras model, depending on\n",
    "if the user is writing a new model or using an existing model.\n",
    "\n",
    "To pipeline a `Functional` model you are writing yourself, each layer call must\n",
    "happen within the scope of an `ipu.keras.PipelineStage` context.\n",
    "In the function below, we assign layers to two different stages.\n",
    "\n",
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bf8fe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_functional_model_with_stages():\n",
    "    input_layer = Input(shape=(28, 28, 1),\n",
    "                                     dtype=tf.float32,\n",
    "                                     batch_size=BATCH_SIZE)\n",
    "    with ipu.keras.PipelineStage(0):\n",
    "        x = Flatten(name='flatten')(input_layer)\n",
    "        x = Dense(256, activation='relu', name=\"dense256\")(x)\n",
    "        x = Dense(128, activation='relu', name=\"dense128\")(x)\n",
    "        x = Dense(64, activation='relu', name=\"dense64\")(x)\n",
    "\n",
    "    with ipu.keras.PipelineStage(1):\n",
    "        x = Dense(32, activation='relu', name=\"dense32\")(x)\n",
    "        x = Dense(10, name=\"logits\")(x)\n",
    "\n",
    "    model = Model(inputs=input_layer,\n",
    "                  outputs=x,\n",
    "                  name=\"multipleIPUfunctional\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e0f46",
   "metadata": {},
   "source": [
    "In case an existing `TensorFlow` model is imported, an additional API\n",
    "is provided to facilitate managing Pipeline Stages assignments.\n",
    "\n",
    "This feature is implemented with `model.get_pipeline_stage_assignment()`\n",
    "and `model.set_pipeline_stage_assignment(assignments)` where `assignments` is\n",
    "the result of calling `get_pipeline_stage_assignment`.\n",
    "For an example with the ResNet50 please check this [documentation](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/keras_tf2.html#pipelining-an-existing-functional-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5899f0c",
   "metadata": {},
   "source": [
    "### Execute Training\n",
    "\n",
    "Next we refresh IPU device configuration and train again with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d281a1",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "configure_ipus(num_ipus=2)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_functional_model_with_stages,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e466251",
   "metadata": {},
   "source": [
    "##  Training a Keras `Sequential model` with pipelining\n",
    "\n",
    "Next we will write a function to create the model using the Keras `Sequential`\n",
    "class as we did above, but with explicit mapping of layers to stages through \n",
    "`set_pipeline_stage_assignment`, which accepts a list of integers as\n",
    "a parameter. This function sets the pipeline stage assignment for all\n",
    "the invocations of all the layers (excluding input layers) in the model\n",
    "which is used to create a model-parallel execution when calling `fit()`,\n",
    "`evaluate()` and `predict()`. \n",
    "\n",
    ">This pipelining stage assignment is ignored when using the `call()` function\n",
    ">on this model.\n",
    "\n",
    "Below you will see pipeline stage assignment like this: `[0, 0, 0, 0, 1, 1])`. \n",
    "This means that first two layers of `Sequential` model are assigned to \n",
    "the first stage, and the remaining layers to the second stage.\n",
    "\n",
    "This list has to be has to be of the same length as the total number\n",
    "of invocations of all the layers in this model, excluding input layers.\n",
    "\n",
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501533e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_sequential_model():\n",
    "    seq_model = Sequential(\n",
    "        layers=[\n",
    "            Flatten(name='flatten'),\n",
    "            Dense(256, activation='relu', name=\"dense256\"),\n",
    "            Dense(128, activation='relu', name=\"dense128\"),\n",
    "            Dense(64, activation='relu', name=\"dense64\"),\n",
    "            Dense(32, activation='relu', name=\"dense32\"),\n",
    "            Dense(10, activation='softmax', name=\"logits\")\n",
    "        ],\n",
    "        name=\"multipleIPUsequential\"\n",
    "    )\n",
    "    seq_model.set_pipeline_stage_assignment([0, 0, 0, 0, 1, 1])\n",
    "\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf5405",
   "metadata": {},
   "source": [
    "### Execute Training\n",
    "\n",
    "Next we refresh IPU device configuration and train again with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189270ba",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "configure_ipus(num_ipus=2)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_pipeline_sequential_model,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce938b7c",
   "metadata": {},
   "source": [
    "## Other `PipelineSchedule` settings\n",
    "\n",
    "Next we can reuse the previous example and apply a different scheduling mode.\n",
    "The modes can be characterized in detail like so (quoting the docstring of \n",
    "`PipelineSchedule`):\n",
    "\n",
    "- `Grouped`: This groups the forward passes on multiple IPUs. This requires\n",
    "  more memory since activations need to be stored until the backward stages run\n",
    "  together. However, since forward passes tend to be smaller than backward \n",
    "  passes, `Grouped` tends to improve the speed of the execution, as different \n",
    "  IPUs don't spend so much time waiting for each other.\n",
    "\n",
    "- `Interleaved`: This schedules the backward passes whenever the forward passes\n",
    "  have just generated some activations.  Consequently fewer activations are \n",
    "  required to be stored between the forward and backward pipeline stages, so \n",
    "  less memory is required.  However, since forward and backward stages tend to \n",
    "  be very different in terms of execution cycles, the overall performance \n",
    "  of the pipeline tends to be slower.\n",
    "\n",
    "- `Sequential`: This is a debug mode, where the pipeline is scheduled in\n",
    "  the same way as if it were a sharded model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b29d222",
   "metadata": {},
   "source": [
    "### Defining the model with `Interleaved` schedule\n",
    "\n",
    "The mode `Grouped` was used in the previous example, as it is the default\n",
    "setting. In this next example we will use the `Interleaved` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9e0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_sequential_model_interleaved():\n",
    "    seq_model = Sequential(\n",
    "        layers=[\n",
    "            Flatten(name='flatten'),\n",
    "            Dense(256, activation='relu', name=\"dense256\"),\n",
    "            Dense(128, activation='relu', name=\"dense128\"),\n",
    "            Dense(64, activation='relu', name=\"dense64\"),\n",
    "            Dense(32, activation='relu', name=\"dense32\"),\n",
    "            Dense(10, activation='softmax', name=\"logits\")\n",
    "        ],\n",
    "        name=\"multipleIPUsequential\"\n",
    "    )\n",
    "    seq_model.set_pipeline_stage_assignment([0, 0, 1, 1, 1, 1])\n",
    "\n",
    "    seq_model.set_pipelining_options(\n",
    "        schedule=ipu.ops.pipelining_ops.PipelineSchedule.Interleaved\n",
    "    )\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dda444",
   "metadata": {},
   "source": [
    "### Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aefa12f",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "configure_ipus(num_ipus=2)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_pipeline_sequential_model_interleaved,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b20fd8",
   "metadata": {},
   "source": [
    "## Summary and further reading\n",
    "\n",
    "In the course of this tutorial multiple examples of model parallelism with IPU\n",
    "devices were presented. Try and change some hyperparameters or IPU count and\n",
    "observe the differences. You can investigate details of execution using \n",
    "the PopVision tool.\n",
    "\n",
    "If you execute this code with environmental variable:\n",
    "```bash\n",
    "POPLAR_ENGINE_OPTIONS='{\"autoReport.all\":\"true\"}' python3 pipelining.py\n",
    "```\n",
    "Or set this variable inside Jupyter Notebook:\n",
    "```python\n",
    "import os\n",
    "os.environ['POPLAR_ENGINE_OPTIONS']='{\"autoReport.all\":\"true\"}'\n",
    "```\n",
    "Then you could use the generated report, which for this tutorial might look\n",
    "like this:\n",
    "```bash\n",
    "ls .\n",
    "> ./tf_report__2021-10-06__02-24-24.631__70052:\n",
    "> archive.a\n",
    "> debug.cbor\n",
    "> framework.json\n",
    "> profile.pop\n",
    "> profile.pop_cache\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b438c0",
   "metadata": {},
   "source": [
    "## PopVision - reading the reports\n",
    "When you open such a report, you could navigate to multiple tabs which present\n",
    "different aspects of the IPU computation. You can find more information on the\n",
    "[PopVision User Guide](https://docs.graphcore.ai/projects/graphcore-popvision-user-guide/en/latest/index.html) page.\n",
    "\n",
    "For further reading about related topics please check:\n",
    "- [Keras API docs](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#keras)\n",
    "- [Upgrading from TF2.1](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/keras_tf2.html#porting-models-from-tensorflow-2-1)\n",
    "- [Automatic Data Parallelism](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/keras_tf2.html#automatic-data-parallelism)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
