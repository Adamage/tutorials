{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2b54d7f",
   "metadata": {},
   "source": [
    "Copyright (c) 2021 Graphcore Ltd. All rights reserved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831cc0dc",
   "metadata": {},
   "source": [
    "# TensorFlow 2: Model Parallelism with IPU Pipelining\n",
    "\n",
    "In this tutorial you will train a selection of simple fully connected models\n",
    "on the MNIST numeral data set and see how training can be parallelised over\n",
    "multiple IPU devices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b30be",
   "metadata": {},
   "source": [
    "## Upgrading to TensorFlow 2\n",
    "\n",
    "Considering that IPU computation can be enabled on both TensorFlow 1 \n",
    "and Tensorflow 2 it is necessary to explain the major differences between them\n",
    "and how it affects implementation of IPU specific code.\n",
    "\n",
    "### Device scopes\n",
    "In IPU APIs for TF2, the scope context `ipu.scopes.ipu_scope(device_id)` was\n",
    "replaced with a strategy context `ipu.ipu_strategy.IPUStrategy().scope().\n",
    "\n",
    "### Training loop\n",
    "Since TF2 moved in the direction of eager execution, we no longer are required\n",
    "to create sessions and use them as context (`with tf.Session()...`). Instead, \n",
    "when using the Keras API, we can use the model instance directly and invoke\n",
    "`model.compile()`, `model.fit()`, and `model.predict()` methods without\n",
    "specifing explicitly the training loop. To enable IPUs, it is just required\n",
    "that these invocations are executed under `IPUStrategy` scope.\n",
    "\n",
    "### Keras extensions to facilitate IPU computing\n",
    "You can find the main documentation on the [GraphCore Keras for IPU](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/keras_tf2.html) page.\n",
    "The framework has been extended to enable IPU devices usage and configuration.\n",
    "All the new code can be found within the `tensorflow.python.ipu` package.\n",
    "\n",
    "### TF2 specific changes\n",
    "There is a guide prepared by the TensorFlow team to conduct migration between\n",
    "versions of TensorFlow library, which you can study [here](https://www.tensorflow.org/guide/migrate).\n",
    "\n",
    "A very exhaustive comparison of both versions can be found [here](https://www.tensorflow.org/guide/migrate/tf1_vs_tf2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebb107a",
   "metadata": {},
   "source": [
    "## Pipelining features\n",
    "In this tutorial, we will create models using the Keras Model class and IPU \n",
    "pipelining features.\n",
    "\n",
    "We are going to use Pipeline Stages to assign operations to devices and to\n",
    "configure parallelism.\n",
    "\n",
    "In the following graphics, FWD and BWD refer to forward and backward passes.\n",
    "\n",
    "The computational stages can be interleaved on the devices in three different \n",
    "ways as described by the `pipeline_schedule` parameter. By default the API \n",
    "will use the `PipelineSchedule.Grouped` mode, where the forward passes are \n",
    "grouped together, and the backward passes are grouped together. \n",
    "![Grouped pipeline](static/grouped_pipeline.png)\n",
    "\n",
    "The main alternative is the `PipelineSchedule.Interleaved` mode, where the \n",
    "forward and backward passes are interleaved, so that fewer activations need \n",
    "to be stored. \n",
    "![Interleaved pipeline](static/interleaved_pipeline.png)\n",
    "\n",
    "Additionally, the `PipelineSchedule.Sequential` mode, where the pipeline is \n",
    "scheduled in the same way as if it were a sharded model, may be useful when \n",
    "debugging your model.\n",
    "![Sharded pipeline](static/sharded_pipeline.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fafbe60",
   "metadata": {},
   "source": [
    "This cell contains the constants applied to the whole tutorial. When modifying\n",
    "these, make sure all the cells below are re-run (including this one)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bcb0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of samples per batch.\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Number of steps to run per execution. The number of batches to run for\n",
    "# each TensorFlow function call. At most it would execute a full epoch.\n",
    "STEPS_PER_EXECUTION = 500\n",
    "\n",
    "# Number of steps per epoch. The total number of steps (batches of samples)\n",
    "# for one epoch to finish and starting the next one. The default `None` is\n",
    "# equal to the number of samples divided by the batch size.\n",
    "STEPS_PER_EPOCH = STEPS_PER_EXECUTION\n",
    "\n",
    "# Number of epochs\n",
    "EPOCHS = 4\n",
    "\n",
    "# Optimizer parameters.\n",
    "LEARNING_RATE = 0.01\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Number of devices that will be attached to this model for training and\n",
    "# inference.\n",
    "NUM_IPUS = 2\n",
    "\n",
    "# Number of steps for which the gradients should be accumulated, for each\n",
    "# configured replica.\n",
    "STEPS_PER_REPLICA = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a57b79f",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f517b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python import ipu\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.python.keras.engine.sequential import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bc210f",
   "metadata": {},
   "source": [
    "## Dataset preparation\n",
    "We need to load the dataset and perform some normalization of values. Below\n",
    "you will find a helper function to use inside IPU context, which will load\n",
    "the input data with labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(batch_size: int, repeat=True):\n",
    "    mnist = keras.datasets.mnist\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "    train_ds = train_ds.shuffle(10000).batch(batch_size, drop_remainder=True)\n",
    "    train_ds = train_ds.map(\n",
    "        lambda d, l: (tf.cast(d, tf.float32), tf.cast(l, tf.float32))\n",
    "    )\n",
    "    if repeat:\n",
    "        return train_ds.repeat()\n",
    "    else:\n",
    "        return train_ds\n",
    "\n",
    "\n",
    "train_ds = create_dataset(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390e529b",
   "metadata": {},
   "source": [
    "Initialise IPU configuration - more details here [`IPUConfig`](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#tensorflow.python.ipu.config.IPUConfig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3235e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ipu_config(\n",
    "        num_ipus: int,\n",
    "        selection_order: Optional[ipu.utils.SelectionOrder] = None\n",
    ") -> ipu.config.IPUConfig:\n",
    "\n",
    "    ipu_configuration = ipu.config.IPUConfig()\n",
    "    ipu_configuration.auto_select_ipus = num_ipus\n",
    "\n",
    "    if selection_order:\n",
    "        ipu_configuration.selection_order = selection_order\n",
    "\n",
    "    ipu_configuration.configure_ipu_system()\n",
    "    return ipu_configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dbc749",
   "metadata": {},
   "source": [
    "This will be the training function reused by all the kinds of models and modes\n",
    "of pipelining.\n",
    "> Note: model creation needs to be processed under the `IPUStrategy().scope()`,\n",
    "> hence this function accepts only the reference to the function which performs\n",
    "> the model creation, not the model instance (as `model_factory` argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45c9cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(strategy,\n",
    "          model_factory,\n",
    "          train_ds,\n",
    "          steps_per_replica: int = STEPS_PER_REPLICA,\n",
    "          steps_per_execution: int = STEPS_PER_EXECUTION,\n",
    "          steps_per_epoch: int = STEPS_PER_EPOCH,\n",
    "          epochs: int = 4):\n",
    "\n",
    "    with strategy.scope():\n",
    "        model = model_factory()\n",
    "\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                from_logits=True\n",
    "            ),\n",
    "            optimizer=tf.keras.optimizers.SGD(\n",
    "                learning_rate=LEARNING_RATE,\n",
    "                momentum=MOMENTUM\n",
    "            ),\n",
    "            steps_per_execution=steps_per_execution\n",
    "        )\n",
    "\n",
    "        if steps_per_replica:\n",
    "            model.set_pipelining_options(\n",
    "                gradient_accumulation_steps_per_replica=steps_per_replica\n",
    "            )\n",
    "\n",
    "        model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aecc73",
   "metadata": {},
   "source": [
    "## Training a Keras `Functional` model on a single IPU\n",
    "\n",
    "Next let's define a function which returns a `Functional` Keras model. This\n",
    "implementation looks very similar to a regular non-IPU Keras model definition.\n",
    "\n",
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b2d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_functional_model(batch_size=BATCH_SIZE):\n",
    "    input_layer = Input(\n",
    "        shape=(28, 28, 1),\n",
    "        dtype=tf.float32,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "    x = Flatten(name='flatten')(input_layer)\n",
    "    x = Dense(256, activation='relu', name=\"dense256\")(x)\n",
    "    x = Dense(128, activation='relu', name=\"dense128\")(x)\n",
    "    x = Dense(64, activation='relu', name=\"dense64\")(x)\n",
    "    x = Dense(32, activation='relu', name=\"dense32\")(x)\n",
    "    x = Dense(10, name=\"logits\")(x)\n",
    "\n",
    "    model = Model(\n",
    "        inputs=input_layer,\n",
    "        outputs=x,\n",
    "        name=\"singleIPU\"\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab974c82",
   "metadata": {},
   "source": [
    "### Execute Training\n",
    "\n",
    "It is essential to create a fresh instance of `IPUConfig` and `IPUStrategy`\n",
    "before training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80435ca4",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "ipu_configuration = make_ipu_config(num_ipus=1)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_functional_model,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87632e72",
   "metadata": {},
   "source": [
    "## Training a Keras `Sequential` model on a single IPU\n",
    "\n",
    "Let us organize the same layers using the `Sequential` Keras model API.\n",
    "This class groups a linear stack of layers into a `tf.Keras.Model`. \n",
    "Then, `Sequential` provides training and inference features on this model.\n",
    "\n",
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8752c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequential_model():\n",
    "    seq_model = Sequential(\n",
    "        layers=[\n",
    "            Flatten(name='flatten'),\n",
    "            Dense(256, activation='relu', name=\"dense256\"),\n",
    "            Dense(128, activation='relu', name=\"dense128\"),\n",
    "            Dense(64, activation='relu', name=\"dense64\"),\n",
    "            Dense(32, activation='relu', name=\"dense32\"),\n",
    "            Dense(10, activation='softmax', name=\"logits\")\n",
    "        ],\n",
    "        name=\"singleIPU\"\n",
    "    )\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea232b27",
   "metadata": {},
   "source": [
    "### Execute Training\n",
    "\n",
    "Next we refresh IPU device configuration and train again with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355daeac",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "ipu_configuration = make_ipu_config(num_ipus=1)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_sequential_model,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e431e",
   "metadata": {},
   "source": [
    "##  Training a Keras `Functional` model with pipelining for two devices\n",
    "\n",
    "The documentation of Pipeline Stages can be found [here](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/perf_training.html#pipelined-training).\n",
    "There are two ways to enable IPU pipelining for a Keras model, depending on\n",
    "if the user is writing a new model or using an existing model.\n",
    "\n",
    "To pipeline a `Functional` model you are writing yourself, each layer call must\n",
    "happen within the scope of an `ipu.keras.PipelineStage` context.\n",
    "In the function below, we assign layers to two different stages.\n",
    "\n",
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f950527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_functional_model_with_stages():\n",
    "    input_layer = Input(shape=(28, 28, 1),\n",
    "                                     dtype=tf.float32,\n",
    "                                     batch_size=BATCH_SIZE)\n",
    "    with ipu.keras.PipelineStage(0):\n",
    "        x = Flatten(name='flatten')(input_layer)\n",
    "        x = Dense(256, activation='relu', name=\"dense256\")(x)\n",
    "        x = Dense(128, activation='relu', name=\"dense128\")(x)\n",
    "        x = Dense(64, activation='relu', name=\"dense64\")(x)\n",
    "\n",
    "    with ipu.keras.PipelineStage(1):\n",
    "        x = Dense(32, activation='relu', name=\"dense32\")(x)\n",
    "        x = Dense(10, name=\"logits\")(x)\n",
    "\n",
    "    model = Model(inputs=input_layer,\n",
    "                  outputs=x,\n",
    "                  name=\"multipleIPUfunctional\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df16a736",
   "metadata": {},
   "source": [
    "In case an existing `TensorFlow` model is imported, an additional API\n",
    "is provided to facilitate managing Pipeline Stages assignments.\n",
    "\n",
    "This feature is implemented with `model.get_pipeline_stage_assignment()`\n",
    "and `model.set_pipeline_stage_assignment(assignments)` where `assignments` is\n",
    "the result of calling `get_pipeline_stage_assignment`.\n",
    "For an example with the ResNet50 please check this [documentation](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/keras_tf2.html#pipelining-an-existing-functional-model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494cdfd4",
   "metadata": {},
   "source": [
    "### Execute Training\n",
    "\n",
    "Next we refresh IPU device configuration and train again with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74624c11",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "ipu_configuration = make_ipu_config(num_ipus=2)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_functional_model_with_stages,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423789b6",
   "metadata": {},
   "source": [
    "##  Training a Keras `Sequential model` with pipelining\n",
    "\n",
    "Next we will write a function to create the model using the Keras `Sequential`\n",
    "class as we did above, but with explicit mapping of layers to stages through \n",
    "`set_pipeline_stage_assignment`, which accepts a list of integers as\n",
    "a parameter. This function sets the pipeline stage assignment for all\n",
    "the invocations of all the layers (excluding input layers) in the model\n",
    "which is used to create a model-parallel execution when calling `fit()`,\n",
    "`evaluate()` and `predict()`. \n",
    "\n",
    ">This pipelining stage assignment is ignored when using the `call()` function\n",
    ">on this model.\n",
    "\n",
    "Below you will see pipeline stage assignment like this: `[0, 0, 1, 1 1, 1])`. \n",
    "This means that first two layers of `Sequential` model are assigned to \n",
    "the first stage, and the remaining layers to the second stage.\n",
    "\n",
    "This list has to be has to be of the same length as the total number\n",
    "of invocations of all the layers in this model, excluding input layers.\n",
    "\n",
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615f95d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_sequential_model():\n",
    "    seq_model = Sequential(\n",
    "        layers=[\n",
    "            Flatten(name='flatten'),\n",
    "            Dense(256, activation='relu', name=\"dense256\"),\n",
    "            Dense(128, activation='relu', name=\"dense128\"),\n",
    "            Dense(64, activation='relu', name=\"dense64\"),\n",
    "            Dense(32, activation='relu', name=\"dense32\"),\n",
    "            Dense(10, activation='softmax', name=\"logits\")\n",
    "        ],\n",
    "        name=\"multipleIPUsequential\"\n",
    "    )\n",
    "    seq_model.set_pipeline_stage_assignment([0, 0, 1, 1, 1, 1])\n",
    "\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a449687",
   "metadata": {},
   "source": [
    "### Execute Training\n",
    "\n",
    "Next we refresh IPU device configuration and train again with the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2aef3",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "ipu_configuration = make_ipu_config(num_ipus=2)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_pipeline_sequential_model,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4345237f",
   "metadata": {},
   "source": [
    "## Showcasing the `PipelineSchedule` setting effects on training\n",
    "\n",
    "Next we can reuse the previous example and apply different scheduling modes,\n",
    "as mentioned at the top of this document. The modes can be characterized\n",
    "in detail like so (quoting the docstring of `PipelineSchedule`):\n",
    "\n",
    "- `Grouped`: This groups the forward passes on multiple IPUs. This requires\n",
    "  more memory since activations need to be stored until the backward stages run\n",
    "  together. However, since forward passes tend to be smaller than backward \n",
    "  passes, `Grouped` tends to improve the speed of the execution, as different \n",
    "  IPUs don't spend so much time waiting for each other.\n",
    "\n",
    "- `Interleaved`: This schedules the backward passes whenever the forward passes\n",
    "  have just generated some activations.  Consequently fewer activations are \n",
    "  required to be stored between the forward and backward pipeline stages, so \n",
    "  less memory is required.  However, since forward and backward stages tend to \n",
    "  be very different in terms of execution cycles, the overall performance \n",
    "  of the pipeline tends to be slower.\n",
    "\n",
    "- `Sequential`: This is a debug mode, where the pipeline is scheduled in\n",
    "  the same way as if it were a sharded model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e623107a",
   "metadata": {},
   "source": [
    "### Define the model\n",
    "\n",
    "The mode `Grouped` was used in the previous example, as it is the default\n",
    "setting. In this next example we will use the `Interleaved` mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd74a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline_sequential_model_interleaved():\n",
    "    seq_model = Sequential(\n",
    "        layers=[\n",
    "            Flatten(name='flatten'),\n",
    "            Dense(256, activation='relu', name=\"dense256\"),\n",
    "            Dense(128, activation='relu', name=\"dense128\"),\n",
    "            Dense(64, activation='relu', name=\"dense64\"),\n",
    "            Dense(32, activation='relu', name=\"dense32\"),\n",
    "            Dense(10, activation='softmax', name=\"logits\")\n",
    "        ],\n",
    "        name=\"multipleIPUsequential\"\n",
    "    )\n",
    "    seq_model.set_pipeline_stage_assignment([0, 0, 1, 1, 1, 1])\n",
    "\n",
    "    seq_model.set_pipelining_options(\n",
    "        schedule=ipu.ops.pipelining_ops.PipelineSchedule.Interleaved\n",
    "    )\n",
    "    return seq_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968e5749",
   "metadata": {},
   "source": [
    "### Execute training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccd0a4c",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "ipu_configuration = make_ipu_config(num_ipus=2)\n",
    "\n",
    "train(\n",
    "    strategy=ipu.ipu_strategy.IPUStrategy(),\n",
    "    model_factory=create_pipeline_sequential_model_interleaved,\n",
    "    train_ds=train_ds\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
