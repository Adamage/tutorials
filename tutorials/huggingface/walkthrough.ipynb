{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d482b85a",
   "metadata": {},
   "source": [
    "Â© Copyright 2020, The Hugging Face Team, Licenced under the Apache License,\n",
    "Version 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36977e01",
   "metadata": {},
   "source": [
    "# Fine-tuning a pretrained transformer\n",
    "This tutorial demonstrates how to fine-tune a pretrained model from the \n",
    "transformers library using IPUs. The tutorial extends the HuggingFace tutorial [Fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)\n",
    "with IPU specific code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb088b2",
   "metadata": {},
   "source": [
    "### Environment preparation\n",
    "Install the Poplar SDK following the instructions in the Getting Started guide \n",
    "for your IPU system. Make sure to run the enable.sh scripts for Poplar and \n",
    "PopART and activate a Python3 virtualenv with PopTorch installed.\n",
    "\n",
    "Then install the package requirements:\n",
    "```bash\n",
    "pip install -r requirements.txt \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f44ff85",
   "metadata": {},
   "source": [
    "### Preparing the datasets\n",
    "\n",
    "As our data, we use the IMDB dataset containing movie reviews together with \n",
    "information whether the review is positive or negative. To load the data we \n",
    "use the datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21199d0",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3cca8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f9b80f",
   "metadata": {},
   "source": [
    "The `load_datasets` method returns a dictionary containing a dataset which is \n",
    "already split. We use the `train` split for training and the `test` split for \n",
    "validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdf6539",
   "metadata": {},
   "source": [
    "Next, the text must be transformed into a form understandable by the model. \n",
    "For this purpose, we create a function responsible for tokenization, which \n",
    "takes as input a batch from the dataset and returns a tokenized example.\n",
    "Note that we set the `max_length` and `truncation` parameters, this ensures \n",
    "that all examples have the same length. Also, we remove the `text` field as \n",
    "it is not accepted as input to the model.\n",
    "\n",
    "We used ELECTRA as our transformer to train. It is an extension of BERT which \n",
    "is characterised by a shorter training time and therefore fits well into the \n",
    "tutorial. The model description together with implementation details can be \n",
    "found in [HuggingFace documentation](https://huggingface.co/transformers/model_doc/electra.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d058c61a",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-generator\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function,\n",
    "                                      remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef3e2be",
   "metadata": {},
   "source": [
    "When the data has been processed, we can adjust it to the input of our model. \n",
    "To do this, we rename the column and set the format to `torch` which will make \n",
    "the data to be stored in tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b14a6a",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)\n",
    "\n",
    "train_dataset = train_dataset.rename_column(original_column_name='label',\n",
    "                                            new_column_name='labels')\n",
    "eval_dataset = eval_dataset.rename_column(original_column_name='label',\n",
    "                                          new_column_name='labels')\n",
    "\n",
    "train_dataset.set_format(type='torch')\n",
    "eval_dataset.set_format(type='torch')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64f12eb4",
   "metadata": {},
   "source": [
    "Next, when the datasets are ready we proceed to create the dataloaders. \n",
    "This is the first time we will use the capabilities of IPU - instead of using \n",
    "the `DataLoader` class from PyTroch we will use the implementation from \n",
    "PopTorch, which inherits from it and is optimised for memory usage and \n",
    "performance.\n",
    "\n",
    "The dataloading and the execution of the model on the IPU can be controlled \n",
    "using `poptorch.Options`. These options are used by PopTorch's wrappers \n",
    "such as `poptorch.DataLoader` and `poptorch.trainingModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75282f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "\n",
    "opts = poptorch.Options().deviceIterations(8)\n",
    "train_dataloader = poptorch.DataLoader(\n",
    "    options=opts, dataset=train_dataset, shuffle=True, batch_size=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_opts = poptorch.Options().deviceIterations(8) \\\n",
    "    .anchorMode(poptorch.AnchorMode.All)\n",
    "\n",
    "eval_dataloader = poptorch.DataLoader(\n",
    "    options=val_opts, dataset=eval_dataset, shuffle=True, batch_size=4,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d43f89",
   "metadata": {},
   "source": [
    "In this example, we have configured `deviceIteration` and `anchorMode`.\n",
    "\n",
    "**Device iterations** is one cycle of that loop, which runs entirely on the IPU \n",
    "(the device), and which starts with a new batch of data. This option specifies \n",
    "the number of batches that is prepared by the host (CPU) for the IPU. \n",
    "The higher this number, the less the IPU has to interact with the CPU, \n",
    "for example to request and wait for data, so that the IPU can loop faster. \n",
    "However, the user will have to wait for the IPU to go over all the iterations \n",
    "before getting the results back. \n",
    "\n",
    "**Anchor mode** specifies which data is returned from the model located on the \n",
    "IPU to the CPU. By default, PPopTorch will only return the last batch to the \n",
    "host machine after all iterations of the device, which is represented by\n",
    "`AnchorMode.Final`. We set this parameter to `AnchorMode.All` to obtain every\n",
    "model output during the validation stage. This has an impact on the performance,\n",
    "due to overhead of transferring more data to the host machine.\n",
    "\n",
    "The list of other options is available in the [documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/overview.html#options)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c85ef4",
   "metadata": {},
   "source": [
    "We now load the the pretrained model and initialize the optimizer. Note that \n",
    "we use `poptorch.optim.AdamW`, which is optimised for distributed training.\n",
    "More optimizers can be found in the [documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/reference.html#optimizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f135ff59",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from poptorch.optim import AdamW\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google/electra-small-generator\",\n",
    "    num_labels=2,\n",
    "    return_dict=False,\n",
    "    torchscript=True\n",
    ")\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ae7c49",
   "metadata": {},
   "source": [
    "Next, we define how to split the model between the IPU devices. We use 4 \n",
    "devices, and we place the embedding layer on the first device `IPU:0`. \n",
    "The encoder in the ELECTRA model consists of 12 layers, which we distribute \n",
    "equally with 4 layers on each device, finally we place the classifier layer \n",
    "on the last IPU:3 device.\n",
    "\n",
    "In order to place a given layer on a particular device, we wrap it using \n",
    "`poptorch.BeginBlock()`, which takes as arguments the instance of \n",
    "`torch.nn.Module`, the name of the layer for display in the [PopVision profiler](https://docs.graphcore.ai/projects/graphcore-popvision-user-guide/en/latest/), \n",
    "and the device ID on which the layer should be placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e5b49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.electra.embeddings = poptorch.BeginBlock(\n",
    "    model.electra.embeddings, \"Embedding\", ipu_id=0\n",
    ")\n",
    "\n",
    "for index, layer in enumerate(model.electra.encoder.layer):\n",
    "    ipu_id = index // 4 + 1\n",
    "    model.electra.encoder.layer[index] = poptorch.BeginBlock(\n",
    "        layer, f\"Encoder{index}\", ipu_id=ipu_id\n",
    "    )\n",
    "\n",
    "model.classifier = poptorch.BeginBlock(\n",
    "    model.classifier, \"Classifier\", ipu_id=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb286c1",
   "metadata": {},
   "source": [
    "We need to take one more step in order to adapt the model from HuggingFace to \n",
    "work with IPU. When a model uses multiple loss functions, or uses a custom loss \n",
    "function, it has to be wrapped in `poptorch.identity_loss(loss)`.\n",
    "\n",
    "Due to the fact that we can not directly modify the model class, we create \n",
    "a class that takes our ELECTRA model as a parameter and overload the `forward` \n",
    "function, in which we call the `forward` function from ELECTRA and then wrap \n",
    "the returned loss in `identity_loss`. Here we we use composition, however \n",
    "this task could be solved using inheritance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e58b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class IPUModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels):\n",
    "        loss, logits = self.model.forward(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        if self.model.training:\n",
    "            loss = poptorch.identity_loss(loss, reduction=\"none\")\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7849a1",
   "metadata": {},
   "source": [
    "We now create `trainingModel` and `inferenceModel`, for that task we use the \n",
    "`poptorch.trainingModel` and `poptorch.inferenceModel`. They takes an instance \n",
    "of a `torch.nn.Module`, such as our model, an instance of `poptorch.Options` \n",
    "which we have instantiated previously, and an optimizer in case of \n",
    "`poptorch.trainingModel`. This wrapper use TorchScript, and manage translation \n",
    "of our model to a program that can be run using IPU. Then we will compile the \n",
    "models using one batch from our dataset.\n",
    "\n",
    "In addition, the compilation of the model automatically moves it to the device. \n",
    "We would like to use only one model (for training or inference) on the device \n",
    "at a time, as this will allow us to use a larger model. Therefore, we call \n",
    "`detachFromDevice` to detach the model from the IPU device.\n",
    "\n",
    "Compilation may take a few minutes. More information about wrapping function \n",
    "can be found in the [documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/reference.html#model-wrapping-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf03432",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "trainingModel = poptorch.trainingModel(\n",
    "    IPUModel(model), options=opts, optimizer=optimizer\n",
    ")\n",
    "trainingModel.compile(**next(iter(train_dataloader)))\n",
    "trainingModel.detachFromDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d06e31f",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "inferenceModel = poptorch.inferenceModel(\n",
    "    IPUModel(model), options=val_opts\n",
    ")\n",
    "inferenceModel.compile(**next(iter(eval_dataloader)))\n",
    "inferenceModel.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4526d52",
   "metadata": {},
   "source": [
    "Our model and data are ready to run on IPU. We proceed to implement the \n",
    "function responsible for training the model. Here we set the model into the \n",
    "training state, and add a progress bar. We do not need to include \n",
    "`loss.backward()` as `poptorch.trainingModel()` does this itself.\n",
    "\n",
    "At the beginning of our method we attach the model to the IPU using the \n",
    "`attachToDevice()` method, and at the end we detach using `detachFromDevice()`. \n",
    "It is worth noting here that detaching the model from the device will \n",
    "automatically synchronise the updated weights of the model and transfer them \n",
    "to the CPU, so when we attach the model to the inference to the IPU device, \n",
    "the model will have the current state and copying the weights between devices \n",
    "is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef167f5e",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 3\n",
    "number_of_iterations = epochs * (len(train_dataloader) + len(eval_dataloader))\n",
    "progress_bar = tqdm(range(number_of_iterations))\n",
    "\n",
    "\n",
    "def train_epoch():\n",
    "    trainingModel.train()\n",
    "    trainingModel.attachToDevice()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        loss, logits = trainingModel(**batch)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    trainingModel.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fb551a",
   "metadata": {},
   "source": [
    "The function to validate is marked with the decorator `torch.no_grad()`, \n",
    "causes the gradients are not calculated, moreover, in addition to setting the \n",
    "model into the evaluation state, we add storing predictions to count the \n",
    "accuracy at the end of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0442cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch():\n",
    "    inferenceModel.eval()\n",
    "    inferenceModel.attachToDevice()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for batch in eval_dataloader:\n",
    "        y_true.extend(batch['labels'].tolist())\n",
    "        loss, logits = inferenceModel(**batch)\n",
    "\n",
    "        y_pred += logits.argmax(dim=1).tolist()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {acc:.3f}')\n",
    "\n",
    "    inferenceModel.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae7bb4c",
   "metadata": {},
   "source": [
    "Finally, by bringing together everything we have written so far, we can start \n",
    "the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdeb55c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_epoch()\n",
    "    val_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d3862c3",
   "metadata": {},
   "source": [
    "In summary, in this tutorial we have successfully fine-tuned a model from \n",
    "HuggingFace for sentiment prediction using IPU devices. If you are interested \n",
    "in other tutorials you are encouraged to check out [Graphcore Tutorials](https://github.com/graphcore/tutorials)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
