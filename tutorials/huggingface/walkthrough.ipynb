{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407ce62a",
   "metadata": {},
   "source": [
    "© Copyright 2021 Graphcore Ltd. All rights reserved.\n",
    "\n",
    "© Copyright 2020, The Hugging Face Team, Licenced under the Apache License,Version 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20813ebf",
   "metadata": {},
   "source": [
    "# Hugging Face: Fine-tuning a pretrained transformer\n",
    "This tutorial demonstrates how to fine-tune a pretrained model from the Hugging \n",
    "Face transformers library using IPUs. It is base on Hugging Face's [fine-tuning a pretrained model](https://huggingface.co/transformers/training.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2dc632",
   "metadata": {},
   "source": [
    "## Environment preparation\n",
    "Install the Poplar SDK following the instructions in the [Getting Started](https://docs.graphcore.ai/en/latest/software.html#getting-started)\n",
    "guide for your IPU system. Make sure to run the enable.sh scripts for Poplar \n",
    "and PopART and activate a Python3 virtualenv with PopTorch installed.\n",
    "\n",
    "Then install the package requirements:\n",
    "```bash\n",
    "pip install -r requirements.txt \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef83e29e",
   "metadata": {},
   "source": [
    "## Preparing the datasets\n",
    "\n",
    "We use the IMDB dataset for this tutorial. It contains movie reviews together \n",
    "with information on whether the review is positive or negative. To load the \n",
    "data we use the datasets library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526368cb",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "raw_datasets = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceda0ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66095ffb",
   "metadata": {},
   "source": [
    "The `load_datasets` method returns a dictionary containing a dataset which is \n",
    "already split. We use the `train` split for training and the `test` split for \n",
    "validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1ce15",
   "metadata": {},
   "source": [
    "Before we can use the dataset, the text must be transformed into a form \n",
    "understandable by the model. For this purpose, we create a function responsible \n",
    "for tokenization, which takes as input a batch from the dataset and returns the \n",
    "tokenised representation. Note, that we set `max_length` and `truncation` \n",
    "parameters, which ensures that all examples have the same length. You can read \n",
    "more about data preprocessing in the Hugging Face transformers library [here](https://huggingface.co/transformers/preprocessing.html#everything-you-always-wanted-to-know-about-padding-and-truncation). \n",
    "Moreover, we remove the `text` field, because we do not need it as an input to \n",
    "our model.\n",
    "\n",
    "We used ELECTRA as our model. It is an extension of BERT which \n",
    "is characterised by a shorter training time and therefore fits well into this \n",
    "tutorial. The model description together with implementation details can be \n",
    "found in the [Hugging Face transformers documentation].(https://huggingface.co/transformers/model_doc/electra.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24dcb6d",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/electra-small-generator\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = raw_datasets.map(tokenize_function,\n",
    "                                      remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d91df3",
   "metadata": {},
   "source": [
    "When the data has been processed, we can adjust it to be an appropriate input \n",
    "format for our model. To do this, we rename the column and set the format to \n",
    "`torch`, which ensures the data is stored in the PyTorch tensor format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10584d21",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(type='torch')\n",
    "\n",
    "train_dataset = tokenized_datasets[\"train\"].rename_column(\n",
    "    original_column_name='label', new_column_name='labels')\n",
    "eval_dataset = tokenized_datasets[\"test\"].rename_column(\n",
    "    original_column_name='label', new_column_name='labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eea54b",
   "metadata": {},
   "source": [
    "Next, when the datasets are ready we proceed to create the dataloaders. \n",
    "This is the first time we will use the capabilities of IPU - instead of using \n",
    "the `DataLoader` class from PyTorch we will use the implementation from \n",
    "PopTorch, which inherits PyTorch and is optimised for memory usage and \n",
    "performance on the IPU.\n",
    "\n",
    "The data loading and the execution of the model on the IPU can be controlled \n",
    "using `poptorch.Options`. These options are used by PopTorch's wrappers \n",
    "such as `poptorch.DataLoader` and `poptorch.trainingModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa540d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import poptorch\n",
    "\n",
    "opts = poptorch.Options().deviceIterations(8)\n",
    "train_dataloader = poptorch.DataLoader(\n",
    "    options=opts, dataset=train_dataset, shuffle=True, batch_size=4,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_opts = poptorch.Options().deviceIterations(8) \\\n",
    "    .anchorMode(poptorch.AnchorMode.All)\n",
    "\n",
    "eval_dataloader = poptorch.DataLoader(\n",
    "    options=val_opts, dataset=eval_dataset, shuffle=True, batch_size=4,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6584e37",
   "metadata": {},
   "source": [
    "In this example, we have configured `deviceIteration` and `anchorMode`.\n",
    "\n",
    "**Device iteration** specifies the number of batches that are executed on an \n",
    "IPU before interacting with the host. The higher the number the less the IPU \n",
    "has to interact with the CPU, for example, to request and wait for data, so \n",
    "that the IPU can loop faster. However, the user will have to wait for the IPU \n",
    "to complete all the iterations before anything is returned to the host machine.\n",
    "\n",
    "**Anchor mode** specifies which data is returned from the IPU to the host \n",
    "machine. By default, PopTorch returns the last batch to the host machine after \n",
    "all iterations of the device, which is represented by `AnchorMode.Final`. \n",
    "We set this parameter to `AnchorMode.All` to obtain every output from the model \n",
    "during the validation stage. This has an adverse impact on throughput due to \n",
    "the overhead of having to transfer more data to the host machine after \n",
    "N device iterations.\n",
    "\n",
    "The full list of options is available in the [documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/overview.html#options)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212e2ca2",
   "metadata": {},
   "source": [
    "## Preparing the model\n",
    "\n",
    "Next, load the pretrained model and initialize the optimizer. Note that \n",
    "we use `poptorch.optim.AdamW`, which is optimised for distributed training.\n",
    "More optimizers can be found [here](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/reference.html#optimizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66026f7b",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from poptorch.optim import AdamW\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"google/electra-small-generator\",\n",
    "    num_labels=2,\n",
    "    return_dict=False,\n",
    "    torchscript=True\n",
    ")\n",
    "optimizer = AdamW(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73012c2f",
   "metadata": {},
   "source": [
    "For this example we will pipeline the model across 4 IPU devices. \n",
    "We start by placing the embedding layer on the first device `IPU:0`.\n",
    "The encoder in the ELECTRA model consists of 12 layers, which we distribute \n",
    "equally with 4 layers on each of the remaining 3 devices. Finally we place the\n",
    "the classifier layer on the last device as well (`IPU:3`).\n",
    "\n",
    "In order to placer a given layer on a particular IPU device, we wrap it using \n",
    "`poptorch.BeginBlock()`, which takes as arguments an instance of \n",
    "`torch.nn.Module`, the name of the layer (which is displayed in the [PopVision Graph Analyser](https://docs.graphcore.ai/projects/graphcore-popvision-user-guide/en/latest/)), \n",
    "and the device ID on which the layer should be placed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b797dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.electra.embeddings = poptorch.BeginBlock(\n",
    "    model.electra.embeddings, \"Embedding\", ipu_id=0\n",
    ")\n",
    "ipu_ids = [1] * 4 + [2] * 4 + [3] * 4\n",
    "for index, (layer, ipu_id) in enumerate(\n",
    "        zip(model.electra.encoder.layer, ipu_ids)):\n",
    "    model.electra.encoder.layer[index] = poptorch.BeginBlock(\n",
    "        layer, f\"Encoder{index}\", ipu_id=ipu_id\n",
    "    )\n",
    "\n",
    "model.classifier = poptorch.BeginBlock(\n",
    "    model.classifier, \"Classifier\", ipu_id=3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9117e7",
   "metadata": {},
   "source": [
    "We need to take one more step in order to adapt the model from the Hugging Face \n",
    "transformers library to run on an IPU. When a model uses multiple loss \n",
    "functions or uses a custom loss function, it has to be wrapped in \n",
    "`poptorch.identity_loss(loss)`.\n",
    "\n",
    "Due to the fact that we can not directly modify the model class, we create \n",
    "a class that takes our ELECTRA model as a parameter and overload the `forward` \n",
    "function, in which we call the `forward` function from ELECTRA and then wrap \n",
    "the returned loss in `identity_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91554792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class IPUModel(torch.nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids, attention_mask, labels):\n",
    "        loss, logits = self.model.forward(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=token_type_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "        )\n",
    "\n",
    "        if self.model.training:\n",
    "            loss = poptorch.identity_loss(loss, reduction=\"none\")\n",
    "\n",
    "        return loss, logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8749a1",
   "metadata": {},
   "source": [
    "Now, we create a model for training and another for inference, using \n",
    "`poptorch.trainingModel` and `poptorch.inferenceModel` respectively. They both \n",
    "take as arguments a `torch.nn.Module` instance and a `poptorch.Options` instance \n",
    "which we have previously instantiated. An optimiser is also required for \n",
    "`poptorch.trainingModel`. This wrapper uses TorchScript, and manages \n",
    "the translation of our model to a program that can be executed on an IPU. Then \n",
    "we will compile the models using one batch from our dataset.\n",
    "\n",
    "We only need to execute one model at a time (between training and inference), \n",
    "therefore we call `detachFromDevice` to explicitly detach each \n",
    "model after compilation. This ensures we aren't holding IPUs unnecessarily and \n",
    "reducing overall utilisation.\n",
    "\n",
    "Compilation may take a few minutes for this model. More information about \n",
    "the wrapping function can be found in the [documentation](https://docs.graphcore.ai/projects/poptorch-user-guide/en/latest/reference.html#model-wrapping-functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1737860f",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "trainingModel = poptorch.trainingModel(\n",
    "    IPUModel(model), options=opts, optimizer=optimizer\n",
    ")\n",
    "trainingModel.compile(**next(iter(train_dataloader)))\n",
    "trainingModel.detachFromDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe816ba5",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "inferenceModel = poptorch.inferenceModel(\n",
    "    IPUModel(model), options=val_opts\n",
    ")\n",
    "inferenceModel.compile(**next(iter(eval_dataloader)))\n",
    "inferenceModel.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f95b41b",
   "metadata": {},
   "source": [
    "## Training and validating the model\n",
    "\n",
    "Our model and data are ready to run on the IPUs. We proceed to implement the \n",
    "function responsible for training the model. Here we set the model into the \n",
    "training state, and add a progress bar. We do not need to include \n",
    "`loss.backward()` as `poptorch.trainingModel` does this itself. \n",
    "\n",
    "Explicit attaching the model to the device is not necessary, because calling \n",
    "the model will make it attached automatically. Note that at the end of the \n",
    "following function we explicitly detach the model from the IPUs it was using. \n",
    "This automatically synchronises the updated weights of the model and transfers \n",
    "them to the CPU, so when we attach the model for the inference to the IPU \n",
    "device, the model will have the current state and copying the weights \n",
    "explicitly between devices is not necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f4e6eb",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "epochs = 3\n",
    "number_of_iterations = epochs * (len(train_dataloader) + len(eval_dataloader))\n",
    "progress_bar = tqdm(range(number_of_iterations))\n",
    "\n",
    "\n",
    "def train_epoch():\n",
    "    trainingModel.train()\n",
    "\n",
    "    for batch in train_dataloader:\n",
    "        loss, logits = trainingModel(**batch)\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    trainingModel.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef373c1e",
   "metadata": {},
   "source": [
    "The function that performs validation is marked with the decorator \n",
    "`torch.no_grad()`, which ensures that gradient calculations are skipped since \n",
    "they are not necessary. Moreover, in addition to setting the model into the \n",
    "evaluation state, we add storing predictions to count the accuracy at the end \n",
    "of the epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca6895a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_epoch():\n",
    "    inferenceModel.eval()\n",
    "\n",
    "    y_pred, y_true = [], []\n",
    "    for batch in eval_dataloader:\n",
    "        loss, logits = inferenceModel(**batch)\n",
    "\n",
    "        y_pred += logits.argmax(dim=1).tolist()\n",
    "        y_true.extend(batch['labels'].tolist())\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(f'Accuracy: {acc:.3f}')\n",
    "\n",
    "    inferenceModel.detachFromDevice()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45a01c5",
   "metadata": {},
   "source": [
    "Finally, by bringing together everything we have written so far, we can start \n",
    "the training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe1f565",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_epoch()\n",
    "    val_epoch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501b8da",
   "metadata": {},
   "source": [
    "To sum up, in this tutorial, we have successfully fine-tuned a model from the \n",
    "Hugging Face transformers library for sentiment prediction using IPUs. If you are interested \n",
    "in other tutorials you are encouraged to check out [Graphcore Tutorials](https://github.com/graphcore/tutorials),\n",
    "and for a more advanced BERT implementation, see this Graphcore [BERT example](https://github.com/graphcore/examples/tree/master/applications/pytorch/bert)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
