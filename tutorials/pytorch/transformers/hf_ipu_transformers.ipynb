{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hugging Face Transformers executed on Graphcore's IPU devices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import poptorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "context_file=\"context.txt\"\n",
    "questions_file=\"questions_file.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(context_file, \"w+\") as f:\n",
    "    f.write(\"Scotland is a country that is part of the United Kingdom. Covering the northern third of the island of Great Britain, mainland Scotland has a 96 mile (154 km) border with England to the southeast and is otherwise surrounded by the Atlantic Ocean to the north and west, the North Sea to the northeast and the Irish Sea to the south. In addition, Scotland includes more than 790 islands; principally within the Northern Isles and the Hebrides archipelagos.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(questions_file, \"w+\") as f:\n",
    "    f.writelines(\n",
    "        [\n",
    "            \"How many islands are there in Scotland?\\n\",\n",
    "            \"What sea is to the south of Scotland?\\n\",\n",
    "            \"How long is Scotland's border in km?\\n\",\n",
    "            \"Where is England in relation to scotland?\\n\"\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inputs(context_file, questions_file, batch_size):\n",
    "    context = context_file.read()\n",
    "    questions = questions_file.readlines()\n",
    "    questions = [q.rstrip() for q in questions]\n",
    "\n",
    "    # Pad last batch with empty question if required\n",
    "    questions += [\"\"] * (len(questions) % batch_size)\n",
    "    return context, questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained model and tokenizer.\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "    'mrm8488/bert-medium-finetuned-squadv2', \n",
    "    return_token_type_ids=True,\n",
    "    return_dict=False\n",
    ")\n",
    "\n",
    "model = transformers.BertForQuestionAnswering.from_pretrained(\n",
    "    'mrm8488/bert-medium-finetuned-squadv2',\n",
    "    return_dict=False\n",
    ")\n",
    "\n",
    "# Parse command-line arguments.\n",
    "context, questions = read_inputs(open(context_file), open(questions_file), batch_size)\n",
    "\n",
    "num_questions = len(questions)\n",
    "num_batches = num_questions // batch_size\n",
    "\n",
    "# Pipeline the model over two IPUs. You must have at least as many batches (questions) as you have IPUs.\n",
    "model.bert.embeddings.position_embeddings = poptorch.BeginBlock(\n",
    "    layer_to_call=model.bert.embeddings.position_embeddings, \n",
    "    ipu_id=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap PyTorch model insde a PopTorch InferenceModel. This will make the model run on the IPU.\n",
    "opts = poptorch.Options().deviceIterations(batch_size)\n",
    "inference_model = poptorch.inferenceModel(model, options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/adamw/adam_env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XXXXXXXXXx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001B[A\n",
      "Graph compilation:   3%|▎         | 3/100 [00:01<01:00]\u001B[A\n",
      "Graph compilation:   4%|▍         | 4/100 [00:02<00:44]\u001B[A\n",
      "Graph compilation:   7%|▋         | 7/100 [00:03<00:41]\u001B[A\n",
      "Graph compilation:  16%|█▌        | 16/100 [00:03<00:11]\u001B[A\n",
      "Graph compilation:  20%|██        | 20/100 [00:04<00:14]\u001B[A\n",
      "Graph compilation:  23%|██▎       | 23/100 [00:05<00:14]\u001B[A\n",
      "Graph compilation:  26%|██▌       | 26/100 [00:05<00:11]\u001B[A\n",
      "Graph compilation:  28%|██▊       | 28/100 [00:06<00:15]\u001B[A\n",
      "Graph compilation:  30%|███       | 30/100 [00:06<00:13]\u001B[A\n",
      "Graph compilation:  32%|███▏      | 32/100 [00:06<00:12]\u001B[A\n",
      "Graph compilation:  34%|███▍      | 34/100 [00:07<00:13]\u001B[A\n",
      "Graph compilation:  42%|████▏     | 42/100 [00:08<00:08]\u001B[A\n",
      "Graph compilation:  45%|████▌     | 45/100 [00:08<00:06]\u001B[A\n",
      "Graph compilation:  49%|████▉     | 49/100 [00:08<00:05]\u001B[A\n",
      "Graph compilation:  51%|█████     | 51/100 [00:10<00:12]\u001B[A\n",
      "Graph compilation:  53%|█████▎    | 53/100 [00:11<00:16]\u001B[A\n",
      "Graph compilation:  56%|█████▌    | 56/100 [00:13<00:19]\u001B[A\n",
      "Graph compilation:  60%|██████    | 60/100 [00:13<00:11]\u001B[A\n",
      "Graph compilation:  62%|██████▏   | 62/100 [00:14<00:09]\u001B[A\n",
      "Graph compilation:  64%|██████▍   | 64/100 [00:15<00:11]\u001B[A\n",
      "Graph compilation:  66%|██████▌   | 66/100 [00:15<00:08]\u001B[A\n",
      "Graph compilation:  68%|██████▊   | 68/100 [00:15<00:06]\u001B[A\n",
      "Graph compilation:  73%|███████▎  | 73/100 [00:16<00:05]\u001B[A\n",
      "Graph compilation:  76%|███████▌  | 76/100 [00:16<00:04]\u001B[A\n",
      "Graph compilation:  82%|████████▏ | 82/100 [00:19<00:04]\u001B[A\n",
      "Graph compilation:  83%|████████▎ | 83/100 [00:19<00:05]\u001B[A\n",
      "Graph compilation:  84%|████████▍ | 84/100 [00:20<00:05]\u001B[A\n",
      "Graph compilation:  87%|████████▋ | 87/100 [00:21<00:03]\u001B[A\n",
      "Graph compilation:  88%|████████▊ | 88/100 [00:24<00:08]\u001B[A\n",
      "Graph compilation:  89%|████████▉ | 89/100 [00:26<00:09]\u001B[A\n",
      "Graph compilation:  90%|█████████ | 90/100 [00:26<00:08]\u001B[A\n",
      "Graph compilation:  91%|█████████ | 91/100 [00:27<00:08]\u001B[A\n",
      "Graph compilation:  92%|█████████▏| 92/100 [00:28<00:06]\u001B[A\n",
      "Graph compilation:  93%|█████████▎| 93/100 [00:29<00:05]\u001B[A\n",
      "Graph compilation: 100%|██████████| 100/100 [00:35<00:00][A\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae66870540a8429aa669340202a4ad7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████     | 1/2 [01:00<01:00, 60.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How many islands are there in Scotland?\n",
      "Answer: more than 790\n",
      "Question: What sea is to the south of Scotland?\n",
      "Answer: irish sea\n",
      "XXXXXXXXXx\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adamw/adam_env/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3e6aa623ed54ca29117d871369cf67b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 2/2 [01:00<00:00, 30.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How long is Scotland's border in km?\n",
      "Answer: 154\n",
      "Question: Where is England in relation to scotland?\n",
      "Answer: southeast\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import trange\n",
    "from tqdm.contrib import tenumerate\n",
    "\n",
    "# Process inputs in batches.\n",
    "for batch_idx in trange(num_batches):\n",
    "    print(\"XXXXXXXXXx\")\n",
    "    input_pairs = [\n",
    "        (questions[batch_size*batch_idx + i], context)\n",
    "        for i in range(batch_size)]\n",
    "\n",
    "    batched_encoding = tokenizer.batch_encode_plus(\n",
    "        input_pairs,\n",
    "        max_length=110,\n",
    "        pad_to_max_length='right'\n",
    "    )\n",
    "\n",
    "    # Convert to PyTorch tensors.\n",
    "    input_batch = torch.tensor(batched_encoding[\"input_ids\"])\n",
    "    attention_batch = torch.tensor(batched_encoding[\"attention_mask\"])\n",
    "\n",
    "    # Execute on IPU.\n",
    "    start_score_pop, end_scores_pop = inference_model(input_batch, attention_batch)\n",
    "\n",
    "    # Process outputs.\n",
    "    for i, (start_score, end_score) in tenumerate(zip(start_score_pop, end_scores_pop)):\n",
    "        answer_start, answer_stop = start_score.argmax(), end_score.argmax()\n",
    "        answer_ids = input_batch[i][answer_start:answer_stop + 1]\n",
    "        answer_tokens = tokenizer.convert_ids_to_tokens(answer_ids,\n",
    "                                                        skip_special_tokens=True)\n",
    "        answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
    "\n",
    "        print(f\"Question: {questions[batch_size*batch_idx + i]}\")\n",
    "        print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}