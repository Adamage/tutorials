{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b25a9a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2021 Graphcore Ltd. All rights reserved.\n",
    "# Copyright 2020 The HuggingFace Team All rights reserved.\n",
    "import collections\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizerFast, default_data_collator\n",
    "import torch\n",
    "import poptorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "706ab395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PadCollate:\n",
    "    \"\"\"\n",
    "    Collate into a batch and pad the batch up to a fixed size.\n",
    "    \"\"\"\n",
    "    def __init__(self, batch_size, padding_val_dict=None):\n",
    "        self.batch_size = batch_size\n",
    "        self.padding_val_dict = padding_val_dict\n",
    "\n",
    "    def pad_tensor(self, x, val):\n",
    "        pad_size = list(x.shape)\n",
    "        pad_size[0] = self.batch_size - x.size(0)\n",
    "        return torch.cat([x, val*torch.ones(*pad_size, dtype=x.dtype)], dim=0)\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        size = len(batch)\n",
    "        batch = default_data_collator(batch)\n",
    "        if size < self.batch_size:\n",
    "            for k in batch.keys():\n",
    "                batch[k] = self.pad_tensor(batch[k], self.padding_val_dict[k])\n",
    "        return batch\n",
    "\n",
    "max_seq_length = 384\n",
    "doc_stride = 128\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "\n",
    "# `prepare_train_features` comes unmodified from\n",
    "# https://github.com/huggingface/transformers/blob/master/examples/pytorch/question-answering/run_qa.py\n",
    "def prepare_train_features(examples):\n",
    "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
    "    # in one example possible giving several features when a context is long, each of those features having a\n",
    "    # context that overlaps a bit the context of the previous feature.\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_seq_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
    "    # its corresponding example. This key gives us just that.\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
    "    # help us compute the start_positions and end_positions.\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    # Let's label those examples!\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        # We will label impossible answers with the index of the CLS token.\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "\n",
    "        # One example can give several spans, this is the index of the example containing this span of text.\n",
    "        sample_index = sample_mapping[i]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        # If no answers are given, set the cls_index as answer.\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "        else:\n",
    "            # Start/end character index of the answer in the text.\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            # Start token index of the current span in the text.\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n",
    "                token_start_index += 1\n",
    "\n",
    "            # End token index of the current span in the text.\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n",
    "                token_end_index -= 1\n",
    "\n",
    "            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
    "            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "            else:\n",
    "                # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n",
    "                # Note: we could go after the last offset if the answer is the last word (edge case).\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8e94f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/adamw/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2650bc61cc664272890efc0720517881",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/adamw/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453/cache-db2e65aee1bcfc2e.arrow\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "\n",
    "datasets = load_dataset(\"squad\")\n",
    "train_dataset = datasets[\"train\"]\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    prepare_train_features,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    load_from_cache_file=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c85bfb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import poptorch\n",
    "import transformers\n",
    "\n",
    "class Wrapped(transformers.BertForQuestionAnswering):\n",
    "    def __init__(self):\n",
    "        super().__init__(transformers.BertConfig())\n",
    "        self.bert.embeddings = poptorch.BeginBlock(self.bert.embeddings, \"Embedding\", ipu_id=0)\n",
    "\n",
    "        for index, layer in enumerate(self.bert.encoder.layer):\n",
    "            self.bert.encoder.layer[index] = poptorch.BeginBlock(layer, f\"Encoder{index}\", ipu_id=1)\n",
    "       \n",
    "        self.qa_outputs = poptorch.BeginBlock(self.qa_outputs, \"QA Outputs\", ipu_id=2)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, start_positions=None, end_positions=None):\n",
    "        inputs = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"token_type_ids\": token_type_ids,\n",
    "            \"start_positions\": start_positions,\n",
    "            \"end_positions\": end_positions\n",
    "        }\n",
    "        output = super().forward(**inputs)\n",
    "        \n",
    "        if self.training:\n",
    "            final_loss = poptorch.identity_loss(output.loss, reduction=\"none\")\n",
    "            return final_loss, output.start_logits, output.end_logits\n",
    "        else:\n",
    "            return output.start_logits, output.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0432aaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<poptorch.options.Options at 0x7f7453071f98>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import popart \n",
    "\n",
    "opts = poptorch.Options()\n",
    "opts.deviceIterations(8)\n",
    "opts.autoRoundNumIPUs(True)\n",
    "#opts.Training.setAutomaticLossScaling(True)\n",
    "opts.anchorMode(poptorch.AnchorMode.Sum)\n",
    "opts.setExecutionStrategy(\n",
    "    poptorch.PipelinedExecution(\n",
    "        poptorch.AutoStage.AutoIncrement\n",
    "    )\n",
    ")\n",
    "opts.Precision.enableStochasticRounding(True)\n",
    "opts.Precision.setPartialsType(torch.float16)\n",
    "opts._Popart.set(\"disableGradAccumulationTensorStreams\", True)\n",
    "opts._Popart.set(\"subgraphCopyingStrategy\", int(popart.SubgraphCopyingStrategy.JustInTime))\n",
    "opts._Popart.set(\"outlineThreshold\", 10.0)\n",
    "opts._Popart.set(\"accumulateOuterFragmentSettings.schedule\",\n",
    "                 int(popart.AccumulateOuterFragmentSchedule.OverlapMemoryOptimized))\n",
    "opts._Popart.set(\"accumulateOuterFragmentSettings.excludedVirtualGraphs\", [\"0\"])\n",
    "\n",
    "\n",
    "mem_prop = {\n",
    "    f'IPU{i}': 0.2\n",
    "    for i in range(5)\n",
    "}\n",
    "opts.setAvailableMemoryProportion(mem_prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f333bf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 384\n",
    "samples_per_step = 2\n",
    "num_epochs = 3\n",
    "\n",
    "train_dataloader = poptorch.DataLoader(\n",
    "    options=opts, \n",
    "    dataset=train_dataset, \n",
    "    shuffle=True, \n",
    "    batch_size=8,\n",
    "    drop_last=True,\n",
    "    collate_fn=PadCollate(\n",
    "        samples_per_step,\n",
    "        {\"input_ids\": 0,\n",
    "         \"attention_mask\": 0,\n",
    "         \"token_type_ids\": 0,\n",
    "         \"start_positions\": sequence_length,\n",
    "         \"end_positions\": sequence_length})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8da78d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import float16, float32\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from poptorch import DataLoader\n",
    "\n",
    "\n",
    "model_ipu = Wrapped().half()\n",
    "\n",
    "regularized_params = []\n",
    "non_regularized_params = []\n",
    "for param in model_ipu.parameters():\n",
    "    if param.requires_grad:\n",
    "        if len(param.shape) == 1:\n",
    "            non_regularized_params.append(param)\n",
    "        else:\n",
    "            regularized_params.append(param)\n",
    "params = [\n",
    "    {\"params\": regularized_params, \"weight_decay\": 0},\n",
    "    {\"params\": non_regularized_params, \"weight_decay\": 0}\n",
    "]\n",
    "optimizer = poptorch.optim.AdamW(\n",
    "    params,\n",
    "    lr=5e-5,\n",
    "    weight_decay=0,\n",
    "    eps=1e-6,\n",
    "    bias_correction=False,\n",
    "    loss_scaling=1.0,\n",
    "    accum_type=float16,\n",
    "    first_order_momentum_accum_type=float16,\n",
    "    second_order_momentum_accum_type=float32\n",
    ")\n",
    "\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer,\n",
    "    0,\n",
    "    num_epochs * len(train_dataloader)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8670fab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ipu.train()\n",
    "training_model = poptorch.trainingModel(model_ipu, opts, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c0b29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Graph compilation:   0%|          | 0/100 [43:12<?]\n",
      "Graph compilation:  34%|███▍      | 34/100 [03:13<05:17]"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "start_compile = time.perf_counter()\n",
    "\n",
    "training_model.compile(sample_batch[\"input_ids\"],\n",
    "                               sample_batch[\"attention_mask\"],\n",
    "                               sample_batch[\"token_type_ids\"],\n",
    "                               sample_batch[\"start_positions\"],\n",
    "                               sample_batch[\"end_positions\"])\n",
    "\n",
    "duration_compilation = time.perf_counter() - start_compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "96d15429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18deeac4fe4452682e7863d9f310878",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4149 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Graph compilation:   0%|          | 0/100 [00:00<?]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:   3%|▎         | 3/100 [00:55<29:52]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:   4%|▍         | 4/100 [00:57<21:08]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:   7%|▋         | 7/100 [01:31<18:53]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  13%|█▎        | 13/100 [01:32<06:41]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  15%|█▌        | 15/100 [01:35<05:35]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  16%|█▌        | 16/100 [01:36<05:00]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  17%|█▋        | 17/100 [01:38<04:23]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  20%|██        | 20/100 [01:38<02:29]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  21%|██        | 21/100 [01:55<05:47]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  21%|██        | 21/100 [01:55<05:47]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  22%|██▏       | 22/100 [01:55<04:39]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  22%|██▏       | 22/100 [02:17<04:39]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  23%|██▎       | 23/100 [02:40<15:57]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  26%|██▌       | 26/100 [02:40<08:02]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  26%|██▌       | 26/100 [03:01<08:02]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  27%|██▋       | 27/100 [03:03<11:36]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  28%|██▊       | 28/100 [03:09<10:35]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  29%|██▉       | 29/100 [03:19<10:41]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  31%|███       | 31/100 [03:19<06:21]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  32%|███▏      | 32/100 [03:19<04:54]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  33%|███▎      | 33/100 [03:20<03:58]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  34%|███▍      | 34/100 [03:28<04:59]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  35%|███▌      | 35/100 [03:40<07:05]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  36%|███▌      | 36/100 [04:06<12:54]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  38%|███▊      | 38/100 [04:18<09:37]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  39%|███▉      | 39/100 [07:17<50:52]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  42%|████▏     | 42/100 [07:54<29:50]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  45%|████▌     | 45/100 [07:59<17:07]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  47%|████▋     | 47/100 [08:00<11:48]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  49%|████▉     | 49/100 [08:07<08:55]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  50%|█████     | 50/100 [08:12<08:00]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  51%|█████     | 51/100 [08:35<09:58]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  52%|█████▏    | 52/100 [08:36<07:55]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  53%|█████▎    | 53/100 [08:56<09:39]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  56%|█████▌    | 56/100 [09:03<05:14]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  60%|██████    | 60/100 [09:03<02:26]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  61%|██████    | 61/100 [09:13<03:00]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  62%|██████▏   | 62/100 [09:13<02:25]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  63%|██████▎   | 63/100 [09:28<03:43]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  64%|██████▍   | 64/100 [12:19<25:42]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  66%|██████▌   | 66/100 [12:31<16:09]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  67%|██████▋   | 67/100 [12:32<12:25]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  68%|██████▊   | 68/100 [12:34<09:22]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  72%|███████▏  | 72/100 [12:34<03:29]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  73%|███████▎  | 73/100 [12:58<04:37]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  76%|███████▌  | 76/100 [13:00<02:30]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  77%|███████▋  | 77/100 [13:02<02:08]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  80%|████████  | 80/100 [13:03<01:05]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  82%|████████▏ | 82/100 [13:08<00:56]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  83%|████████▎ | 83/100 [13:10<00:49]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  84%|████████▍ | 84/100 [13:22<01:14]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  87%|████████▋ | 87/100 [13:24<00:36]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  88%|████████▊ | 88/100 [13:36<00:53]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  89%|████████▉ | 89/100 [14:19<02:16]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  90%|█████████ | 90/100 [14:22<01:42]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  91%|█████████ | 91/100 [14:26<01:17]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  92%|█████████▏| 92/100 [14:28<00:55]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  93%|█████████▎| 93/100 [14:39<00:57]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation:  94%|█████████▍| 94/100 [14:41<00:38]\u001b[A\u001b[A\n",
      "\n",
      "Graph compilation: 100%|██████████| 100/100 [14:51<00:00]\u001b[A\u001b[A\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "In poptorch/python/poptorch.cpp:1220: 'popart_exception': Out of memory on tile 1472: 3631524 bytes used but tiles only have 638976 bytes of memory\nError raised in:\n  [0] popart::popx::IrLowering::getExecutable()\n  [1] popart::popx::Executablex::getPoplarExecutable()\n  [2] popart::popx::Devicex::prepare()\n  [3] popart::Session::prepareDevice(bool)\n  [4] poptorch::Compiler::compileAndPrepareDevice()\n  [5] popart::Session::prepareDevice: Poplar compilation\n  [6] Compiler::compileAndPrepareDevice\n  [7] LowerToPopart::compile\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-99c11ff92980>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_type_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_positions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_positions\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         )\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/adam_env/lib/python3.6/site-packages/poptorch/_poplar_executor.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misCompiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_attached\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/adam_env/lib/python3.6/site-packages/poptorch/_poplar_executor.py\u001b[0m in \u001b[0;36m_compile\u001b[0;34m(self, in_tensors)\u001b[0m\n\u001b[1;32m    368\u001b[0m                     narrow_tensor_fn)\n\u001b[1;32m    369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compileWithTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Compiling the model using scripting'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/adam_env/lib/python3.6/site-packages/poptorch/_poplar_executor.py\u001b[0m in \u001b[0;36m_compileWithTrace\u001b[0;34m(self, trace_args)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0;31m# trigger the compilation process in all the other processes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misCompiled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_executable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpoptorch_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompileWithTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtrace_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;31m# Load the engine and connect the streams in all the processes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mError\u001b[0m: In poptorch/python/poptorch.cpp:1220: 'popart_exception': Out of memory on tile 1472: 3631524 bytes used but tiles only have 638976 bytes of memory\nError raised in:\n  [0] popart::popx::IrLowering::getExecutable()\n  [1] popart::popx::Executablex::getPoplarExecutable()\n  [2] popart::popx::Devicex::prepare()\n  [3] popart::Session::prepareDevice(bool)\n  [4] poptorch::Compiler::compileAndPrepareDevice()\n  [5] popart::Session::prepareDevice: Poplar compilation\n  [6] Compiler::compileAndPrepareDevice\n  [7] LowerToPopart::compile\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        outputs = training_model(\n",
    "            batch[\"input_ids\"],\n",
    "            batch[\"attention_mask\"],\n",
    "            batch[\"token_type_ids\"],\n",
    "            batch[\"start_positions\"],\n",
    "            batch[\"end_positions\"]\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        \n",
    "        lr_scheduler.step()\n",
    "        training_model.setOptimizer(optimizer)\n",
    "\n",
    "        progress_bar.set_description(\n",
    "            f\"Epoch: {epoch}, LR={lr_scheduler.get_last_lr()[0]:.2e}, loss={loss:3.3f}\"\n",
    "        )\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f882e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_model.detachFromDevice()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e225637f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4fc28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
