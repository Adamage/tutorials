{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b92a06c",
   "metadata": {},
   "source": [
    "# Keras tutorial: How to run on IPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b55c62",
   "metadata": {},
   "source": [
    "This tutorial provides an introduction on how to run Keras models on IPUs, and features that allow you to fully utilise \n",
    "the capability of the IPU. Please refer to the [TensorFlow 2 Keras API reference](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#module-tensorflow.python.ipu.keras) \n",
    "for full details of all available features.\n",
    "\n",
    "Requirements:\n",
    "* Installed and enabled Poplar\n",
    "* Installed the Graphcore port of TensorFlow 2\n",
    "\n",
    "Refer to the Getting Started guide for your IPU System for instructions.\n",
    "\n",
    "#### Directory Structure\n",
    "\n",
    "* `completed_demos`: Completed versions of the scripts described in this tutorial\n",
    "* `completed_example`: A completed example of running Keras models on the IPU\n",
    "* `test`: A directory that contains test scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e8cc41",
   "metadata": {},
   "source": [
    "#### Keras MNIST example\n",
    "\n",
    "The script below (which also can be found in `demo.py`) illustrates a simple example using the MNIST numeral dataset, \n",
    "which consists of 60,000 images for training and 10,000 images for testing. The images are of handwritten digits 0-9, \n",
    "and they must be classified according to which digit they represent. MNIST classification is a toy example problem, \n",
    "but is sufficient to outline the concepts introduced in this tutorial.\n",
    "\n",
    "Without changes, the script will run the Keras model on the CPU. It is based on the original Keras tutorial and as such \n",
    "is vanilla Keras code. You can run this now to see its output. In the following sections, we will go through the changes \n",
    "needed to make this run on the IPU.\n",
    "\n",
    "Running cell below or `python3 demo.py` gives the following throughput values for training:\n",
    "\n",
    "```\n",
    "Epoch 1/3\n",
    "938/938 [==============================] - 10s 10ms/step - loss: 1.6732 - accuracy: 0.4536\n",
    "Epoch 2/3\n",
    "938/938 [==============================] - 9s 10ms/step - loss: 0.3618 - accuracy: 0.8890\n",
    "Epoch 3/3\n",
    "938/938 [==============================] - 9s 10ms/step - loss: 0.2376 - accuracy: 0.9289\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e56c2e",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "\n",
    "# Store class and shape information.\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def prepare_data():\n",
    "    # Load the MNIST dataset from keras.datasets\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    # Normalize the images.\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "    # When dealing with images, we usually want an explicit channel dimension, even when it is 1.\n",
    "    # Each sample thus has a shape of (28, 28, 1).\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    # Finally, convert class assignments to a binary class matrix.\n",
    "    # Each row can be seen as a rank-1 \"one-hot\" tensor.\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "\n",
    "def model_fn():\n",
    "    # Input layer - \"entry point\" / \"source vertex\".\n",
    "    input_layer = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Add layers to the graph.\n",
    "    x = keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(input_layer)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "    x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = keras.layers.Flatten()(x)\n",
    "    x = keras.layers.Dropout(0.5)(x)\n",
    "    x = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return input_layer, x\n",
    "\n",
    "\n",
    "print('Keras MNIST example, running on CPU')\n",
    "(x_train, y_train), (x_test, y_test) = prepare_data()\n",
    "\n",
    "# Model.__init__ takes two required arguments, inputs and outputs.\n",
    "model = keras.Model(*model_fn())\n",
    "\n",
    "# Compile our model with Stochastic Gradient Descent as an optimizer\n",
    "# and Categorical Cross Entropy as a loss.\n",
    "model.compile('sgd', 'categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "print('\\nTraining')\n",
    "model.fit(x_train, y_train, epochs=3, batch_size=batch_size)\n",
    "\n",
    "print('\\nEvaluation')\n",
    "model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe15a0c",
   "metadata": {},
   "source": [
    "\n",
    "#### Running the example on the IPU\n",
    "\n",
    "In this section, we will make a series of edits to `demo.py` in order to train the model using the IPU. Make a copy of `demo.py` to follow along.\n",
    "\n",
    "##### 1. Import the TensorFlow IPU module\n",
    "\n",
    "First, we import the TensorFlow IPU module.\n",
    "\n",
    "Add the following import statement to the beginning of your script:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8e1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python import ipu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10be39c6",
   "metadata": {},
   "source": [
    "\n",
    "For the `ipu` module to function properly, we must import it directly rather than accessing it through the top-level TensorFlow module.\n",
    "\n",
    "##### 2. Preparing the dataset\n",
    "\n",
    "Some extra care must be taken when preparing a dataset for training a Keras model on the IPU. The Poplar software stack \n",
    "does not support using tensors with shapes which are not known when the model is compiled, so we must make sure the sizes \n",
    "of our datasets are divisible by the batch size. We introduce a utility function, `make_divisible`, which computes the \n",
    "largest number, no larger than a given number, which is divisible by a given divisor. This will be of further use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464a40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_divisible(number):\n",
    "    return number - number % batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606abbf8",
   "metadata": {},
   "source": [
    "Adjust dataset lengths to be divisible by the batch size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83837694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_trim_to_size():\n",
    "    (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "    train_data_len = x_train.shape[0]\n",
    "    train_data_len = make_divisible(train_data_len)\n",
    "    x_train, y_train = x_train[:train_data_len], y_train[:train_data_len]\n",
    "\n",
    "    test_data_len = x_test.shape[0]\n",
    "    test_data_len = make_divisible(test_data_len)\n",
    "    x_test, y_test = x_test[:test_data_len], y_test[:test_data_len]\n",
    "\n",
    "    x_train = x_train.astype(\"float32\") / 255\n",
    "    x_test = x_test.astype(\"float32\") / 255\n",
    "\n",
    "    x_train = np.expand_dims(x_train, -1)\n",
    "    x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b260ea",
   "metadata": {},
   "source": [
    "\n",
    "With a batch size of 64, we lose 32 training examples and 48 evaluation examples, which is less than 0.2% of each dataset.\n",
    "\n",
    "There are other ways to prepare a dataset for training on the IPU. You can create a `tf.data.Dataset` object using your \n",
    "data, then use its `.repeat()` method to create a looped version of the dataset. If you do not want to lose any data, \n",
    "you can pad the datasets with tensors of zeros, then set `sample_weight` to be a vector of 1’s and 0’s according to \n",
    "which values are real so the extra values don’t affect the training process (though this may be slower than using \n",
    "the other methods)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d7d4e8",
   "metadata": {},
   "source": [
    "##### 3. Add IPU configuration\n",
    "\n",
    "To use the IPU, you must create an IPU session configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a49398",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config = ipu.config.IPUConfig()\n",
    "ipu_config.auto_select_ipus = 1\n",
    "ipu_config.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337d96e5",
   "metadata": {},
   "source": [
    "This is all we need to get a small model up and running, though a full list of configuration options is available in the \n",
    "[API documentation](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#tensorflow.python.ipu.config.IPUConfig)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765ca558",
   "metadata": {},
   "source": [
    "##### 4. Specify IPU strategy\n",
    "\n",
    "Next, add the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d8e085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an execution strategy.\n",
    "strategy = ipu.ipu_strategy.IPUStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf40b8",
   "metadata": {},
   "source": [
    "The `tf.distribute.Strategy` is an API to distribute training across multiple devices. `IPUStrategy` is a subclass which \n",
    "targets a system with one or more IPUs attached. Another subclass, [IPUMultiWorkerStrategy](https://docs.graphcore.ai/projects/tensorflow-user-guide/en/latest/api.html#tensorflow.python.ipu.ipu_multi_worker_strategy.IPUMultiWorkerStrategy), \n",
    "targets a multi-system configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873def49",
   "metadata": {},
   "source": [
    "##### 5. Wrap the model within the IPU strategy scope\n",
    "\n",
    "Creating variables and Keras models within the scope of the `IPUStrategy` object will ensure that they are placed on the \n",
    "IPU. To do this, we create a `strategy.scope()` context manager and move all the model code inside it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d07068",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "print('Keras MNIST example, running on IPU')\n",
    "(x_train, y_train), (x_test, y_test) = prepare_data_trim_to_size()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = keras.Model(*model_fn())\n",
    "\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "    model.summary()\n",
    "    print('\\nTraining')\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "    print('\\nEvaluation')\n",
    "    model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d83329",
   "metadata": {},
   "source": [
    "Note that the function `model_fn()` can be readily reused, and all we really need to do is move the code inside the \n",
    "context of `strategy.scope()`. Prior to the release of version 2.2.0 of the Poplar SDK, it would have been necessary to \n",
    "make the model an instance of the `ipu.keras.Model` class, which has been removed as of version 2.2.0.\n",
    "\n",
    "While all computation will now be performed on the IPU, the initialisation of variables will still be performed on the host.\n",
    "\n",
    "\n",
    "##### 6. Results\n",
    "\n",
    "Running the code gives the following throughput values for training:\n",
    "\n",
    "```\n",
    "937/937 [==============================] - 45s 3ms/step - loss: 1.5260 - accuracy: 0.4949\n",
    "Epoch 2/3\n",
    "937/937 [==============================] - 2s 3ms/step - loss: 0.3412 - accuracy: 0.8968\n",
    "Epoch 3/3\n",
    "937/937 [==============================] - 3s 3ms/step - loss: 0.2358 - accuracy: 0.9294\n",
    "```\n",
    "\n",
    "The training time has been significantly reduced by use of the IPU. We ignore the reported total for the first \n",
    "epoch because this time includes the model's compilation time.\n",
    "\n",
    "The file `completed_demos/completed_demo_ipu.py` shows what the code looks like after the above changes are made. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3037e5c3",
   "metadata": {},
   "source": [
    "#### Going faster by setting `steps_per_execution`\n",
    "\n",
    "The IPU implementation above is fast, but not as fast as it could be. This is because, unless we specify otherwise, \n",
    "the program that runs on the IPU will only process a single batch, so we cannot get a speedup from loading the data \n",
    "asynchronously and using a looped version of this program.\n",
    "\n",
    "To change this, we must set the `steps_per_execution` argument in `model.compile()`. This sets the number of batches \n",
    "processed in each execution of the underlying IPU program.\n",
    "\n",
    "Now not only the number of data must divide equally into all batches, but also the number of batches must divide into \n",
    "the number of steps, for this purpose we will overload the make_divisible function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcc0b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_divisible(number):\n",
    "    steps_per_execution = number // batch_size\n",
    "    return number - number % (batch_size * steps_per_execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e5d929",
   "metadata": {},
   "source": [
    "The number of examples in the dataset must be divisible by the number of examples processed per execution \n",
    "(that is, `steps_per_execution * batch_size`). Here, we set `steps_per_execution` to be `(length of dataset) // batch_size` \n",
    "for maximum throughput and so that we do not lose any more data than we have to, though this code should work just \n",
    "as well with a different, smaller value.\n",
    "\n",
    "Now we update the code from `with strategy.scope():` onwards by passing `steps_per_execution` as an argument to \n",
    "`model.compile()`, and providing our `batch_size` value to `model.fit()` and `model.evaluate()`. \n",
    "We can re-compile the model with a different value of `steps_per_execution` between running `model.fit()` and \n",
    "`model.evaluate()`, so we do so here, although it isn't compulsory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f1cb86",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "print('Keras MNIST example, running on IPU with steps_per_execution')\n",
    "(x_train, y_train), (x_test, y_test) = prepare_data_trim_to_size()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = keras.Model(*model_fn())\n",
    "\n",
    "    # Compile our model with Stochastic Gradient Descent as an optimizer\n",
    "    # and Categorical Cross Entropy as a loss.\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics=[\"accuracy\"],\n",
    "                  steps_per_execution=len(x_train) // batch_size)\n",
    "\n",
    "    model.summary()\n",
    "    print('\\nTraining')\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "    print('\\nEvaluation')\n",
    "    model.evaluate(x_test, y_test)\n",
    "\n",
    "    model.summary()\n",
    "    print('\\nTraining')\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=3, batch_size=batch_size)\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics=[\"accuracy\"],\n",
    "                  steps_per_execution=len(x_test) // batch_size)\n",
    "\n",
    "    print('\\nEvaluation')\n",
    "    model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243d7efb",
   "metadata": {},
   "source": [
    "\n",
    "Running this code, the model trains much faster:\n",
    "\n",
    "```\n",
    "937/937 [==============================] - 43s 46ms/step - loss: 1.0042 - accuracy: 0.6783\n",
    "Epoch 2/3\n",
    "937/937 [==============================] - 0s 224us/step - loss: 0.3021 - accuracy: 0.9079\n",
    "Epoch 3/3\n",
    "937/937 [==============================] - 0s 222us/step - loss: 0.2240 - accuracy: 0.9326\n",
    "```\n",
    "\n",
    "The file `completed_demos/completed_demo_faster.py` shows what the code looks like after the above changes are made."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b88e54",
   "metadata": {},
   "source": [
    "\n",
    "#### Replication\n",
    "\n",
    "Another way to speed up the training of a model is to make a copy of the model on each of multiple IPUs, updating \n",
    "the parameters of the model on all IPUs after each forward and backward pass. This is called _replication_, and can be \n",
    "done in Keras with very few code changes. \n",
    "\n",
    "First, we'll add variables for the number of IPUs and the number of replicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e1fbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ipus = num_replicas = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7975a4",
   "metadata": {},
   "source": [
    "Because our model is written for one IPU, the number of replicas will be equal to the number of IPUs.\n",
    "\n",
    "We will need to adjust for the fact that with replication, a batch is processed on each replica for each step, \n",
    "so `steps_per_execution` needs to be divisible by the number of replicas. Also, the maximum value of \n",
    "`steps_per_execution` is now `train_data_len // (batch_size * num_replicas)`, since the number of examples processed \n",
    "in each step is now `(batch_size * num_replicas)`. We therefore add two lines to the dataset-adjustment code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b6db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_divisible(number):\n",
    "    return number - number % (batch_size * num_replicas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce60879e",
   "metadata": {},
   "source": [
    "\n",
    "We'll need to acquire multiple IPUs, so we update the configuration step:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a667c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ipu_config = ipu.config.IPUConfig()\n",
    "ipu_config.auto_select_ipus = num_ipus\n",
    "ipu_config.configure_ipu_system()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1de8e",
   "metadata": {},
   "source": [
    "These are all the changes we need to make to replicate the model and train on multiple IPUs. \n",
    "There is no need to explicitly copy the model or organise the exchange of weight updates between the IPUs because \n",
    "all of these details are handled automatically, as long as we select multiple IPUs and create and use our model within \n",
    "the scope of an `IPUStrategy` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b180b72b",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "print('Keras MNIST example, running on IPU with replication')\n",
    "(x_train, y_train), (x_test, y_test) = prepare_data_trim_to_size()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = keras.Model(*model_fn())\n",
    "\n",
    "    # Compile our model with Stochastic Gradient Descent as an optimizer\n",
    "    # and Categorical Cross Entropy as a loss.\n",
    "    train_steps = make_divisible(len(x_train))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics=[\"accuracy\"],\n",
    "                  steps_per_execution=train_steps // (batch_size * num_replicas))\n",
    "\n",
    "    model.summary()\n",
    "    print('\\nTraining')\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=3, batch_size=64)\n",
    "\n",
    "    print('\\nEvaluation')\n",
    "    model.evaluate(x_test, y_test)\n",
    "\n",
    "    model.summary()\n",
    "    print('\\nTraining')\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=3, batch_size=batch_size)\n",
    "\n",
    "    test_steps = make_divisible(len(x_test))\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics=[\"accuracy\"],\n",
    "                  steps_per_execution=test_steps // (batch_size * num_replicas))\n",
    "\n",
    "    print('\\nEvaluation')\n",
    "    model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d903cc2c",
   "metadata": {},
   "source": [
    "With replication, the model trains even faster:\n",
    "\n",
    "```\n",
    "936/936 [==============================] - 44s 47ms/step - loss: 1.1886 - accuracy: 0.6213\n",
    "Epoch 2/3\n",
    "936/936 [==============================] - 0s 135us/step - loss: 0.3155 - accuracy: 0.9054\n",
    "Epoch 3/3\n",
    "936/936 [==============================] - 0s 134us/step - loss: 0.2277 - accuracy: 0.9304\n",
    "```\n",
    "However, we do not get a perfect 2x speedup because the gradients must be exchanged between the IPUs before each weight update.\n",
    "\n",
    "The file `completed_demos/completed_demo_replicated.py` shows what the code looks like after the above changes are made. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380a783f",
   "metadata": {},
   "source": [
    "#### Pipelining\n",
    "\n",
    "Pipelining can also be enabled to split a Keras model across multiple IPUs. A pipelined model will execute multiple \n",
    "sections (called _stages_) of a model on individual IPUs concurrently by pipelining mini-batches of data through the stages.\n",
    "\n",
    "One of the key features of pipelining on the IPU is _gradient accumulation_. Forward and backward passes will be \n",
    "performed on several batches without performing a weight update. Instead, a cumulative sum of the gradients is updated \n",
    "after each forward and backward pass, and the weight update is applied only after a certain number of batches have been \n",
    "processed. This helps ensure consistency between the weights used in the forward and backward passes, and can be used \n",
    "to train with a batch size that wouldn't fit otherwise. To learn more about the specifics of pipelining, \n",
    "you can read [the relevant section of the Technical Note on Model Parallelism in TensorFlow](https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html).\n",
    "\n",
    "In this final part of the tutorial, we will pipeline our model over two stages. We will need to change the value of \n",
    "`num_replicas`, and create a variable for the number of gradient accumulation steps per replica:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc9add4",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ipus = 2\n",
    "num_replicas = num_ipus // 2\n",
    "gradient_accumulation_steps_per_replica = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855f353d",
   "metadata": {},
   "source": [
    "The number of gradient accumulation steps is the number of batches for which we perform the forward and backward passes \n",
    "before performing a weight update. \n",
    "\n",
    "There are multiple ways to execute a pipeline, called _schedules_. The grouped and interleaved schedules\n",
    " are the most efficient because they execute stages in parallel, while the sequential schedule is mostly used for debugging. \n",
    " In this tutorial, we will use the grouped schedule, which is the default.\n",
    "\n",
    "When using the grouped schedule, `gradient_accumulation_steps_per_replica` must be divisible by \n",
    "`(number of pipeline stages) * 2`. When using the interleaved schedule, `gradient_accumulation_steps_per_replica` \n",
    "must be divisible by `(number of pipeline stages)`. You can read more about the specifics of the different pipeline \n",
    "schedules in [the relevant section of the technical note on Model parallelism with TensorFlow](https://docs.graphcore.ai/projects/tf-model-parallelism/en/latest/pipelining.html#pipeline-scheduling).\n",
    "\n",
    "If we use more than two IPUs, the model will be automatically replicated to fill up the requested number of IPUs. \n",
    "For example, if we select 8 IPUs for our 2-IPU model, four replicas of the model will be produced.\n",
    "\n",
    "We also need to adjust `steps_per_execution` to be divisible by the total number of gradient accumulation steps across \n",
    "all replicas, so we make a slight change to the dataset-adjusting code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e93aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_divisible(number):\n",
    "    return number - number % (batch_size * num_replicas * gradient_accumulation_steps_per_replica)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a65fff6",
   "metadata": {},
   "source": [
    "When defining a model using the Keras Functional API, we control what parts of the model go into which stages with the \n",
    "`PipelineStage` context manager. Replace the model implementation in `demo.py` with:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b109aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn_pipielines():\n",
    "    # Input layer - \"entry point\" / \"source vertex\".\n",
    "    input_layer = keras.Input(shape=input_shape)\n",
    "\n",
    "    # Add graph nodes for the first pipeline stage.\n",
    "    with ipu.keras.PipelineStage(0):\n",
    "        x = keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\")(input_layer)\n",
    "        x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\")(x)\n",
    "\n",
    "    # Add graph nodes for the second pipeline stage.\n",
    "    with ipu.keras.PipelineStage(1):\n",
    "        x = keras.layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "        x = keras.layers.Flatten()(x)\n",
    "        x = keras.layers.Dropout(0.5)(x)\n",
    "        x = keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return input_layer, x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dde20c1",
   "metadata": {},
   "source": [
    "Any operations created inside a `PipelineStage(x)` context manager will be placed in the `x`th pipeline stage \n",
    "(where the stages are numbered starting from 0). Here, the model has been divided into two pipeline stages that run concurrently.\n",
    "\n",
    "If you define your model using the Keras Sequential API, you can use the model's `set_pipeline_stage_assignment` \n",
    "method to assign pipeline stages to layers.\n",
    "\n",
    "Now all we need to do is configure the pipelining-specific aspects of our model. \n",
    "Add the following line just before the first call to `model.compile()`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a022f15a",
   "metadata": {
    "tags": [
     "sst_hide_output"
    ]
   },
   "outputs": [],
   "source": [
    "print('Keras MNIST example, running on IPU with pipelining')\n",
    "(x_train, y_train), (x_test, y_test) = prepare_data_trim_to_size()\n",
    "\n",
    "with strategy.scope():\n",
    "    model = keras.Model(*model_fn_pipielines())\n",
    "\n",
    "    model.set_pipelining_options(\n",
    "        gradient_accumulation_steps_per_replica=gradient_accumulation_steps_per_replica,\n",
    "        pipeline_schedule=ipu.ops.pipelining_ops.PipelineSchedule.Grouped\n",
    "    )\n",
    "\n",
    "    train_steps_per_execution = make_divisible(len(x_train)) // (batch_size * num_replicas)\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics=[\"accuracy\"],\n",
    "                  steps_per_execution=train_steps_per_execution)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    print('\\nTraining')\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=3, batch_size=batch_size)\n",
    "\n",
    "    print('\\nEvaluation')\n",
    "    test_steps_per_execution = make_divisible(len(x_test)) // (batch_size * num_replicas)\n",
    "    model.compile('sgd', 'categorical_crossentropy', metrics=[\"accuracy\"],\n",
    "                  steps_per_execution=test_steps_per_execution)\n",
    "\n",
    "    model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248509e3",
   "metadata": {},
   "source": [
    "\n",
    "Within the scope of an `IPUStrategy`, IPU-specific methods such as `set_pipelining_options` are dynamically added \n",
    "to the base `keras.Model` class, which allows us to configure IPU-specific aspects of the model. \n",
    "We could use the interleaved schedule here by changing `Grouped` to `Interleaved`.\n",
    "\n",
    "The file `completed_demos/completed_demo_pipelining.py` shows what the code looks like after the above changes are made. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f348707e",
   "metadata": {},
   "source": [
    "#### Completed example\n",
    "\n",
    "The folder `completed_example` contains a complete implementation of the illustrated Keras model which is more easily \n",
    "configured than the scripts in the `completed_demos` directory. This has been provided for you to experiment with. \n",
    "Run `python3 completed_example/main.py` to run the standard Keras model on a CPU.\n",
    "\n",
    "The `--use-ipu` and `--pipelining` flags allow you to run the Keras model on the IPU and (optionally) adopt the \n",
    "pipelining feature respectively. The gradient accumulation count can be adjusted \n",
    "with the `--gradient-accumulation-count` flag.\n",
    "\n",
    "Note that the code in `completed_example` has been refactored into 3 parts:\n",
    "* `main.py`: Main code to be run.\n",
    "* `model.py`: Implementation of a standard Keras model and a pipelined Keras model.\n",
    "* `utils.py`: Contains functions that load the data and argument parser.\n",
    "\n",
    "#### License\n",
    "This example is licensed under the Apache License 2.0 - see the LICENSE file in this directory.\n",
    "\n",
    "Copyright (c) 2021 Graphcore Ltd. All rights reserved.\n",
    "\n",
    "This directory contains derived work from the following:\n",
    "\n",
    "Keras simple MNIST convnet example: https://github.com/keras-team/keras-io/blob/master/examples/vision/mnist_convnet.py\n",
    "\n",
    "Copyright holder unknown (author: François Chollet 2015)\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "    \n",
    "Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "\n",
    "See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
